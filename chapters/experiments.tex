\chapter{Experiments}\label{chap:experiments}

In this chapter we evaluate the preconditioning technique introduced in \cref{chap:sketching} inside the implementation of the IPM algorithm of \textcite{AndersenAndersen-MosekInteriorPointMethod} provided by the open source Python-based scientific computing library \texttt{scipy}~\cite{Scipy}.

\section{The Homogeneous Algorithm}

The interior-point method implemented in \texttt{scipy} is based on the MOSEK interior-point optimizer as published by \textcite{AndersenAndersen-MosekInteriorPointMethod}.
The most important difference to the general infeasible IPM described in \cref{alg:ipm} is its use of the homogeneous model presented in~\cite{XuHungYe-SimplifiedHomogeneousAlgorithm}.
While in \cref{chap:ipm} the original LP \cref{eqn:primal-lp} was solved by considering the primal and dual LP at the same time and adding a complimentary condition, the homogeneous algorithm replaces this primal-dual formulation by the self-dual LP
\begin{subequations}
  \begin{align}
    \vek{r}_p \coloneqq \mat{A}\vek{x} - \vek{b} \tau &= \vek{0} \\
    \vek{r}_d \coloneqq \mat{A}^T \vek{y} + \vek{s} - \vek{c}\tau &= \vek{0} \\
    \vek{r}_g \coloneqq - \vek{c}^T\vek{x} + \vek{b}^T\vek{y} - \kappa &= 0 \\
    \vek{x}, \vek{s} &\geq \vek{0} \\
    \tau, \kappa &\geq 0
  \end{align}
\end{subequations}
where \(\tau\) and \(\kappa\) are additional variables.
Observe that setting \(\vek{x}\), \(\vek{y}\), \(\vek{s}\), \(\tau\) and \(\kappa\) to zero gives a solution, so the LP above is always feasible.
The crucial advantage of using this homogeneous model is that there is always a solution where \((\vek{x}, \tau)\) and \((\vek{s}, \kappa)\) are strictly complimentary and every such solution can be transformed into either a solution of the original LP (if \(\tau > 0\)) or a certificate that the original LP is infeasible or unbounded (if \(\kappa > 0\)).
In other words, adding two additional variables the interior-point method can also handle infeasible and unbounded LPs.

The Newton system that would correspond to \cref{eqn:newton-system} is now given by
\begin{equation}\label{eqn:homogeneous-newton-system}
  \begin{pmatrix}
    \mat{A}    & -\vek{b} &           &         &         \\
               & -\vek{c} & \mat{A}^T & \mat{I} &         \\
    -\vek{c}^T &          & \vek{b}^T &         & -1 \\
    \mat{S}    &          &           & \mat{X} &         \\
               & \kappa   &           &         & \tau
  \end{pmatrix}
  \begin{pmatrix}
    \Delta\vek{x} \\
    \Delta\tau \\
    \Delta\vek{y} \\
    \Delta\vek{s} \\
    \Delta\kappa
  \end{pmatrix}
  =
  \begin{pmatrix}
    -\vek{r}_p \\
    -\vek{r}_d \\
    -\vek{r}_g \\
    -\vek{x} \circ \vek{s} + \sigma \mu \vek{1} \\
    -\tau \kappa + \sigma \mu
  \end{pmatrix}
\end{equation}
where \(\mu = (\vek{x}^T \vek{s} + \tau \kappa)/(n+1)\).
In the implementation the right hand side is modified to balance out the ambitiousness of reducing the infeasibility and the complementarity gap using heuristics.
Optionally, one can also enable an adaptation of Mehrotra's predictor-corrector approach first proposed in \cite{Mehrotra-PredictorCorrector} which first solves \cref{eqn:homogeneous-newton-system} for \(\sigma = 0\) and then solves the same system with a modified right hand side to determine the search direction.

In any case, it is necessary to solve \cref{eqn:homogeneous-newton-system} for various right hand sides.
The problem can be simplified as in \cref{chap:ipm} by exploiting that \(\mat{X}\) and \(\mat{S}\) are easily invertible.
The remaining linear system involves the matrix
\begin{equation}
  \begin{pmatrix}
    -\mat{X}^{-1}\mat{S} & \mat{A}^T & -\vek{c} \\
    \mat{A}              &           & -\vek{b} \\
    -\vek{c}^T           & \vek{b}^T & \tau^{-1}\kappa 
  \end{pmatrix}
\end{equation}
and can be transformed into two linear systems using the matrix from the augmented system \cref{eqn:augmented-system}.
Each of these can now be solved using the normal equation \(\mat{A}\mat{D}^2\mat{A}^T\vek{v} = \vek{p}\) for some vector \(\vek{p}\).
The details of this transformation process can be found in section 5 of \cite{AndersenAndersen-MosekInteriorPointMethod}.
The important takeaway is that solving \cref{eqn:homogeneous-newton-system} can be done by solving the normal equation twice for different right hand sides and some cheap matrix-vector products.
This allows us to use the sketching based preconditioning explained in \cref{chap:sketching} inside this IPM implementation.
Moreover, because the two linear systems involve the same matrix the preconditioner only needs to be determined once for both systems.

Some more differences between \cref{alg:ipm} and this implementation need to be addressed.
First, the neighbourhood used in this algorithm also includes a one-sided \(\infty\)-norm bound but drops the bound on the residual norms:
\begin{equation}
  \mathcal{N}(\gamma) = \Set{ \vek{x}, \vek{y}, \vek{s}, \tau, \kappa | \vek{x}, \vek{s} \geq \vek{0}, \ \tau, \kappa \geq 0, \ (\vek{x}, \tau) \circ (\vek{s}, \kappa) \geq (1-\gamma) \mu \vek{1} }
\end{equation}
Second, the step size \(\alpha\) is chosen to be the largest possible step that does not violate the nonnegativity constraint, multiplied by a constant slightly smaller than 1 and then reduced until the new iterate is inside the neighbourhood.
Third, the stopping criterion used is based on 
\begin{equation}
  \rho_p \coloneqq \frac{\norm{\vek{r}_p}_2}{\max(1, \norm{\vek{r}^0_p}_2)}, \ 
  \rho_d \coloneqq \frac{\norm{\vek{r}_d}_2}{\max(1, \norm{\vek{r}^0_d}_2)}, \ 
  \rho_A \coloneqq \frac{\abs{\vek{c}^T\vek{x} - \vek{b}^T \vek{y}}}{\tau + \abs{\vek{b}^T\vek{y}}}
\end{equation}
measuring the size of the primal and dual residuals and of the duality gap.
As long as no infeasibility of the original LP is detected, the algorithm stops once the maximum of these three values drops below a user-specified tolerance.
For our purposes let \(\rho_{\mathrm{tol}} = \max(\rho_p, \rho_d, \rho_A)\) measure the smallest tolerance for which the algorithm would terminate at a certain point during the algorithm.
Lastly, the starting point of the algorithm is typically chosen as \((\vek{x}, \tau, \vek{y}, \vek{s}, \kappa) = (\vek{1}, 1, \vek{0}, \vek{1}, 1)\) but with an option to instead use a heuristic at the same cost as one additional IPM iteration.

\section{Problem Instances}

To evaluate the performance of the IPM, random LP instances 
\begin{equation}\label{eqn:primal-lp-instance}
  \min \vek{c}^T \vek{x} \quad \text{subject to} \quad \mat{A}\vek{x} = \vek{b}, \ \vek{x} \geq \vek{0} \tag{P}
 \end{equation}
with \(\mat{A} \in \R^{m \times n}\), \(\vek{b} \in \R^m\), \(\vek{c} \in \R^n\) were generated that fit all of the desired characteristics: \(m \ll n\), \(\mat{A}\) is sparse, \(\rank(\mat{A}) = m\) and the LP is feasible and bounded.
To that end, we chose \(m = 1\,000\), \(n = 100\,000\) and constructed \(\mat{A}\) by randomly choosing two nonzero entries per column which were filled with values drawn from a standard normal distribution and also adding normally distributed values on the diagonal.
This ensures that \(\mat{A}\) is a sparse rectangular matrix with full row rank.
In order to obtain an LP that is feasible and bounded it is easiest to generate it a way that both the primal and dual problem are feasible.
The entries of \(\tilde{\vek{x}}, \tilde{\vek{s}} \in \R^n\) were chosen uniformly random from \([0, 1]\) and the entries of \(\tilde{\vek{y}} \in \R^m\) were randomly chosen using a standard normal distribution.
Then setting
\begin{equation}
  \vek{b} = \mat{A} \tilde{\vek{x}} \quad \text{and} \quad \vek{c} = \mat{A}^T \tilde{\vek{y}} + \tilde{\vek{s}}
\end{equation}
ensures the feasibility and boundedness of \cref{eqn:primal-lp-instance}.

\section{Experiment Configuration}

The experiments were all run on an Intel® Core™ i7-1165G7 using \texttt{scipy} version 1.6.2 and \texttt{numpy} version 1.20.2 with OpenBLAS version 0.3.8.
The code for the homogeneous IPM algorithm described above was taken from the \texttt{scipy.optimize} package\footnote{\url{https://github.com/scipy/scipy/blob/v1.6.2/scipy/optimize/_linprog_ip.py}} and adapted to incorporate iterative solvers and sketching based preconditioning.
To avoid inconsistent results the routines for eliminating unnecessary constraints and automatically rescaling the instances were disabled as were the use of the predictor-corrector method and the initial point heuristic.
Each parameter combination in the following experiments were run twice on the same four randomly generated instances.
This way we have hopefully captured the typical behaviour of the algorithm without giving some parameter setting an unfair advantage based on luck.

\section{Impact of the Sketching Parameters}

The first question we want to tackle is how good the preconditioners are, how fast they can be computed and how the answers to these questions are influenced by the sketching parameters \(w\) and \(s\) as defined in \cref{def:s-hashing-matrix}. \(w\) controls the number of columns of the sparse sketching matrix while \(s\) determines the number of nonzero entries per column.
By \cref{thm:sparse-ose} we expect larger values of \(w\) and \(s\) to give better preconditioners while also increasing the runtime of computing \(\mat{W}\mat{D}\mat{A}^T\) and its QR decomposition.

\begin{figure}[htbp]
  \centering%
  \ifdraft{%
    \includegraphics[width=6.1in, height=3.97in]{example-image-a}%
  }{%
    \import{plots/}{sketching_parameters_condition_number_history.pgf}%
  }
  \caption{Condition numbers as the IPM algorithm progresses}%
\end{figure}

\begin{figure}[htbp]%
  \centering%
  \ifdraft{%
    \includegraphics[width=6.1in, height=3.97in]{example-image-a}%
  }{%
    \import{plots/}{sketching_parameters_condition_number.pgf}%
  }
  \caption{Condition numbers depending on \(w\) and \(s\)}%
\end{figure}%

\begin{figure}[htbp]%
  \centering%
  \ifdraft{%
    \includegraphics[width=6.1in, height=3.97in]{example-image-a}%
  }{%
    \import{plots/}{sketching_parameters_sparsity.pgf}%
  }
  \caption{Sparsity of \(\mat{W}\mat{D}\mat{A}^T\) depending on \(w\) and \(s\)}%
\end{figure}%

\begin{figure}
  \centering
  \ifdraft{%
    \includegraphics[width=6.1in, height=3.97in]{example-image-a}%
  }{%
    \import{plots/}{sketching_parameters_generate_sketch_duration.pgf}
  }
  \caption{Time spent generating the \(s\)-hashing matrix \(\mat{W}\)}%
\end{figure}

\begin{figure}
  \centering%
  \ifdraft{%
    \includegraphics[width=6.1in, height=3.97in]{example-image-a}%
  }{%
    \import{plots/}{sketching_parameters_sketching_duration.pgf}%
  }
  \caption{Time spent computing the product \(\mat{W}\mat{D}\mat{A}^T\)}%
\end{figure}

\begin{figure}
  \centering%
  \ifdraft{%
    \includegraphics[width=6.1in, height=3.97in]{example-image-a}%
  }{%
    \import{plots/}{sketching_parameters_decomposition_duration.pgf}%
  }
  \caption{Time spent computing the QR decomposition of \(\mat{W}\mat{D}\mat{A}^T\)}%
\end{figure}