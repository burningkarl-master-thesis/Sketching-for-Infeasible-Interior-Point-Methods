\chapter{Experiments}\label{chap:experiments}

In this chapter we evaluate the preconditioning technique introduced in \cref{chap:sketching} inside the implementation of the IPM algorithm of \textcite{AndersenAndersen-MosekInteriorPointMethod} provided by the open source Python-based scientific computing library \texttt{scipy}~\cite{Scipy}.

\section{The Homogeneous Algorithm}\label{sec:homogeneous-algorithm}

The interior-point method implemented in \texttt{scipy} is based on the MOSEK interior-point optimizer as published by \textcite{AndersenAndersen-MosekInteriorPointMethod}.
The most important difference to the general infeasible IPM described in \cref{alg:ipm} is its use of the homogeneous model presented in~\cite{XuHungYe-SimplifiedHomogeneousAlgorithm}.
While in \cref{chap:ipm} the original LP \cref{eqn:primal-lp} was solved by considering the primal and dual LP at the same time and adding a complementary condition, the homogeneous algorithm replaces this primal-dual formulation by the self-dual LP
\begin{subequations}
  \begin{align}
    \vek{r}_p \coloneqq \mat{A}\vek{x} - \vek{b} \tau &= \vek{0} \\
    \vek{r}_d \coloneqq \mat{A}^T \vek{y} + \vek{s} - \vek{c}\tau &= \vek{0} \\
    \vek{r}_g \coloneqq - \vek{c}^T\vek{x} + \vek{b}^T\vek{y} - \kappa &= 0 \\
    \vek{x}, \vek{s} &\geq \vek{0} \\
    \tau, \kappa &\geq 0
  \end{align}
\end{subequations}
where \(\tau\) and \(\kappa\) are additional variables.
Observe that setting \(\vek{x}\), \(\vek{y}\), \(\vek{s}\), \(\tau\) and \(\kappa\) to zero gives a solution, so the LP above is always feasible.
The crucial advantage of using this homogeneous model is that there is always a solution where \((\vek{x}, \tau)\) and \((\vek{s}, \kappa)\) are strictly complementary and every such solution can be transformed into either a solution of the original LP (if \(\tau > 0\)) or a certificate that the original LP is infeasible or unbounded (if \(\kappa > 0\)).
In other words, by adding two additional variables the interior-point method becomes able to handle infeasible and unbounded LPs.

The Newton system that would correspond to \cref{eqn:newton-system} is now given by
\begin{equation}\label{eqn:homogeneous-newton-system}
  \begin{pmatrix}
    \mat{A}    & -\vek{b} &           &         &         \\
               & -\vek{c} & \mat{A}^T & \mat{I} &         \\
    -\vek{c}^T &          & \vek{b}^T &         & -1 \\
    \mat{S}    &          &           & \mat{X} &         \\
               & \kappa   &           &         & \tau
  \end{pmatrix}
  \begin{pmatrix}
    \Delta\vek{x} \\
    \Delta\tau \\
    \Delta\vek{y} \\
    \Delta\vek{s} \\
    \Delta\kappa
  \end{pmatrix}
  =
  \begin{pmatrix}
    -\vek{r}_p \\
    -\vek{r}_d \\
    -\vek{r}_g \\
    -\vek{x} \circ \vek{s} + \sigma \mu \vek{1} \\
    -\tau \kappa + \sigma \mu
  \end{pmatrix}
\end{equation}
where \(\mu = (\vek{x}^T \vek{s} + \tau \kappa)/(n+1)\).
In the implementation the right-hand side is modified to balance out the ambitiousness of reducing the infeasibility and the complementarity gap using heuristics.
Optionally, one can also enable an adaptation of Mehrotra's predictor-corrector approach first proposed in \cite{Mehrotra-PredictorCorrector} which first solves \cref{eqn:homogeneous-newton-system} for \(\sigma = 0\) and then solves the same system with a modified right-hand side to determine the search direction.
To solve \cref{eqn:homogeneous-newton-system} for arbitrary right-hand sides it is transformed into two linear systems \(\mat{A}\mat{D}^2\mat{A}^T\vek{u} = \vek{p}\) for different vectors \(\vek{p}\) as detailed in section 5 of \cite{AndersenAndersen-MosekInteriorPointMethod}.
This allows us to use the sketching-based preconditioning explained in \cref{sec:sketching-preconditioning} inside this IPM implementation.
Moreover, because the two linear systems involve the same matrix the preconditioner only needs to be determined once for both systems.

Some more differences between \cref{alg:ipm} and this implementation need to be addressed.
First, the neighbourhood used in this algorithm also includes a one-sided \(\infty\)-norm bound but drops the bound on the residual norms:
\begin{equation}
  \mathcal{N}(\gamma) = \Set{ \vek{x}, \vek{y}, \vek{s}, \tau, \kappa | \vek{x}, \vek{s} \geq \vek{0}, \ \tau, \kappa \geq 0, \ (\vek{x}, \tau) \circ (\vek{s}, \kappa) \geq (1-\gamma) \mu \vek{1} }
\end{equation}
Second, the step size \(\alpha\) is essentially chosen to be the largest possible step inside the neighbourhood without any line search minimizing \(\mu\).
Third, the stopping criterion used is based on 
\begin{equation}
  \rho_p \coloneqq \frac{\norm{\vek{r}_p}_2}{\max(1, \norm{\vek{r}^0_p}_2)}, \ 
  \rho_d \coloneqq \frac{\norm{\vek{r}_d}_2}{\max(1, \norm{\vek{r}^0_d}_2)}, \ 
  \rho_A \coloneqq \frac{\abs{\vek{c}^T\vek{x} - \vek{b}^T \vek{y}}}{\tau + \abs{\vek{b}^T\vek{y}}}
\end{equation}
measuring the size of the primal and dual residuals and of the duality gap.
Let \(\rho_{\mathrm{tol}} = \max(\rho_p, \rho_d, \rho_A)\) then the algorithm stops once \(\rho_{\mathrm{tol}}\) drops below a user-specified tolerance or when infeasibility or unboundedness is detected.
Lastly, the starting point of the algorithm is chosen to be \((\vek{x}, \tau, \vek{y}, \vek{s}, \kappa) = (\vek{1}, 1, \vek{0}, \vek{1}, 1)\) as \textcite{AndersenAndersen-MosekInteriorPointMethod} report that it works well for most problems.

\section{Problem Instances}\label{sec:problem-instance}

To evaluate the performance of the IPM, random LP instances 
\begin{equation}\label{eqn:primal-lp-instance}
  \min \vek{c}^T \vek{x} \quad \text{subject to} \quad \mat{A}\vek{x} = \vek{b}, \ \vek{x} \geq \vek{0} \tag{P}
 \end{equation}
with \(\mat{A} \in \R^{m \times n}\), \(\vek{b} \in \R^m\), \(\vek{c} \in \R^n\) were generated that fit all the desired characteristics: \(m \ll n\), \(\mat{A}\) is sparse, \(\rank(\mat{A}) = m\) and the LP is feasible and bounded.
To that end, we chose \(m = 1\,000\), \(n = 100\,000\) and constructed \(\mat{A}\) by randomly choosing two nonzero entries per column which were filled with values drawn from a standard normal distribution and also adding normally distributed values on the diagonal.
Next, a random feasible point \((\vek{x}, \vek{y}, \vek{s})\) was generated where the entries of \(\vek{x}\) and \(\vek{s}\) where drawn from a uniform distribution on \([0, 1]\) and the entries of \(\vek{y}\) from a standard normal distribution.
Setting \(\vek{b} = \mat{A} \vek{x}\) and \(\vek{c} = \mat{A}^T \vek{y} + \vek{s}\) ensures feasibility and boundedness of \cref{eqn:primal-lp-instance}.

\section{Experiment Configuration}\label{sec:experiment-configuration}

The experiments were all run on an Intel® Core™ i7-1165G7 using \texttt{scipy} version 1.6.2 and \texttt{numpy} version 1.20.2 with OpenBLAS version 0.3.8.
The code for the homogeneous IPM algorithm described above was taken from the \texttt{scipy.optimize} package\footnote{\url{https://github.com/scipy/scipy/blob/v1.6.2/scipy/optimize/_linprog_ip.py}} and adapted to incorporate iterative solvers and sketching-based preconditioning.
To avoid inconsistent results the routines for eliminating unnecessary constraints and automatically rescaling the instances were disabled as were the use of the predictor-corrector method and the initial point heuristic.
Each parameter combination in the following experiments was run twice on the same four randomly generated instances.
To visualize the range of recorded values the following graphs all include error bars.
The height of the line or bar indicates the median value while the error bars indicate the range from the 5th percentile to the 95th percentile.

\section{Impact of the Sketching Parameters}\label{sec:experiment-sketching-parameters}

The first question we want to tackle is how good the preconditioners are, how fast they can be computed and how the answers to these questions are influenced by the sketching parameters \(w\) and \(s\) as defined in \cref{def:s-hashing-matrix}.
Remember that \(w\) controls the number of columns of the sparse sketching matrix, while \(s\) determines the number of nonzero entries per column.
By \cref{thm:sparse-ose} we expect larger values of \(w\) and \(s\) to give better preconditioners while also increasing the runtime of computing \(\mat{W}\mat{D}\mat{A}^T\) and its QR decomposition.

\begin{figure}[tbp]
  \centering%
  \ifdraft{%
    \includegraphics[width=6.1in, height=3.97in]{example-image-a}%
  }{%
    \import{plots/}{sketching_parameters_condition_number_history.pgf}%
  }
  \caption{Condition numbers as the IPM algorithm progresses}%
  \label{fig:condition_number_history}
\end{figure}

\begin{figure}[tbp]%
  \centering%
  \ifdraft{%
    \includegraphics[width=6.1in, height=3.97in]{example-image-a}%
  }{%
    \import{plots/}{sketching_parameters_condition_number.pgf}%
  }
  \caption{Condition numbers depending on \(w\) and \(s\)}%
  \label{fig:condition_number}
\end{figure}%

\Cref{fig:condition_number_history} shows how the condition number of the condition numbers of the unpreconditioned matrix \(\mat{A}\mat{D}^2\mat{A}^T\) and the preconditioned matrix \(\mat{R}^{-T}\mat{A}\mat{D}^2\mat{A}^T\mat{R}^{-1}\) evolves during a run of the IPM algorithm.
Here, the sketching matrix is constructed using \(w=2m\) and \(s \in \{2, 3, 4, 5\}\).
While the condition number of the unpreconditioned system steadily rises up to about \(10^{12}\), the values for the preconditioned systems look much better.
For all cases except \(s = 2\) the condition numbers stay below 100.
\cref{fig:condition_number} shows the same condition numbers for all combinations of \(s \in \{3,4,5\}\) and \(w \in \{1.5m, 2m, 2.5m, 3m\}\) and includes values from all iterations.
As predicted by the theory the condition numbers are effectively bounded by a constant which decreases as \(w\) and \(s\) increase.
Interestingly, the lower end of the error bars line up for every fixed value of \(w\), suggesting that the value of \(w\) is the decisive factor for how low the condition number can become while \(s\) mainly influences the range of the distribution.

\begin{figure}[tbp]%
  \centering%
  \ifdraft{%
    \includegraphics[width=6.1in, height=3.97in]{example-image-a}%
  }{%
    \import{plots/}{sketching_parameters_sparsity.pgf}%
  }
  \caption{Sparsity of \(\mat{W}\mat{D}\mat{A}^T\) depending on \(w\) and \(s\)}%
  \label{fig:sparsity}
\end{figure}%

Next, \cref{fig:sparsity} shows the sparsity of \(\mat{W}\mat{D}\mat{A}^T\) by plotting the number of nonzero entries of the matrix.
While we will see that in this work the impact of the sparsity on the running time is negligible, for other applications the sparsity preservation that is achieved by sketching with a sparse matrix may be of great importance.
The red line indicates the number of nonzero entries of \(\mat{A}\) itself while all other colours bear the same meaning as in \cref{fig:condition_number}.
A simple upper bound to the number of nonzero entries is given by \(\nnz(\mat{W}\mat{D}\mat{A}^T) \leq s \nnz(\mat{A})\).
Even though in reality the number of nonzero entries is lower, the graph reflects that \(s\) is the main influence on the number of nonzero entries while the value of \(w\) only plays a minor role.
Interestingly, for example for \(w = 2m\) about \(25\%\) to \(40\%\) of the entries of the sketched matrix are nonzero while the same value for \(\mat{A}\) is at around \(0.2\%\).
This means that the absolute number of nonzero entries is only increased by a small factor, yet the relative density is increased to the point where the matrix is not very sparse anymore.

\begin{figure}[tbp]
  \centering
  \ifdraft{%
    \includegraphics[width=6.1in, height=3.97in]{example-image-a}%
  }{%
    \import{plots/}{sketching_parameters_generate_sketch_duration.pgf}
  }
  \caption{Time spent generating the \(s\)-hashing matrix \(\mat{W}\)}%
  \label{fig:generate_sketch_duration}
\end{figure}

\begin{figure}[tbp]
  \centering%
  \ifdraft{%
    \includegraphics[width=6.1in, height=3.97in]{example-image-a}%
  }{%
    \import{plots/}{sketching_parameters_sketching_duration.pgf}%
  }
  \caption{Time spent computing the product \(\mat{W}\mat{D}\mat{A}^T\)}%
  \label{fig:sketching_duration}
\end{figure}

\begin{figure}[tbp]
  \centering%
  \ifdraft{%
    \includegraphics[width=6.1in, height=3.97in]{example-image-a}%
  }{%
    \import{plots/}{sketching_parameters_decomposition_duration.pgf}%
  }
  \caption{Time spent computing the QR decomposition of \(\mat{W}\mat{D}\mat{A}^T\)}%
  \label{fig:decomposition_duration}
\end{figure}

\Cref{fig:generate_sketch_duration,fig:sketching_duration,fig:decomposition_duration} visualize the time spent in the different steps of sketching.
First, consider generating the sketching matrix \(\mat{W}\).
It turns out, that randomly sampling the row indices from \(\{1, \ldots, m\}\) for all columns without replacement is much faster than sampling them with replacement for each column separately, so it was beneficial to generate slightly more row indices than needed and then discard those instance where collisions occurred.
This deletion process might explain the low dependence on \(w\) in \cref{fig:generate_sketch_duration} but in any case the time is negligibly small for our purposes.
The time it takes to multiply the sketching matrix with \(\mat{D}\mat{A}^T\) is similarly small as shown by \cref{fig:sketching_duration} and mainly depends on the value of \(s\) as predicted by the theory.
The majority of the sketching process is actually spent determining a QR decomposition of \(\mat{W}\mat{D}\mat{A}^T\), see \cref{fig:decomposition_duration}.
Because the available sparse QR implementation did not bring any performance gains, a QR decomposition algorithm for dense matrices was used whose runtime, unsurprisingly, mainly depends on the matrix dimensions, i.e. on \(w\).
As such the runtime of determining a preconditioner depends mainly on \(w\), so we chose a moderate value of \(w=2m\) and a large value of \(s=5\) for the following experiments.

\section{Impact of the CG Tolerance}\label{sec:experiment-cg-tolerance}

Any direct solver for a linear system takes a fixed amount of time to achieve a solution of very high accuracy.
Iterative solvers come with the benefit that the solutions are refined in every iteration so that a trade-off between running time and accuracy is possible.
Due to numerical difficulties iterative solvers cannot achieve arbitrarily low errors, so we decided to fix the number of CG iterations at different values to see how they influence the solver accuracy and subsequently the course of the IPM.

\begin{figure}[tbp]
  \centering%
  \ifdraft{%
    \includegraphics[width=6.1in, height=3.97in]{example-image-a}%
  }{%
    \import{plots/}{tolerances_residual_norms.pgf}%
  }
  \caption{Relative errors achieved by CG and Cholesky}%
  \label{fig:residual_norms}
\end{figure}

\Cref{fig:residual_norms} shows the impact of varying the number of CG iterations on the relative errors for the preconditioned system and for the normal equation using data from all iterations.
The figure also includes the relative error achieved by using a Cholesky decomposition of the normal equation.
Evidently, the CG method applied to the preconditioned system fails to improve the residuals any further once the number of iterations reaches roughly 100 and the relative errors remain worse compared to the Cholesky decomposition.

\begin{figure}[tbp]
  \centering%
  \ifdraft{%
    \includegraphics[width=6.1in, height=3.97in]{example-image-a}%
  }{%
    \import{plots/}{tolerances_accuracy_history.pgf}%
  }
  \caption{Solution accuracy during the latter IPM iterations}%
  \label{fig:accuracy_history}
\end{figure}

\begin{figure}[tbp]
  \centering%
  \ifdraft{%
    \includegraphics[width=6.1in, height=3.97in]{example-image-a}%
  }{%
    \import{plots/}{tolerances_accuracy_vs_time.pgf}%
  }
  \caption{Solution accuracy vs. time per IPM iteration}%
  \label{fig:accuracy_vs_time}
\end{figure}

The effect on the accuracy of the IPM is exemplified in \cref{fig:accuracy_history}.
Here, the solution accuracy as measured by \(\rho_{\mathrm{tol}}\) is tracked over the final iterations of the IPM for one of the randomly generated LP instances.
It can be clearly seen that at some point during the optimisation process a breakdown occurs that leads to a sudden increase of \(\rho_{\mathrm{tol}}\).
In our experiments this was always due to an increase in \(\rho_p\) which measures the primal feasibility.
Even though the minimum \(\rho_{\mathrm{tol}}\) value is achieved at different IPM iterations for the different parameter settings the average number of iterations it took for \(\rho_{\mathrm{tol}}\) to reach its minimum across multiple LP instances was around 125 independently of the number of CG iterations.
As expected, the minimum value of \(\rho_{\mathrm{tol}}\) is lower the smaller the error in the solution of the normal equation.
The trade-off between solution accuracy and time spent per IPM iteration can be more closely examined in \cref{fig:accuracy_vs_time}.
Here, the best value of \(\rho_{\mathrm{tol}}\) that was achieved during each run of the algorithm is plotted against the average time each iteration took.

This time per iteration includes both the time to compute the preconditioner \(\mat{R}\) and the time it takes the CG solver to perform a fixed amount of iterations.
The latter is very much influenced by the way the matrix-vector products required for the CG algorithm are implemented.
For these experiments all products were evaluated lazily in the sense that the matrix \(\mat{R}^{-T}\mat{A}\mat{D}^2\mat{A}^T\mat{R}^{-1}\) was not formed explicitly but every time a vector was to be multiplied with it, three matrix-vector products and two triangular solves were performed.
Preliminary experiments indicate that due to very efficient routines for matrix inversion and matrix-matrix multiplication explicitly forming the product is similarly fast for 100 CG iterations and outperforms our implementation if more iterations are performed.

\section{Comparison with Direct Solvers}\label{sec:experiment-direct-comparison}

To finish this chapter, a direct comparison between our iterative approach and direct solvers is due.
The two direct methods we will compare it with are a Cholesky decomposition of \(\mat{A}\mat{D}^2\mat{A}^T\) and a QR decomposition of the unsketched matrix \(\mat{D}\mat{A}^T\).
The parameters for preconditioning were chosen to be \(w=2m\) and \(s=5\) and 100 CG iterations were performed.
\Cref{table:runtime_comparison} shows that, while computing the QR decomposition of the sketched matrix \(\mat{W}\mat{D}\mat{A}^T\) is much faster than computing the QR decomposition of the full matrix \(\mat{D}\mat{A}^T\), the preconditioned CG method cannot compete with the speed of the Cholesky decomposition.
Additionally, \cref{table:accuracy_comparison} demonstrates that the direct solvers generate more accurate solutions as the iterative approach can.

\begin{table}[htbp]
  \centering
  \sisetup{
    table-number-alignment=center,
    table-figures-integer=2,
    table-figures-decimal=3,
    table-auto-round=true,
  }
  \begin{tabular}{cSSSS}
    \toprule
    Method & {\makecell{Computing \\ \(\mat{W}\mat{D}\mat{A}^T\)}} & {\makecell{QR/Cholesky \\ decomposition}} & {\makecell{Solving \\ \(\mat{A}\mat{D}^2\mat{A}^T\vek{u} = \vek{p}\)}} & {Total} \\
    \midrule
    Preconditioned CG      & 0.052411 &  0.382210 & 0.686770 &  1.121391 \\
    Cholesky decomposition & {---}      &  0.042749 & 0.003998 &  0.046747 \\
    QR decomposition       & {---}      & 24.074416 & 0.004848 & 24.079265 \\
    \bottomrule
  \end{tabular}
  \caption{Runtime comparison: Average time per IPM iteration in seconds}
  \label{table:runtime_comparison}
\end{table}

\begin{table}[htbp]
  \centering
  \sisetup{
    table-number-alignment=center,
    table-figures-decimal=3,
    table-figures-exponent=2,
    table-figures-integer=1,
    table-sign-exponent=true,
    table-auto-round=true,
  }
  \begin{tabular}{cSSS}
    \toprule
    Method & \multicolumn{3}{c}{Best \(\rho_{\mathrm{tol}}\)} \\
           & {5th percentile} & {Median} & {95th percentile} \\
    \midrule
    Preconditioned CG      & 5.817955e-10 & 1.169027e-08 & 6.936769e-08 \\
    Cholesky decomposition & 3.278248e-11 & 9.673081e-11 & 1.298677e-10 \\
    QR decomposition       & 8.473068e-11 & 9.310012e-11 & 1.014696e-10 \\
    \bottomrule
  \end{tabular}
  \caption{Accuracy comparison: Best \(\rho_{\mathrm{tol}}\) over 150 IPM iterations}
  \label{table:accuracy_comparison}
\end{table}

\section{Summary}\label{sec:experiment-summary}

The experiments in this chapter showed that indeed the QR decomposition of \(\mat{W}\mat{D}\mat{A}^T\) produces good preconditioners that are able to reduce the condition number of \(\mat{A}\mat{D}^2\mat{A}^T\) by multiple orders of magnitude.
The quality of the linear solver for the normal equation directly affects the solution accuracy of the IPM algorithm as a whole so that the time-accuracy trade-off of the iterative solver can be transferred to a time-accuracy trade-off for the IPM algorithm.
Unfortunately, even if only very few CG iterations are performed solving the normal equation using the Cholesky decomposition is much faster because computing the QR decomposition of \(\mat{W}\mat{D}\mat{A}^T\) already takes about ten times as long as computing the Cholesky decomposition of \(\mat{A}\mat{D}^2\mat{A}^T\).
Whether there is a parameter regime regarding matrix dimensions and the sparsity of \(\mat{A}\), where the preconditioned CG method can outperform the Cholesky decomposition, is still an open question.