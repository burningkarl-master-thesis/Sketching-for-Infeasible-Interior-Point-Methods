\chapter{Experiments}\label{chap:experiments}

In this chapter we evaluate the preconditioning technique introduced in \cref{chap:sketching} inside the implementation of the IPM algorithm of \textcite{AndersenAndersen-MosekInteriorPointMethod} provided by the open source Python-based scientific computing library \texttt{scipy}~\cite{Scipy}.

\section{The Homogeneous Algorithm}

The interior-point method implemented in \texttt{scipy} is based on the MOSEK interior-point optimizer as published by \textcite{AndersenAndersen-MosekInteriorPointMethod}.
The most important difference to the general infeasible IPM described in \cref{alg:ipm} is its use of the homogeneous model presented in~\cite{XuHungYe-SimplifiedHomogeneousAlgorithm}.
While in \cref{chap:ipm} the original LP \cref{eqn:primal-lp} was solved by considering the primal and dual LP at the same time and adding a complimentary condition, the homogeneous algorithm replaces this primal-dual formulation by the self-dual LP
\begin{subequations}
  \begin{align}
    \vek{r}_p \coloneqq \mat{A}\vek{x} - \vek{b} \tau &= \vek{0} \\
    \vek{r}_d \coloneqq \mat{A}^T \vek{y} + \vek{s} - \vek{c}\tau &= \vek{0} \\
    \vek{r}_g \coloneqq - \vek{c}^T\vek{x} + \vek{b}^T\vek{y} - \kappa &= 0 \\
    \vek{x}, \vek{s} &\geq \vek{0} \\
    \tau, \kappa &\geq 0
  \end{align}
\end{subequations}
where \(\tau\) and \(\kappa\) are additional variables.
Observe that setting \(\vek{x}\), \(\vek{y}\), \(\vek{s}\), \(\tau\) and \(\kappa\) to zero gives a solution, so the LP above is always feasible.
The crucial advantage of using this homogeneous model is that there is always a solution where \((\vek{x}, \tau)\) and \((\vek{s}, \kappa)\) are strictly complimentary and every such solution can be transformed into either a solution of the original LP (if \(\tau > 0\)) or a certificate that the original LP is infeasible or unbounded (if \(\kappa > 0\)).
In other words, adding two additional variables the interior-point method can also handle infeasible and unbounded LPs.

The Newton system that would correspond to \cref{eqn:newton-system} is now given by
\begin{equation}\label{eqn:homogeneous-newton-system}
  \begin{pmatrix}
    \mat{A}    & -\vek{b} &           &         &         \\
               & -\vek{c} & \mat{A}^T & \mat{I} &         \\
    -\vek{c}^T &          & \vek{b}^T &         & -1 \\
    \mat{S}    &          &           & \mat{X} &         \\
               & \kappa   &           &         & \tau
  \end{pmatrix}
  \begin{pmatrix}
    \Delta\vek{x} \\
    \Delta\tau \\
    \Delta\vek{y} \\
    \Delta\vek{s} \\
    \Delta\kappa
  \end{pmatrix}
  =
  \begin{pmatrix}
    -\vek{r}_p \\
    -\vek{r}_d \\
    -\vek{r}_g \\
    -\vek{x} \circ \vek{s} + \sigma \mu \vek{1} \\
    -\tau \kappa + \sigma \mu
  \end{pmatrix}
\end{equation}
where \(\mu = (\vek{x}^T \vek{s} + \tau \kappa)/(n+1)\).
In the implementation the right-hand side is modified to balance out the ambitiousness of reducing the infeasibility and the complementarity gap using heuristics.
Optionally, one can also enable an adaptation of Mehrotra's predictor-corrector approach first proposed in \cite{Mehrotra-PredictorCorrector} which first solves \cref{eqn:homogeneous-newton-system} for \(\sigma = 0\) and then solves the same system with a modified right-hand side to determine the search direction.

In any case, it is necessary to solve \cref{eqn:homogeneous-newton-system} for various right-hand sides.
The problem can be simplified as in \cref{chap:ipm} by exploiting that \(\mat{X}\) and \(\mat{S}\) are easily invertible.
The remaining linear system involves the matrix
\begin{equation}
  \begin{pmatrix}
    -\mat{X}^{-1}\mat{S} & \mat{A}^T & -\vek{c} \\
    \mat{A}              &           & -\vek{b} \\
    -\vek{c}^T           & \vek{b}^T & \tau^{-1}\kappa 
  \end{pmatrix}
\end{equation}
and can be transformed into two linear systems using the matrix from the augmented system \cref{eqn:augmented-system}.
Each of these can now be solved using the normal equation \(\mat{A}\mat{D}^2\mat{A}^T\vek{v} = \vek{p}\) for some vector \(\vek{p}\).
The details of this transformation process can be found in section 5 of \cite{AndersenAndersen-MosekInteriorPointMethod}.
The important takeaway is that solving \cref{eqn:homogeneous-newton-system} can be done by solving the normal equation twice for different right-hand sides and some cheap matrix-vector products.
This allows us to use the sketching based preconditioning explained in \cref{chap:sketching} inside this IPM implementation.
Moreover, because the two linear systems involve the same matrix the preconditioner only needs to be determined once for both systems.

Some more differences between \cref{alg:ipm} and this implementation need to be addressed.
First, the neighbourhood used in this algorithm also includes a one-sided \(\infty\)-norm bound but drops the bound on the residual norms:
\begin{equation}
  \mathcal{N}(\gamma) = \Set{ \vek{x}, \vek{y}, \vek{s}, \tau, \kappa | \vek{x}, \vek{s} \geq \vek{0}, \ \tau, \kappa \geq 0, \ (\vek{x}, \tau) \circ (\vek{s}, \kappa) \geq (1-\gamma) \mu \vek{1} }
\end{equation}
Second, the step size \(\alpha\) is chosen to be the largest possible step that does not violate the nonnegativity constraint, multiplied by a constant slightly smaller than 1 and then reduced until the new iterate is inside the neighbourhood.
Third, the stopping criterion used is based on 
\begin{equation}
  \rho_p \coloneqq \frac{\norm{\vek{r}_p}_2}{\max(1, \norm{\vek{r}^0_p}_2)}, \ 
  \rho_d \coloneqq \frac{\norm{\vek{r}_d}_2}{\max(1, \norm{\vek{r}^0_d}_2)}, \ 
  \rho_A \coloneqq \frac{\abs{\vek{c}^T\vek{x} - \vek{b}^T \vek{y}}}{\tau + \abs{\vek{b}^T\vek{y}}}
\end{equation}
measuring the size of the primal and dual residuals and of the duality gap.
As long as no infeasibility of the original LP is detected, the algorithm stops once the maximum of these three values drops below a user-specified tolerance.
For our purposes let \(\rho_{\mathrm{tol}} = \max(\rho_p, \rho_d, \rho_A)\) measure the smallest tolerance for which the algorithm would terminate at a certain point during the algorithm.
Lastly, the starting point of the algorithm is typically chosen as \((\vek{x}, \tau, \vek{y}, \vek{s}, \kappa) = (\vek{1}, 1, \vek{0}, \vek{1}, 1)\) but with an option to instead use a heuristic at the same cost as one additional IPM iteration.

\section{Problem Instances}

To evaluate the performance of the IPM, random LP instances 
\begin{equation}\label{eqn:primal-lp-instance}
  \min \vek{c}^T \vek{x} \quad \text{subject to} \quad \mat{A}\vek{x} = \vek{b}, \ \vek{x} \geq \vek{0} \tag{P}
 \end{equation}
with \(\mat{A} \in \R^{m \times n}\), \(\vek{b} \in \R^m\), \(\vek{c} \in \R^n\) were generated that fit all the desired characteristics: \(m \ll n\), \(\mat{A}\) is sparse, \(\rank(\mat{A}) = m\) and the LP is feasible and bounded.
To that end, we chose \(m = 1\,000\), \(n = 100\,000\) and constructed \(\mat{A}\) by randomly choosing two nonzero entries per column which were filled with values drawn from a standard normal distribution and also adding normally distributed values on the diagonal.
This ensures that \(\mat{A}\) is a sparse rectangular matrix with full row rank.
In order to obtain an LP that is feasible and bounded it is easiest to generate it a way that both the primal and dual problem are feasible.
The entries of \(\tilde{\vek{x}}, \tilde{\vek{s}} \in \R^n\) were chosen uniformly random from \([0, 1]\) and the entries of \(\tilde{\vek{y}} \in \R^m\) were randomly chosen using a standard normal distribution.
Then setting
\begin{equation}
  \vek{b} = \mat{A} \tilde{\vek{x}} \quad \text{and} \quad \vek{c} = \mat{A}^T \tilde{\vek{y}} + \tilde{\vek{s}}
\end{equation}
ensures the feasibility and boundedness of \cref{eqn:primal-lp-instance}.

\section{Experiment Configuration}

The experiments were all run on an Intel® Core™ i7-1165G7 using \texttt{scipy} version 1.6.2 and \texttt{numpy} version 1.20.2 with OpenBLAS version 0.3.8.
The code for the homogeneous IPM algorithm described above was taken from the \texttt{scipy.optimize} package\footnote{\url{https://github.com/scipy/scipy/blob/v1.6.2/scipy/optimize/_linprog_ip.py}} and adapted to incorporate iterative solvers and sketching based preconditioning.
To avoid inconsistent results the routines for eliminating unnecessary constraints and automatically rescaling the instances were disabled as were the use of the predictor-corrector method and the initial point heuristic.
Each parameter combination in the following experiments were run twice on the same four randomly generated instances.
This way we have hopefully captured the typical behaviour of the algorithm without giving some parameter setting an unfair advantage based on luck.
To visualize the range of recorded values the following graphs all include error bars.
The height of the line or bar indicates the median value while the error bars indicate the range from the 5th percentile to the 95th percentile.

\section{Impact of the Sketching Parameters}

The first question we want to tackle is how good the preconditioners are, how fast they can be computed and how the answers to these questions are influenced by the sketching parameters \(w\) and \(s\) as defined in \cref{def:s-hashing-matrix}. \(w\) controls the number of columns of the sparse sketching matrix while \(s\) determines the number of nonzero entries per column.
By \cref{thm:sparse-ose} we expect larger values of \(w\) and \(s\) to give better preconditioners while also increasing the runtime of computing \(\mat{W}\mat{D}\mat{A}^T\) and its QR decomposition.

\begin{figure}[tbp]
  \centering%
  \ifdraft{%
    \includegraphics[width=6.1in, height=3.97in]{example-image-a}%
  }{%
    \import{plots/}{sketching_parameters_condition_number_history.pgf}%
  }
  \caption{Condition numbers as the IPM algorithm progresses}%
  \label{fig:condition_number_history}
\end{figure}

\begin{figure}[tbp]%
  \centering%
  \ifdraft{%
    \includegraphics[width=6.1in, height=3.97in]{example-image-a}%
  }{%
    \import{plots/}{sketching_parameters_condition_number.pgf}%
  }
  \caption{Condition numbers depending on \(w\) and \(s\)}%
  \label{fig:condition_number}
\end{figure}%

\Cref{fig:condition_number_history} shows how the condition number of the condition numbers of the unpreconditioned matrix \(\mat{A}\mat{D}^2\mat{A}^T\) and the preconditioned matrix \(\mat{R}^{-T}\mat{A}\mat{D}^2\mat{A}^T\mat{R}^{-1}\) evolves during a run of the IPM algorithm.
Here the sketching matrix was constructed using \(w=2m\) and \(s \in \{2, 3, 4, 5\}\).
While the condition number of the unpreconditioned system steadily rises up to about \(10^{12}\) the values for the preconditioned systems look much better.
For all but the \(s=2\) case the condition numbers consistently stay below \(100\).
\cref{fig:condition_number} shows the same condition numbers for all combinations of \(s \in \{3,4,5\}\) and \(w \in \{1.5m, 2m, 2.5m, 3m\}\).
As predicted by the theory the condition numbers are effectively bounded by a constant which decreases as \(w\) and \(s\) increase.
Interestingly, the lower end of the error bars line up for every fixed value of \(w\), suggesting that the value of \(w\) is the decisive factor for how low the condition number can become while \(s\) mainly influences the range of the distribution.

\begin{figure}[tbp]%
  \centering%
  \ifdraft{%
    \includegraphics[width=6.1in, height=3.97in]{example-image-a}%
  }{%
    \import{plots/}{sketching_parameters_sparsity.pgf}%
  }
  \caption{Sparsity of \(\mat{W}\mat{D}\mat{A}^T\) depending on \(w\) and \(s\)}%
  \label{fig:sparsity}
\end{figure}%

Next, \cref{fig:sparsity} shows the sparsity of \(\mat{W}\mat{D}\mat{A}^T\) by plotting the number of nonzero entries of the matrix.
While in this work the impact of the sparsity on the running time is negligible, as we will see, for other applications the sparsity preservation that is achieved by sketching with a sparse matrix may be of great importance.
The red line indicates the number of nonzero entries of \(\mat{A}^T\) itself, all other colours bear the same meaning as in \cref{fig:condition_number}.
A simple upper bound to the number of nonzero entries is given by \(\nnz(\mat{W}\mat{D}\mat{A}^T) \leq s \nnz(\mat{A})\).
Even though in reality the number of nonzero entries is lower, the graph reflects that \(s\) is the main influence on the number of nonzero entries while the value of \(w\) only plays a minor role.
Note that for example, for \(w = 2m\) about \(25\%\) to \(40\%\) of the entries of the sketched matrix are nonzero while the same value for \(\mat{A}\) is at around \(0.2\%\).
This means that the absolute number of nonzero entries is only increased by a small factor, yet the relative density is increased to the point where the matrix is hardly considered sparse anymore.

\begin{figure}[tbp]
  \centering
  \ifdraft{%
    \includegraphics[width=6.1in, height=3.97in]{example-image-a}%
  }{%
    \import{plots/}{sketching_parameters_generate_sketch_duration.pgf}
  }
  \caption{Time spent generating the \(s\)-hashing matrix \(\mat{W}\)}%
  \label{fig:generate_sketch_duration}
\end{figure}

\begin{figure}[tbp]
  \centering%
  \ifdraft{%
    \includegraphics[width=6.1in, height=3.97in]{example-image-a}%
  }{%
    \import{plots/}{sketching_parameters_sketching_duration.pgf}%
  }
  \caption{Time spent computing the product \(\mat{W}\mat{D}\mat{A}^T\)}%
  \label{fig:sketching_duration}
\end{figure}

\begin{figure}[tbp]
  \centering%
  \ifdraft{%
    \includegraphics[width=6.1in, height=3.97in]{example-image-a}%
  }{%
    \import{plots/}{sketching_parameters_decomposition_duration.pgf}%
  }
  \caption{Time spent computing the QR decomposition of \(\mat{W}\mat{D}\mat{A}^T\)}%
  \label{fig:decomposition_duration}
\end{figure}

\Cref{fig:generate_sketch_duration,fig:sketching_duration,fig:decomposition_duration} visualize the time spent in the different steps of sketching.
First, consider generating the sketching matrix \(\mat{W}\).
It turns out, that randomly sampling the row indices from \(\{1, \ldots, m\}\) for all columns without replacement is much faster than sampling them with replacement for each column separately, so it was beneficial to generate slightly more row indices than needed and then discard those instance where collisions occurred.
This deletion process might explain the low dependence on \(w\) in \cref{fig:generate_sketch_duration} but in any case the time is negligibly small for our purposes.
The time it takes to multiply the sketching matrix with \(\mat{D}\mat{A}^T\) is similarly small as shown by \cref{fig:sketching_duration}.
Similarly to the number of nonzero entries of \(\mat{W}\mat{D}\mat{A}^T\) this duration mainly depends on the value of \(s\).
The majority of the sketching process is actually spent determining a QR decomposition of \(\mat{W}\mat{D}\mat{A}^T\), see \cref{fig:decomposition_duration}.
Because the available sparse QR implementation did not bring any performance gains a QR decomposition algorithm for dense matrices was used whose runtime, unsurprisingly, mainly depends on the matrix dimensions, i.e. on \(w\).
As such the runtime of determining a preconditioner depends mainly on \(w\), so we chose a moderate value of \(w=2m\) and a large value of \(s=5\) for the following experiments.

\section{Impact of the CG Tolerance}

Any direct solver for a linear system takes a fixed amount of time to achieve a solution of very high accuracy.
Iterative solvers come with the benefit that the solutions are refined in every iteration so that a trade-off between running time and accuracy is possible.
Due to numerical difficulties iterative solvers cannot achieve arbitrarily low errors and so it was easiest to fix the number of CG iterations at different values to see how they impact the solver accuracy and subsequently the course of the IPM.

\begin{figure}[tbp]
  \centering%
  \ifdraft{%
    \includegraphics[width=6.1in, height=3.97in]{example-image-a}%
  }{%
    \import{plots/}{tolerances_residual_norms.pgf}%
  }
  \caption{Relative errors achieved by CG and Cholesky}%
  \label{fig:residual_norms}
\end{figure}

\Cref{fig:residual_norms} shows the impact of varying the number of CG iterations on the relative errors for the preconditioned system and, once transformed via \(\tilde{\vek{q}} = \mat{R}^{-1}\vek{q}\), for the normal equation.
The figure also includes the relative error achieved by using a Cholesky decomposition of the normal equation.
Evidently, the CG method applied to the preconditioning system fails to improve the residuals any further once the number of iterations reaches about roughly 100.
At this point it stagnates at values between \(10^{-12}\) and \(10^{-15}\).
Compared to the Cholesky decomposition the relative errors for solving the normal equation are slightly worse.

\begin{figure}[tbp]
  \centering%
  \ifdraft{%
    \includegraphics[width=6.1in, height=3.97in]{example-image-a}%
  }{%
    \import{plots/}{tolerances_accuracy_history.pgf}%
  }
  \caption{Solution accuracy during the latter IPM iterations}%
  \label{fig:accuracy_history}
\end{figure}

\begin{figure}[tbp]
  \centering%
  \ifdraft{%
    \includegraphics[width=6.1in, height=3.97in]{example-image-a}%
  }{%
    \import{plots/}{tolerances_accuracy_vs_time.pgf}%
  }
  \caption{Solution accuracy vs. time per IPM iteration}%
  \label{fig:accuracy_vs_time}
\end{figure}

The effect on the accuracy of the IPM is exemplified in \cref{fig:accuracy_history}.
Here, the solution accuracy as measured by \(\rho_{\mathrm{tol}}\) is tracked over the final iterations of the IPM for one of the randomly generated LP instance.
It can be clearly seen that at some point during the optimisation process a breakdown occurs that leads to a sudden increase of \(\rho_{\mathrm{tol}}\).
In our experiments this was always due to an increase in \(\rho_p\) which measures the primal feasibility which can occur because the IPM implementation does not include a bound on the primal and dual residuals in its neighbourhood definition. 
It is not clear if adding this bound would help to minimize \(\rho_{\mathrm{tol}}\) further or just lead to miniscule step sizes \(\alpha\).
For this IPM implementation as expected, the minimum value of \(\rho_{\mathrm{tol}}\) is lower the smaller the error in the solution of the normal equation.
The trade-off between solution accuracy and time spent per iteration can be more closely examined in \cref{fig:accuracy_vs_time}.
One can conclude that more CG iterations lead to better solutions up to about 100 iterations and also that using a Cholesky factorisation consistently outperforms the iterative solver both in solution quality and in runtime.

The time per IPM iteration as recorded in \cref{fig:accuracy_vs_time} includes both the time to compute the preconditioner \(\mat{R}\) and the time it takes the CG solver to perform a fixed amount of iterations.
The latter is very much influenced by the way the matrix vector products required for the CG algorithm are implemented.
For these experiments all products were evaluated lazily in the sense that the matrix \(\mat{R}^{-T}\mat{A}\mat{D}^2\mat{A}^T\mat{R}^{-1}\) was not formed explicitly but every time a vector was to be multiplied with it, three matrix vector products and two triangular solves were performed.
Preliminary experiments indicate that due to very efficient routines for matrix inversion and matrix matrix multiplication explicitly forming the product is similarly fast for 100 CG iterations and outperforms our implementation if more iterations are performed.

\section{Comparison with Direct Solvers}

To finish this chapter, a direct comparison between our iterative approach and direct solvers is due.
The two direct methods we will compare it with are a Cholesky decomposition of \(\mat{A}\mat{D}^2\mat{A}^T\) and a QR decomposition of the unsketched matrix \(\mat{D}\mat{A}^T\).
The parameters for preconditioning were chosen to be \(w=2m\) and \(s=5\) and 100 CG iterations were performed.
\Cref{table:runtime_comparison} shows that while computing the QR decomposition of the sketched matrix \(\mat{W}\mat{D}\mat{A}^T\) is much faster than computing the QR decomposition of the full matrix \(\mat{D}\mat{A}^T\) the preconditioned CG method cannot compete with the speed of the Cholesky decomposition.
Additionally, \cref{table:accuracy_comparison} demonstrates that the direct approach generates more accurate solutions as the iterative approach can.

\begin{table}[htbp]
  \centering
  \sisetup{
    table-number-alignment=center,
    table-figures-integer=2,
    table-figures-decimal=3,
    table-auto-round=true,
  }
  \begin{tabular}{cSSSS}
    \toprule
    Method & {\makecell{Computing \\ \(\mat{W}\mat{D}\mat{A}^T\)}} & {\makecell{QR/Cholesky \\ decomposition}} & {\makecell{Solving \\ \(\mat{A}\mat{D}^2\mat{A}^T\tilde{\vek{q}} = \vek{p}\)}} & {Total} \\
    \midrule
    Preconditioned CG      & 0.052411 &  0.382210 & 0.686770 &  1.121391 \\
    Cholesky decomposition & {---}      &  0.042749 & 0.003998 &  0.046747 \\
    QR decomposition       & {---}      & 24.074416 & 0.004848 & 24.079265 \\
    \bottomrule
  \end{tabular}
  \caption{Runtime comparison: Average time per IPM iteration in seconds}
  \label{table:runtime_comparison}
\end{table}

\begin{table}[htbp]
  \centering
  \sisetup{
    table-number-alignment=center,
    table-figures-decimal=3,
    table-figures-exponent=2,
    table-figures-integer=1,
    table-sign-exponent=true,
    table-auto-round=true,
  }
  \begin{tabular}{cSSS}
    \toprule
    Method & \multicolumn{3}{c}{Best \(\rho_{\mathrm{tol}}\)} \\
           & {5th percentile} & {Median} & {95th percentile} \\
    \midrule
    Preconditioned CG      & 5.817955e-10 & 1.169027e-08 & 6.936769e-08 \\
    Cholesky decomposition & 3.278248e-11 & 9.673081e-11 & 1.298677e-10 \\
    QR decomposition       & 8.473068e-11 & 9.310012e-11 & 1.014696e-10 \\
    \bottomrule
  \end{tabular}
  \caption{Accuracy comparison: Best \(\rho_{\mathrm{tol}}\) over 150 IPM iterations}
  \label{table:accuracy_comparison}
\end{table}

\section{Summary}

The experiments in this chapter showed that indeed the QR decomposition of \(\mat{W}\mat{D}\mat{A}^T\), where \(\mat{W}\) is a sparse sketching matrix, produces good preconditioners that are able to reduce the condition number of \(\mat{A}\mat{D}^2\mat{A}^T\) by multiple orders of magnitude.
The quality of the linear solver for the normal equation directly affects the solution accuracy of the IPM algorithm as a whole so that the time-accuracy trade-off of the iterative solver can be directly transferred to a time-accuracy trade-off for the IPM algorithm.
Unfortunately, even if only very few CG iterations are performed solving the normal equation using the Cholesky decomposition is much faster because computing the QR decomposition of \(\mat{W}\mat{D}\mat{A}^T\) already takes about ten times as long as computing the Cholesky decomposition of \(\mat{A}\mat{D}^2\mat{A}^T\).
Whether there is a parameter regime regarding matrix dimensions and the sparsity of \(\mat{A}\) where the preconditioned CG method can outperform the Cholesky decomposition is still an open question.