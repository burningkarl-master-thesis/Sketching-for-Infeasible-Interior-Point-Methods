\chapter{Theoretical Convergence}\label{chap:convergence}

After the previous two chapters introduced the important ideas and algorithms, we need to establish their convergence properties.
Note that \cref{alg:ipm} is the same infeasible inexact long-step IPM as in~\cite{Monteiro-ConvergenceAnalysisLongStepInfeasibleIPMs,Avron-FasterRandomizedInfeasibleIPMs} but with a different preconditioner.
Thus, the following proofs are very similar to those in their works with only slight changes.
The essential assumptions in the following proofs are that the algorithms work with exact arithmetic, that solutions of \labelcref{eqn:primal-lp,eqn:dual-lp} exist and that \(\mat{A}\) has full row rank.

Let \((\vek{x}, \vek{y}, \vek{s}) \in \mathcal{N}_{-\infty}(\gamma)\) denote a possible iterate during the course of \cref{alg:ipm}, \((\hat{\Delta\vek{x}}, \hat{\Delta\vek{y}}, \hat{\Delta\vek{s}})\) the Newton direction determined in \cref{line:compute-approximate-newton} and 
\begin{align}
  (\vek{x}(\alpha), \vek{y}(\alpha), \vek{s}(\alpha)) &\coloneqq (\vek{x}, \vek{y}, \vek{s}) + \alpha (\hat{\Delta\vek{x}}, \hat{\Delta\vek{y}}, \hat{\Delta\vek{s}}) \\
  \mu(\alpha) &\coloneqq {\vek{x}(\alpha)}^T \vek{s}(\alpha) \\
  \vek{r}(\alpha) &\coloneqq (\mat{A}\vek{x}(\alpha) - \vek{b}, \mat{A}^T \vek{y}(\alpha) + \vek{s}(\alpha) - \vek{c}).
\end{align}
Using this notation~\textcite{Monteiro-ConvergenceAnalysisLongStepInfeasibleIPMs} showed that the stepsize \(\bar{\alpha}\) determined in \cref{alg:ipm} is bounded from below.

\begin{lemma}[Lemma 3.6 in~\cite{Monteiro-ConvergenceAnalysisLongStepInfeasibleIPMs}]\label{thm:alpha-bar-bound}
  Assume that 
  \begin{itemize}
    \item \(\gamma \in (0, 1)\), \(\sigma \in (0, \frac{4}{5})\),
    \item \((\vek{x}, \vek{y}, \vek{s}) \in \mathcal{N}_{-\infty}(\gamma)\) and
    \item \((\hat{\Delta\vek{x}}, \hat{\Delta\vek{y}}, \hat{\Delta\vek{s}})\) satifies \cref{eqn:approx-newton} such that \(\norm{\vek{v}}_\infty \leq \gamma \sigma \mu / 4\).
  \end{itemize}
  Then the stepsize \(\bar{\alpha}\) determined in \cref{line:alpha-tilde,line:alpha-bar} satifies
  \[ \bar{\alpha} \geq \min \Set{1, \frac{\min \Set{\gamma \sigma, 1 - \frac{5}{4}\sigma} \mu}{4 \norm{\hat{\Delta\vek{x}} \circ \hat{\Delta\vek{s}}}_\infty}} \]
  and
  \[ \mu(\bar{\alpha}) \leq \Paren{1 - \Paren{1 - \frac{5}{4}\sigma} \frac{\bar{\alpha}}{2}} \mu. \]
\end{lemma}

Note that the assumptions in this lemma are not stated in this form in the original lemma but are instead inferred from the definition of their algorithm.
In the proof they show that \(\norm{\vek{v}}_\infty \leq \gamma \sigma \mu / 4\) and then deduce the results from it.
It suffices now to upper-bound \(\norm{\hat{\Delta\vek{x}} \circ \hat{\Delta\vek{s}}}_\infty\) to show that \(\mu\) decreases enough in each iteration.
To this end, we need a key observation which motivated us to shift the error term in \cref{chap:ipm} using the perturbation vector \(\vek{v}\):
The form of the error term in \cref{eqn:approximate-newton} ensures that at each iteration the residuals \(\vek{r} = (\vek{r}_p, \vek{r}_d)\) lie on the line segment between \(\vek{r}^0\) and \(\vek{0}\)
because \(\vek{r}(\alpha) = (1 - \alpha) \vek{r}(0)\).
In other words, \(\vek{r} = \eta \vek{r}^0\) with \(\eta \in [0, 1]\) for every iterate in \cref{alg:ipm}.

\begin{lemma}[Lemma 16 in~\cite{Avron-FasterRandomizedInfeasibleIPMs}]\label{thm:delta-x-s-bound}
  Assume that
  \begin{itemize}
    \item \(\gamma \in (0, 1)\), \(\sigma \in (0, 1)\), 
    \item \((\vek{x}^0, \vek{y}^0, \vek{s}^0) \in \mathcal{G}\) satisfies \((\vek{x}^0, \vek{s}^0) \geq (\vek{x}^*, \vek{s}^*)\) for some \((\vek{x}^*, \vek{y}^*, \vek{s}^*) \in \mathcal{F}^*\), 
    \item \((\vek{x}, \vek{y}, \vek{s}) \in \mathcal{N}_{-\infty}(\gamma)\) with \(\vek{r} = \eta \vek{r}^0\) for some \(\eta \in [0, 1]\) and
    \item \((\hat{\Delta\vek{x}}, \hat{\Delta\vek{y}}, \hat{\Delta\vek{s}})\) satifies \cref{eqn:approx-newton} such that \(\norm{\vek{v}}_2 \leq \gamma \sigma \mu / 4\).
  \end{itemize}
  Then
  \[ \norm{\mat{D}^{-1}\hat{\Delta\vek{x}}}_2, \norm{\mat{D}\hat{\Delta\vek{s}}}_2 \leq \Paren{1 + \frac{\sigma^2}{1- \gamma} - 2\sigma}^{1/2} \sqrt{n \mu} + \frac{6}{\sqrt{1 - \gamma}} n\sqrt{\mu} + \frac{\gamma \sigma}{4 \sqrt{1 - \gamma}}\sqrt{\mu}\]
  which implies that \(\norm{\mat{D}^{-1}\hat{\Delta\vek{x}}}_2\) and \(\norm{\mat{D}\hat{\Delta\vek{s}}}_2\) are bounded by \(C n \sqrt{\mu}\) for some constant \(C > 0\) depending on \(\gamma\) and \(\sigma\).
\end{lemma}

These two lemmas are enough to show the convergence of \cref{alg:ipm}.

\begin{proof}[Proof of \cref{thm:ipm-convergence}]
  By the choice of the initial point \((\vek{x}^0, \vek{y}^0, \vek{s}^0) \in \mathcal{N}_{-\infty}(\gamma)\) and \(\bar{\alpha}\) in \cref{line:alpha-tilde,line:alpha-bar} every iterate \((\vek{x}^k, \vek{y}^k, \vek{s}^k)\) is inside the specified neighbourhood \(\mathcal{N}_{-\infty}(\gamma)\).
  Therefore, \(\mu^k \leq \e_\mu \mu^0\) implies \(\norm{\vek{r}^k} \leq \e_\mu \norm{\vek{r}^0}\) and it suffices to show the former for some \(k = O(n^2 \log(1/\e_\mu))\).

  By \cref{thm:delta-x-s-bound} the search direction determined in \cref{line:compute-approximate-newton} satisfies
  \[ \norm{\hat{\Delta\vek{x}} \circ \hat{\Delta\vek{s}}}_\infty \leq \norm{\hat{\Delta\vek{x}} \circ \hat{\Delta\vek{s}}}_2 \leq \norm{\mat{D}^{-1}\hat{\Delta\vek{x}}}_2 \norm{\mat{D}\hat{\Delta\vek{s}}}_2 \leq C_1 n^2 \mu^k\]
  in every iteration, such that by \cref{thm:alpha-bar-bound} (\(\norm{\vek{v}}_\infty \leq \norm{\vek{v}}_2 \leq \gamma \sigma \mu^k/4\))
  \[ \bar{\alpha} \geq \min \Set{1, \frac{\min \Set{\gamma \sigma, 1 - \frac{5}{4}\sigma} \mu^k}{4 C_1 n^2 \mu^k}} \geq \frac{C_2}{n^2} \]
  where \(C_1, C_2 > 0\) are constants depending only on \(\gamma\) and \(\sigma\).
  Plugging this inequality in the second bound in \cref{thm:alpha-bar-bound} we get
  \[ \mu^{k+1} = \mu(\bar{\alpha}) \leq \Paren{ 1 - \Paren{1 - \frac{5}{4} \sigma} \frac{\bar{\alpha}}{2}} \mu^k \leq \Paren{ 1 - \Paren{1 - \frac{5}{4} \sigma} \frac{C_2}{2 n^2}} \mu^k \leq \Paren{1 - \frac{C_3}{n^2}} \mu^k \]
  for some constant \(C_3 > 0\).
  By induction this means that \(\mu^k \leq {(1 - \frac{C_3}{n^2})}^k \mu^0\) for all \(k \geq 0\).
  If \(\e_\mu \geq 1\) the algorithm terminates instantly so we can assume that \(\e_\mu < 1\).
  Now let \(k \geq n^2 \log(1/\e_\mu) / C_3\) which implies
  \[ k \log\Paren{1 - \frac{C_3}{n^2}} \leq k \Paren{- \frac{C_3}{n^2}} \leq -\log(1/\e_\mu) = \log(\e_\mu) \]
  using \(\log(\beta) \leq \beta-1\) for all \(\beta > 0\).
  Applying the exponential function on both sides gives \({(1 - \frac{C_3}{n^2})}^k \leq \e_\mu\) which shows that \(O(n^2 \log(1/\e_\mu))\) iterations suffice to get \(\mu^k \leq \e_\mu \mu^0\).
\end{proof}

Next, we turn to the correctness of \cref{alg:newton-direction} which is concerned with calculating a sufficiently good approximation of the Newton direction.
The proofs follow Lemma 11 to 13 in~\cite{Avron-FasterRandomizedInfeasibleIPMs} with slight simplifications.
As linear convergence of the CG method was already established in \cref{thm:cg-residual-bound} our main goal is to bound the norm of the initial residual \(\mat{R}^{-T}\vek{p}\).
To this end, we introduce the following bound that can already be found in~\cite{Monteiro-ConvergenceAnalysisLongStepInfeasibleIPMs}.

\begin{lemma}[Lemma 3.2 in~\cite{Monteiro-ConvergenceAnalysisLongStepInfeasibleIPMs}, Lemma 11 in~\cite{Avron-FasterRandomizedInfeasibleIPMs}]\label{thm:eta-bound}
  Assume that
  \begin{itemize}
    \item \((\vek{x}^0, \vek{y}^0, \vek{s}^0) \in \mathcal{G}\) satisfies \((\vek{x}^0, \vek{s}^0) \geq (\vek{x}^*, \vek{s}^*)\) for some \((\vek{x}^*, \vek{y}^*, \vek{s}^*) \in \mathcal{F}^*\) and
    \item \((\vek{x}, \vek{y}, \vek{s}) \in \mathcal{G}\) satifies \(\vek{r} = \eta \vek{r}^0\) for some \(\eta \in [0, 1]\).
  \end{itemize}
  Then \(\eta \leq \vek{x}^T \vek{s} / {\vek{x}^0}^T \vek{s}^0\) implies \(\eta ({\vek{x}^0}^T \vek{s} + \vek{x}^T {\vek{s}^0}) \leq 3 n \mu\).
  % Assume that the point \((\vek{x}^0, \vek{y}^0, \vek{s}^0) \in \mathcal{G}\) satisfies \((\vek{x}^0, \vek{s}^0) \geq (\vek{x}^*, \vek{s}^*)\) for some \((\vek{x}^*, \vek{y}^*, \vek{s}^*) \in \mathcal{F}^*\).
  % Then for any point \((\vek{x}, \vek{y}, \vek{s}) \in \mathcal{G}\) such that the corresponding residuals satisfy \(\vek{r} = \eta \vek{r}^0\) for some \(\eta \in [0, 1]\) and \(\eta \leq \vek{x}^T \vek{s} / {\vek{x}^0}^T \vek{s}^0\) we have
  % \(\eta ({\vek{x}^0}^T \vek{s} + \vek{x}^T {\vek{s}^0}) \leq 3 n \mu\).
\end{lemma}

Again, this makes use of the observation that the residual \(\vek{r}\) of any iterate lies on the line segment between \(\vek{r}^0\) and \(\vek{0}\).
Note that additionally, by the definition of the neighbourhood \(\mathcal{N}_{-\infty}(\gamma)\), any point in it with \(\vek{r} = \eta \vek{r}^0\) also satisifies \(\eta \leq \mu^k / \mu^0 = {\vek{x}^k}^T \vek{s}^k / {\vek{x}^0}^T \vek{s}^0\).

\begin{lemma}[Lemma 12 in~\cite{Avron-FasterRandomizedInfeasibleIPMs} (slightly simplified)]\label{thm:initial-residual-bound}
  Assume that
  \begin{itemize}
    \item \(\gamma \in (0, 1)\), \(\sigma \in (0, 1)\), \((\vek{x}^*, \vek{y}^*, \vek{s}^*) \in \mathcal{F}^*\),
    \item \((\vek{x}^0, \vek{y}^0, \vek{s}^0) \in \mathcal{G}\) satisfies \(\vek{x}^0 \geq \vek{x}^*\) and \(\vek{s}^0 \geq \max\set{\vek{s}^*, \abs{\mat{A}^T\vek{y}^0 - \vek{c}}}\)
    \item \((\vek{x}, \vek{y}, \vek{s}) \in \mathcal{N}_{-\infty}(\gamma)\) satisfies \(\vek{r} = \eta \vek{r}^0\) for some \(\eta \in [0, 1]\) and
    \item \(\mat{R}^{-T} \in \R^{m \times m}\) satisifies \(\norm{\mat{R}^{-T}\mat{A}\mat{D}}_2 \leq \sqrt{2}\).
  \end{itemize}
  Then
  \[ \norm{\mat{R}^{-T} \vek{p}}_2 \leq \sqrt{\frac{2 \mu}{1 - \gamma}} \Paren{7n + \sigma \sqrt{n}} \leq 8 \sqrt{\frac{2}{1 - \gamma}} n \sqrt{\mu}. \]
\end{lemma}

\begin{proof}
  First, we will deduce bounds for \(\mat{S}(\vek{x}^0 - \vek{x}^*)\) and \(\mat{X}\vek{r}_d^0\) that we need later.
  Note that because \(\vek{0} \leq \vek{x}^* \leq \vek{x}^0\) and \(\vek{s} \geq \vek{0}\) we have \(\vek{0} \leq \mat{S}(\vek{x}^0 - \vek{x}^*) \leq \mat{S}\vek{x}^0 \), so
  \[
    \norm{\mat{S}(\vek{x}^0 - \vek{x}^*)}_2
    \leq \norm{\mat{S}\vek{x}^0}_2
    \leq \norm{\mat{S}\vek{x}^0}_1
    = \vek{s}^T \vek{x}^0.
  \]
  Using a similar argument as above, \(\vek{0} \leq \abs{\mat{A}^T \vek{y}^0 - \vek{c}} \leq \vek{s}^0\) implies \(\norm{\mat{X} \abs{\mat{A}^T \vek{y}^0 - \vek{c}}}_2 \leq \norm{\mat{X}\vek{s}^0}_2\) and therefore
  \begin{align*}
    \norm{\mat{X}\vek{r}_d^0}_2
    &= \norm{\mat{X}(\mat{A}^T \vek{y}^0 + \vek{s}^0 - \vek{c})}_2
    \leq \norm{\mat{X}\vek{s}^0}_2 + \norm{\mat{X} \abs{\mat{A}^T \vek{y}^0 - \vek{c}}}_2 \\
    &\leq 2 \norm{\mat{X}\vek{s}^0}_2
    \leq 2 \norm{\mat{X}\vek{s}^0}_1
    = 2 \vek{x}^T \vek{s}^0.
  \end{align*}
  Now, consider the expression in the claim. Because
  \begin{align*}
    \mat{R}^{-T}\vek{p}
    &= \mat{R}^{-T}\Paren{-\vek{r}_p + \mat{A} ( -\mat{S}^{-1} \mat{X} \vek{r}_d + \vek{x} - \sigma \mu \mat{S}^{-1} \vek{1} )} \\
    &= \mat{R}^{-T}\Paren{-\vek{r}_p -\mat{A}\mat{S}^{-1}\mat{X} \vek{r}_d + \mat{A}\vek{x} - \sigma \mu \mat{A}\mat{S}^{-1} \vek{1} } \\
    &= \mat{R}^{-T}\Paren{-\eta \vek{r}_p^0 - \eta \mat{A}\mat{S}^{-1}\mat{X} \vek{r}_d^0 + \mat{A}\vek{x} - \sigma \mu \mat{A}\mat{S}^{-1} \vek{1} } \\
    &= \mat{R}^{-T}\Paren{- \eta \mat{A}(\vek{x}^0 - \vek{x}^*) - \eta \mat{A}\mat{S}^{-1}\mat{X} \vek{r}_d^0 + \mat{A}\vek{x} - \sigma \mu \mat{A}\mat{S}^{-1} \vek{1} } \\
    &= \mat{R}^{-T}\mat{A}\Paren{- \eta (\vek{x}^0 - \vek{x}^*) - \eta \mat{S}^{-1}\mat{X} \vek{r}_d^0 + \vek{x} - \sigma \mu \mat{S}^{-1} \vek{1} } \\
    &= \mat{R}^{-T}\mat{A}\mat{S}^{-1}\Paren{- \eta \mat{S}(\vek{x}^0 - \vek{x}^*) - \eta \mat{X} \vek{r}_d^0 + \mat{S}\vek{x} - \sigma \mu \vek{1} } \\
    &= \mat{R}^{-T}\mat{A}\mat{D}{(\mat{S}\mat{X})}^{-1/2}\Paren{- \eta \mat{S}(\vek{x}^0 - \vek{x}^*) - \eta \mat{X} \vek{r}_d^0 + \mat{S}\vek{x} - \sigma \mu \vek{1} }
  \end{align*}
  using the definition of \(\vek{p}\) from \cref{eqn:normal-equation} and the assumption \(\vek{r} = \eta \vek{r}^0\), it suffices to bound each part of the last expression.
  \(\norm{\mat{R}^{-T}\mat{A}\mat{D}}_2 \leq \sqrt{2}\) by assumption,
  \[\norm{{(\mat{S}\mat{X})}^{-1/2}}_2 = \max_{1 \leq i \leq n} {(x_i s_i)}^{-1/2} \leq \sqrt{\frac{1}{(1 - \gamma) \mu}}\]
  by the definition of the neighbourhood and
  \(\norm{\mat{S}\vek{x}}_2 \leq \norm{\mat{S}\vek{x}}_1 = n \mu\)
  using the definition of \(\mu\).
  \(\norm{\mat{S}(\vek{x}^0 - \vek{x}^*)}_2\) and \(\norm{\mat{X}\vek{r}_d^0}_2\) were already bounded above and lastly, \(\norm{\vek{1}}_2 = \sqrt{n}\).
  This implies
  \begin{align*}
    \norm{\mat{R}^{-T} \vek{p}}_2 
    &\leq \norm{\mat{R}^{-T}\mat{A}\mat{D}}_2 \norm{{(\mat{S}\mat{X})}^{-1/2}}_2 \Paren{\eta \norm{\mat{S}(\vek{x}^0 - \vek{x}^*)} + \eta \norm{\mat{X}\vek{r}_d^0} + \norm{\mat{S}\vek{x}}_2 + \sigma \mu \norm{\vek{1}}_2} \\
    &\leq \sqrt{2} \sqrt{\frac{1}{(1-\gamma) \mu}} \Paren{\eta \vek{s}^T \vek{x}^0 + 2 \eta \vek{x}^T \vek{s}^0 + n\mu + \sigma \mu \sqrt{n}} \\
    &\leq \sqrt{2} \sqrt{\frac{1}{(1-\gamma) \mu}} \Paren{6 n \mu + n\mu + \sigma \mu \sqrt{n}}
    = \sqrt{\frac{2 \mu}{1 - \gamma}} \Paren{7 n + \sigma \sqrt{n}}
  \end{align*}
  with the help of \cref{thm:eta-bound}.
\end{proof}

Equipped with this result, we can prove the main result regarding \cref{alg:newton-direction}.

\begin{proof}[Proof of \cref{thm:approximate-newton-convergence}]
  By \cref{thm:sparse-ose} the matrix \(\mat{W}\) selected in \cref{line:draw-sketching-matrix} of \cref{alg:newton-direction} is a subspace embedding for \(\mat{D}\mat{A}^T\) with probability at least \(1 - \delta\), so it suffices to show all of the claims for this case.
  Therefore, assume that \(\mat{W}\) is indeed a \((1 \pm \e)\) \(\ell_2\)-subspace embedding for \(\mat{D}\mat{A}^T\) for \(\e = 1/3\) from now on.
  In this case the QR decomposition \(\mat{Q}\mat{R} = \mat{W}\mat{D}\mat{A}^T\) exists and by \cref{thm:condition-number-bound} we have
  \[\norm{\mat{R}^{-T}\mat{A}\mat{D}}_2 \leq \sqrt{\frac{4}{3}} \leq \sqrt{2}.\]
  Let \(\vek{q}^j\) be the iterates of the CG algorithm in \cref{line:cg} and let \(\vek{f}^j\) be the residuals defined by
\( \vek{f}^j = \mat{R}^{-T} \mat{A} \mat{D}^2 \mat{A} \mat{R}^{-1} \vek{q}^j - \mat{R}^{-T} \vek{p} \).
  Using \cref{thm:initial-residual-bound} and \(\vek{q}^0 = \vek{0}\) we have \(\norm{\vek{f}^0}_2 = \norm{\mat{R}^{-T} \vek{p}}_2 \leq Cn\sqrt{\mu}\) for some constant \(C > 0\) depending on \(\gamma\).
  \Cref{thm:cg-residual-bound} immediately implies that \(\norm{\vek{f}^j}_2\) converges to zero and the so \cref{line:cg} will terminate.
  Moreover, it tells us that for \(j \geq \log_{1/2}(\e_{\vek{v}}/C n^2 \mu) = \log_2(C n^2 \mu / \e_{\vek{v}})\) we have
  \[ \norm{\vek{f}^j}_2 \leq \Paren{\frac{1}{2}}^j \norm{\vek{f}^0}_2 \leq \frac{\e_{\vek{v}}}{C n^2 \mu} C n \sqrt{\mu} \leq \frac{\e_{\vek{v}}}{n \sqrt{\mu}} \]
  so \cref{line:cg} will terminate after \(O(\log(n^2 \mu / \e_{\vek{v}}))\) CG iterations.
  By \cref{thm:perturbation-vector} and the definition of \(\vek{v}\) in \cref{line:compute-v} \(\norm{\vek{f}^j}_2 \leq \e_{\vek{v}}/n\sqrt{\mu}\) implies \(\norm{\vek{v}}_2 \leq \e_{\vek{v}}\).
  \Cref{line:compute-delta-x,line:compute-delta-y,line:compute-delta-s} compute \((\hat{\Delta\vek{x}}, \hat{\Delta\vek{y}}, \hat{\Delta\vek{s}})\) which satisfy \cref{eqn:perturbed-search-direction-choice} for \(\tilde{\vek{f}} = \mat{R}^T \vek{f}\).
  Using \cref{thm:perturbation-vector} again we can see that
  \begin{align*}
    \mat{A}\hat{\Delta\vek{x}} 
    &= \mat{A} (-\vek{x} + \sigma \mu \mat{S}^{-1}\vek{1} - \mat{S}^{-1}\mat{X}\hat{\Delta\vek{s}} - \mat{S}^{-1}\vek{v}) \\
    &= -\mat{A}\vek{x} + \sigma \mu \mat{A}\mat{S}^{-1}\vek{1} - \mat{A}\mat{S}^{-1}\mat{X}\hat{\Delta\vek{s}} - \mat{A}\mat{S}^{-1}\vek{v} \\
    &= -\mat{A}\vek{x} + \sigma \mu \mat{A}\mat{S}^{-1}\vek{1} + \mat{A}\mat{S}^{-1}\mat{X}\vek{r}_d + \mat{A}\mat{S}^{-1}\mat{X}\mat{A}^T \hat{\Delta\vek{y}} - \mat{A}\mat{S}^{-1}\vek{v} \\
    &= -\mat{A}\vek{x} + \sigma \mu \mat{A}\mat{S}^{-1}\vek{1} + \mat{A}\mat{S}^{-1}\mat{X}\vek{r}_d + \vek{p} + \vek{R}^T\vek{f} - \mat{A}\mat{S}^{-1}\vek{v} \\
    &= -\vek{r}_p + \mat{R}^T \vek{f} - \mat{A}\mat{S}^{-1}\vek{v} \\
    &= -\vek{r}_p \\
    \mat{A}^T \hat{\Delta\vek{y}} + \hat{\Delta\vek{s}}
    &= \mat{A}^T \hat{\Delta\vek{y}} - \vek{r}_d - \mat{A}^T \hat{\Delta\vek{y}} \\
    &= -\vek{r}_d \\
    \mat{S}\hat{\Delta\vek{x}} + \mat{X}\hat{\Delta\vek{s}} 
    &= \mat{S}(-\vek{x} + \sigma \mu \mat{S}^{-1}\vek{1} - \mat{S}^{-1}\mat{X}\hat{\Delta\vek{s}} - \mat{S}^{-1}\vek{v}) + \mat{X}\hat{\Delta\vek{s}} \\
    &= -\mat{S}\vek{x} + \sigma \mu \vek{1} - \mat{X}\hat{\Delta\vek{s}} - \vek{v} + \mat{X}\hat{\Delta\vek{s}} \\
    &= -\vek{x} \circ \vek{s} + \sigma \mu \vek{1} - \vek{v}
  \end{align*}
  which shows that the determined search direction indeed satisfies \cref{eqn:perturbed-search-direction} with \(\norm{\vek{v}}_2 \leq \e_{\vek{v}}\).

  After showing that the algorithm works correctly, we need to bound the running time.
  Multiplication with \(\mat{X}, \mat{X}^{1/2}, \mat{S}, \mat{S}^{1/2}\) and their inverses can be done in \(O(n)\), multiplication with \(\mat{A}\) and \(\mat{A}^T\) in \(O(\nnz(\mat{A}))\) and multiplication with \(\mat{R}^{-1}\) and \(\mat{R}^{-T}\) in \(O(m^2)\) using triangular solves.
  Because \(m \leq n \leq \nnz(\mat{A})\) we can thus conclude that \cref{line:compute-residuals,line:compute-p,line:compute-delta-x,line:compute-delta-y,line:compute-delta-s} run in \(O(\nnz(\mat{A}) + m^2)\) operations.
  Constructing the random sparse embedding matrix in \cref{line:draw-sketching-matrix} requires us to choose \(n \cdot s\) random nonzero entries and therefore needs \(O(n s)\) time.
  Computing \(\mat{W}\mat{D}\mat{A}^T\) in \cref{line:compute-qr} can be done in \(O(s \nnz(\mat{A}))\) because of the sparsity of \(\mat{W}\) and computing the QR decomposition of a \(w \times m\) matrix takes \(O(w m^2)\) operations.
  The number of CG iterations in \cref{line:cg} were already bounded by \(O(\log(n^2 \mu / \e_{\vek{v}}))\) and the runtime of each iteration is dominated by a matrix-vector product involving \(\mat{R}^{-T}\mat{A}\mat{D}^2\mat{A}^T\mat{R}^{-1}\) which takes \(O(\nnz(\mat{A}) + m^2)\) flops.
  Lastly, the perturbation vector \(\vek{v}\) is calculated in \cref{line:compute-v} in \(O(ns + m^2)\) time.
  Combining these upper bounds gives
  \[ O(\nnz(\mat{A}) + m^2 + ns + s \nnz(\mat{A}) + wm^2 + \log(n^2 \mu / \e_{\vek{v}})(\nnz(\mat{A}) + m^2)) \]
  operations which simplifies to
  \[ O(\log(m/\delta) (\nnz(\mat{A}) + m^3) + \log(n^2 \mu / \e_{\vek{v}})(\nnz(\mat{A}) + m^2)) \]
  operations because \(w\) and \(s\) were chosen according to \cref{thm:sparse-ose}.
\end{proof}

% \Cref{thm:ipm-convergence} is concerned with the convergence and runtime of \cref{alg:ipm} while \cref{thm:approximate-newton-convergence} shows the correctness and runtime of \cref{alg:newton-direction}.
% The two algorithms and theorems can be combined by using \cref{alg:newton-direction} in \cref{line:compute-approx-newton} of \cref{alg:ipm} which yields the following theorem.

% \begin{theorem}
%   Assume that \(\gamma \in (0, 1)\) and \(\sigma \in (0, \frac{4}{5})\) are constant and that the initial point \((\vek{x}^0, \vek{y}^0, \vek{s}^0) \in \mathcal{G}\) satisfies \(\vek{x}^0 \circ \vek{s}^0 \geq (1-\gamma)\vek{1}\) and \((\vek{x}^0, \vek{s}^0) \geq (\vek{x}^*, \vek{s}^*)\) for some \((\vek{x}^*, \vek{y}^*, \vek{s}^*) \in \mathcal{F}^*\).
%   Furthermore, assume that \cref{alg:ipm} uses \cref{alg:newton-direction} with \(\e_{\vek{v}} = \gamma \sigma \mu^k / 4\) to determine the approximate Newton direction in every step.
%   Then the algorithm terminates successfully using only \(O(???)\) flops with probability at least \(99\%\) for \(\delta = O(n^{-2})\).
%  \end{theorem}
