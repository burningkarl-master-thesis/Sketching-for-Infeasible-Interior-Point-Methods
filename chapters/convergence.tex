\chapter{Theoretical Convergence}\label{chap:convergence}

After the previous two chapters introduced the important ideas and algorithms, we need to establish their convergence properties.
Note that \cref{alg:ipm} is the same infeasible inexact long-step IPM as in~\cite{Monteiro-ConvergenceAnalysisLongStepInfeasibleIPMs,Avron-FasterRandomizedInfeasibleIPMs} but with a different preconditioner.
Thus, the following proofs are very similar to those in their works with only slight changes.
The essential assumptions in the following proofs are that the algorithms work with exact arithmetic, that solutions of \labelcref{eqn:primal-lp,eqn:dual-lp} exist and that \(\mat{A}\) has full row rank.

Let \((\vek{x}, \vek{y}, \vek{s}) \in \mathcal{N}_{-\infty}(\gamma)\) denote a possible iterate during the course of \cref{alg:ipm}, \((\hat{\Delta\vek{x}}, \hat{\Delta\vek{y}}, \hat{\Delta\vek{s}})\) the Newton direction determined in \cref{line:compute-approx-newton} and 
\begin{align}
  (\vek{x}(\alpha), \vek{y}(\alpha), \vek{s}(\alpha)) &\coloneqq (\vek{x}, \vek{y}, \vek{s}) + \alpha (\hat{\Delta\vek{x}}, \hat{\Delta\vek{y}}, \hat{\Delta\vek{s}}) \\
  \mu(\alpha) &\coloneqq {\vek{x}(\alpha)}^T \vek{s}(\alpha) \\
  \vek{r}(\alpha) &\coloneqq (\mat{A}\vek{x}(\alpha) - \vek{b}, \mat{A}^T \vek{y}(\alpha) + \vek{s}(\alpha) - \vek{c}).
\end{align}
Using this notation~\textcite{Monteiro-ConvergenceAnalysisLongStepInfeasibleIPMs} showed that the stepsize \(\bar{\alpha}\) determined in \cref{alg:ipm} is bounded from below.

\begin{lemma}[Lemma 3.6 in~\cite{Monteiro-ConvergenceAnalysisLongStepInfeasibleIPMs}]\label{thm:alpha-bar-bound}
  Assume that 
  \begin{itemize}
    \item \(\gamma \in (0, 1)\), \(\sigma \in (0, \frac{4}{5})\),
    \item \((\vek{x}, \vek{y}, \vek{s}) \in \mathcal{N}_{-\infty}(\gamma)\) and
    \item \((\hat{\Delta\vek{x}}, \hat{\Delta\vek{y}}, \hat{\Delta\vek{s}})\) satifies \cref{eqn:approx-newton} such that \(\norm{\vek{v}}_\infty \leq \gamma \sigma \mu / 4\).
  \end{itemize}
  Then the stepsize \(\bar{\alpha}\) determined in \cref{line:alpha-tilde,line:alpha-bar} satifies
  \[ \bar{\alpha} \geq \min \Set{1, \frac{\min \Set{\gamma \sigma, 1 - \frac{5}{4}\sigma} \mu}{4 \norm{\hat{\Delta\vek{x}} \circ \hat{\Delta\vek{s}}}_\infty}} \]
  and
  \[ \mu(\bar{\alpha}) \leq \Paren{1 - \Paren{1 - \frac{5}{4}\sigma} \frac{\bar{\alpha}}{2}} \mu. \]
\end{lemma}

Note that the assumptions in this lemma are not stated in this form in the original lemma but are instead inferred from the definition of their algorithm.
In the proof they show that \(\norm{\vek{v}}_\infty \leq \gamma \sigma \mu / 4\) and then deduce the results from this.
It suffices now to upper-bound \(\norm{\hat{\Delta\vek{x}} \circ \hat{\Delta\vek{s}}}_\infty\) to show that \(\mu\) decreases enough in each iteration.
To show this we need a key observation which motivated us to shift the error term in \cref{chap:ipm} using the perturbation vector \(\vek{v}\):
The form of the error term in \cref{eqn:approx-newton} ensures that at each iteration the residuals \(\vek{r} = (\vek{r}_p, \vek{r}_d)\) lie on the line segment between \(\vek{r}^0\) and \(\vek{0}\)
because \(\vek{r}(\alpha) = (1 - \alpha) \vek{r}(0)\).
In other words, \(\vek{r} = \eta \vek{r}^0\) with \(\eta \in [0, 1]\) for every iterate in \cref{alg:ipm}.

\begin{lemma}[Lemma 16 in~\cite{Avron-FasterRandomizedInfeasibleIPMs}]\label{thm:delta-x-s-bound}
  Assume that
  \begin{itemize}
    \item \(\gamma \in (0, 1)\), \(\sigma \in (0, 1)\), 
    \item \((\vek{x}^0, \vek{y}^0, \vek{s}^0) \in \mathcal{G}\) satisfies \((\vek{x}^0, \vek{s}^0) \geq (\vek{x}^*, \vek{s}^*)\) for some \((\vek{x}^*, \vek{y}^*, \vek{s}^*) \in \mathcal{F}^*\), 
    \item \((\vek{x}, \vek{y}, \vek{s}) \in \mathcal{N}_{-\infty}(\gamma)\) with \(\vek{r} = \eta \vek{r}^0\) for some \(\eta \in [0, 1]\) and
    \item \((\hat{\Delta\vek{x}}, \hat{\Delta\vek{y}}, \hat{\Delta\vek{s}})\) satifies \cref{eqn:approx-newton} such that \(\norm{\vek{v}}_2 \leq \gamma \sigma \mu / 4\).
  \end{itemize}
  Then
  \[ \norm{\mat{D}^{-1}\hat{\Delta\vek{x}}}_2, \norm{\mat{D}\hat{\Delta\vek{s}}}_2 \leq \Paren{1 + \frac{\sigma^2}{1- \gamma} - 2\sigma}^{1/2} \sqrt{n \mu} + \frac{6}{\sqrt{1 - \gamma}} n\sqrt{\mu} + \frac{\gamma \sigma}{4 \sqrt{1 - \gamma}}\sqrt{\mu}\]
  which implies that \(\norm{\mat{D}^{-1}\hat{\Delta\vek{x}}}_2\) and \(\norm{\mat{D}\hat{\Delta\vek{s}}}_2\) are bounded by \(C n \sqrt{\mu}\) for some constant \(C > 0\) depending on \(\gamma\) and \(\sigma\).
\end{lemma}

These two lemmas are enough to show the convergence of \cref{alg:ipm}.

\begin{proof}[Proof of \cref{thm:ipm-convergence}]
  By the choice of the initial point \((\vek{x}^0, \vek{y}^0, \vek{s}^0) \in \mathcal{N}_{-\infty}(\gamma)\) and \(\bar{\alpha}\) in \cref{line:alpha-tilde,line:alpha-bar} every iterate \((\vek{x}^k, \vek{y}^k, \vek{s}^k)\) is inside the specified neighbourhood \(\mathcal{N}_{-\infty}(\gamma)\).
  Therefore, \(\mu^k \leq \e_\mu \mu^0\) implies \(\norm{\vek{r}^k} \leq \e_\mu \norm{\vek{r}^0}\) and it suffices to show the former for some \(k = O(n^2 \log(1/\e_\mu))\).

  By \cref{thm:delta-x-s-bound} the search direction determined in \cref{line:compute-approx-newton} satisfies
  \[ \norm{\hat{\Delta\vek{x}} \circ \hat{\Delta\vek{s}}}_\infty \leq \norm{\hat{\Delta\vek{x}} \circ \hat{\Delta\vek{s}}}_2 \leq \norm{\mat{D}^{-1}\hat{\Delta\vek{x}}}_2 \norm{\mat{D}\hat{\Delta\vek{s}}}_2 \leq C_1 n^2 \mu^k\]
  in every iteration, such that by \cref{thm:alpha-bar-bound}
  \[ \bar{\alpha} \geq \min \Set{1, \frac{\min \Set{\gamma \sigma, 1 - \frac{5}{4}\sigma} \mu^k}{4 C_1 n^2 \mu^k}} \geq \frac{C_2}{n^2} \]
  where \(C_1, C_2 > 0\) are constants depending only on \(\gamma\) and \(\sigma\).
  Plugging this inequality in the second bound in \cref{thm:alpha-bar-bound} we get
  \[ \mu^{k+1} = \mu(\bar{\alpha}) \leq \Paren{ 1 - \Paren{1 - \frac{5}{4} \sigma} \frac{\bar{\alpha}}{2}} \mu^k \leq \Paren{ 1 - \Paren{1 - \frac{5}{4} \sigma} \frac{C_2}{2 n^2}} \mu^k \leq \Paren{1 - \frac{C_3}{n^2}} \mu^k \]
  for some constant \(C_3 > 0\).
  By induction this means that \(\mu^k \leq {(1 - \frac{C_3}{n^2})}^k \mu^0\) for all \(k \geq 0\).
  If \(\e_\mu \geq 1\) the algorithm terminates instantly so we can assume that \(\e_\mu < 1\).
  Now let \(k \geq n^2 \log(1/\e_\mu) / C_3\) which implies
  \[ k \log\Paren{1 - \frac{C_3}{n^2}} \leq k \Paren{- \frac{C_3}{n^2}} \leq -\log(1/\e_\mu) = \log(\e_\mu) \]
  using \(\log(\beta) \leq \beta-1\) for all \(\beta > 0\).
  Applying the exponential function on both sides gives \({(1 - \frac{C_3}{n^2})}^k \leq \e_\mu\) which shows that \(O(n^2 \log(1/\e_\mu))\) iterations suffice to get \(\mu^k \leq \e_\mu \mu^0\).
\end{proof}

Next, we turn to the correctness of \cref{alg:newton-direction} which is concerned with calculating a sufficiently good approximation of the Newton direction.
The proofs follow Lemma 11 to 13 in~\cite{Avron-FasterRandomizedInfeasibleIPMs} with slight simplifications.
As linear convergence of the CG method was already established in \cref{thm:cg-residual-bound} our main goal is to bound the norm of the initial residual \(\mat{R}^{-T}\vek{p}\).
To this end, we introduce the following bound that can already be found in~\cite{Monteiro-ConvergenceAnalysisLongStepInfeasibleIPMs}.

\begin{lemma}[Lemma 3.2 in~\cite{Monteiro-ConvergenceAnalysisLongStepInfeasibleIPMs}, Lemma 11 in~\cite{Avron-FasterRandomizedInfeasibleIPMs}]\label{thm:eta-bound}
  Assume that
  \begin{itemize}
    \item \((\vek{x}^0, \vek{y}^0, \vek{s}^0) \in \mathcal{G}\) satisfies \((\vek{x}^0, \vek{s}^0) \geq (\vek{x}^*, \vek{s}^*)\) for some \((\vek{x}^*, \vek{y}^*, \vek{s}^*) \in \mathcal{F}^*\) and
    \item \((\vek{x}, \vek{y}, \vek{s}) \in \mathcal{G}\) satifies \(\vek{r} = \eta \vek{r}^0\) for some \(\eta \in [0, 1]\).
  \end{itemize}
  Then \(\eta \leq \vek{x}^T \vek{s} / {\vek{x}^0}^T \vek{s}^0\) implies \(\eta ({\vek{x}^0}^T \vek{s} + \vek{x}^T {\vek{s}^0}) \leq 3 n \mu\).
  % Assume that the point \((\vek{x}^0, \vek{y}^0, \vek{s}^0) \in \mathcal{G}\) satisfies \((\vek{x}^0, \vek{s}^0) \geq (\vek{x}^*, \vek{s}^*)\) for some \((\vek{x}^*, \vek{y}^*, \vek{s}^*) \in \mathcal{F}^*\).
  % Then for any point \((\vek{x}, \vek{y}, \vek{s}) \in \mathcal{G}\) such that the corresponding residuals satisfy \(\vek{r} = \eta \vek{r}^0\) for some \(\eta \in [0, 1]\) and \(\eta \leq \vek{x}^T \vek{s} / {\vek{x}^0}^T \vek{s}^0\) we have
  % \(\eta ({\vek{x}^0}^T \vek{s} + \vek{x}^T {\vek{s}^0}) \leq 3 n \mu\).
\end{lemma}

Again this makes use of the observation that the residual \(\vek{r}\) of any iterate lies on the line segment between \(\vek{r}^0\) and \(\vek{0}\).
Note that additionally by the definition of the neighbourhood \(\mathcal{N}_{-\infty}(\gamma)\) any point in it with \(\vek{r} = \eta \vek{r}^0\) also satisifies \(\eta \leq \mu^k / \mu^0 = {\vek{x}^k}^T \vek{s}^k / {\vek{x}^0}^T \vek{s}^0\).

\begin{lemma}[Lemma 12 in~\cite{Avron-FasterRandomizedInfeasibleIPMs} (slightly simplified)]\label{thm:initial-residual-bound}
  Assume that
  \begin{itemize}
    \item \(\gamma \in (0, 1)\), \(\sigma \in (0, 1)\), \((\vek{x}^*, \vek{y}^*, \vek{s}^*) \in \mathcal{F}^*\),
    \item \((\vek{x}^0, \vek{y}^0, \vek{s}^0) \in \mathcal{G}\) satisfies \(\vek{x}^0 \geq \vek{x}^*\) and \(\vek{s}^0 \geq \max\set{\vek{s}^*, \abs{\mat{A}^T\vek{y}^0 - \vek{c}}}\)
    \item \((\vek{x}, \vek{y}, \vek{s}) \in \mathcal{N}_{-\infty}(\gamma)\) satisfies \(\vek{r} = \eta \vek{r}^0\) for some \(\eta \in [0, 1]\) and
    \item \(\mat{R}^{-T} \in \R^{m \times m}\) satisifies \(\norm{\mat{R}^{-T}\mat{A}\mat{D}}_2 \leq \sqrt{2}\).
  \end{itemize}
  Then
  \[ \norm{\mat{R}^{-T} \vek{p}}_2 \leq \sqrt{\frac{2 \mu}{1 - \gamma}} \Paren{10n + \sigma \sqrt{n}} \leq 11 \sqrt{\frac{2}{1 - \gamma}} n \sqrt{\mu}. \]
\end{lemma}

\begin{proof}
  First, we will deduce bounds for \(\eta\mat{S}(\vek{x}^0 - \vek{x}^*)\) and \(\eta\mat{X}\vek{r}_d^0\) from \cref{thm:eta-bound} that we need later.
  Note that because \(\vek{0} \leq \vek{x}^* \leq \vek{x}^0\) and \(\vek{s} \geq \vek{0}\) we have \(\vek{0} \leq \mat{S}(\vek{x}^0 - \vek{x}^*) \leq \mat{S}\vek{x}^0 \), so
  \[
    \eta \norm{\mat{S}(\vek{x}^0 - \vek{x}^*)}_2
    \leq \eta \norm{\mat{S}\vek{x}^0}_2
    \leq \eta \norm{\mat{S}\vek{x}^0}_1
    = \eta \vek{s}^T \vek{x}^0
    \leq \eta ({\vek{x}^0}^T \vek{s} + \vek{x}^T {\vek{s}^0})
    \leq 3 n \mu.
  \]
  Using a similar argument as above, \(\vek{0} \leq \abs{\mat{A}^T \vek{y}^0 - \vek{c}} \leq \vek{s}^0\) implies \(\norm{\mat{X} \abs{\mat{A}^T \vek{y}^0 - \vek{c}}}_2 \leq \norm{\mat{X}\vek{s}^0}_2\) and therefore
  \begin{align*}
    \eta \norm{\mat{X}\vek{r}_d^0}_2
    &= \eta \norm{\mat{X}(\mat{A}^T \vek{y}^0 + \vek{s}^0 - \vek{c})}_2
    \leq \eta \Paren{\norm{\mat{X}\vek{s}^0}_2 + \norm{\mat{X} \abs{\mat{A}^T \vek{y}^0 - \vek{c}}}_2} \\
    &\leq 2 \eta \norm{\mat{X}\vek{s}^0}_2
    \leq 2 \eta \norm{\mat{X}\vek{s}^0}_1
    = 2 \eta \vek{x}^T \vek{s}^0
    \leq 2 \eta ({\vek{x}^0}^T \vek{s} + \vek{x}^T {\vek{s}^0})
    \leq 6n\mu.
  \end{align*}
  Now, consider the expression in the claim. Because
  \begin{align*}
    \mat{R}^{-T}\vek{p}
    &= \mat{R}^{-T}\Paren{-\vek{r}_p + \mat{A} ( -\mat{S}^{-1} \mat{X} \vek{r}_d + \vek{x} - \sigma \mu \mat{S}^{-1} \vek{1} )} \\
    &= \mat{R}^{-T}\Paren{-\vek{r}_p -\mat{A}\mat{S}^{-1}\mat{X} \vek{r}_d + \mat{A}\vek{x} - \sigma \mu \mat{A}\mat{S}^{-1} \vek{1} } \\
    &= \mat{R}^{-T}\Paren{-\eta \vek{r}_p^0 - \eta \mat{A}\mat{S}^{-1}\mat{X} \vek{r}_d^0 + \mat{A}\vek{x} - \sigma \mu \mat{A}\mat{S}^{-1} \vek{1} } \\
    &= \mat{R}^{-T}\Paren{- \eta \mat{A}(\vek{x}^0 - \vek{x}^*) - \eta \mat{A}\mat{S}^{-1}\mat{X} \vek{r}_d^0 + \mat{A}\vek{x} - \sigma \mu \mat{A}\mat{S}^{-1} \vek{1} } \\
    &= \mat{R}^{-T}\mat{A}\Paren{- \eta (\vek{x}^0 - \vek{x}^*) - \eta \mat{S}^{-1}\mat{X} \vek{r}_d^0 + \vek{x} - \sigma \mu \mat{S}^{-1} \vek{1} } \\
    &= \mat{R}^{-T}\mat{A}\mat{S}^{-1}\Paren{- \eta \mat{S}(\vek{x}^0 - \vek{x}^*) - \eta \mat{X} \vek{r}_d^0 + \mat{S}\vek{x} - \sigma \mu \vek{1} } \\
    &= \mat{R}^{-T}\mat{A}\mat{D}{(\mat{S}\mat{X})}^{-1/2}\Paren{- \eta \mat{S}(\vek{x}^0 - \vek{x}^*) - \eta \mat{X} \vek{r}_d^0 + \mat{S}\vek{x} - \sigma \mu \vek{1} }
  \end{align*}
  using the definition of \(\vek{p}\) from \cref{eqn:normal-equation} and the assumption \(\vek{r} = \eta \vek{r}^0\), it suffices to bound each part of the last expression.
  \(\norm{\mat{R}^{-T}\mat{A}\mat{D}}_2 \leq \sqrt{2}\) by assumption,
  \[\norm{{(\mat{S}\mat{X})}^{-1/2}}_2 = \max_{1 \leq i \leq n} {(x_i s_i)}^{-1/2} \leq \sqrt{\frac{1}{(1 - \gamma) \mu}}\]
  by the definition of the neighbourhood and
  \(\norm{\mat{S}\vek{x}}_2 \leq \norm{\mat{S}\vek{x}}_1 = n \mu\)
  using the definition of \(\mu\).
  The two terms containing \(\eta\) were already bounded above and lastly, \(\norm{\vek{1}}_2 = \sqrt{n}\).
  This implies
  \begin{align*}
    \norm{\mat{R}^{-T} \vek{p}}_2 
    &\leq \norm{\mat{R}^{-T}\mat{A}\mat{D}}_2 \norm{{(\mat{S}\mat{X})}^{-1/2}}_2 \Paren{\eta \norm{\mat{S}(\vek{x}^0 - \vek{x}^*)} + \eta \norm{\mat{X}\vek{r}_d^0} + \norm{\mat{S}\vek{x}}_2 + \sigma \mu \norm{\vek{1}}_2} \\
    &\leq \sqrt{2} \sqrt{\frac{1}{(1-\gamma) \mu}} \Paren{3n\mu + 6n\mu + n\mu + \sigma \mu \sqrt{n}}
    = \sqrt{\frac{2 \mu}{1 - \gamma}} \Paren{10 n + \sigma \sqrt{n}}
  \end{align*}
  as claimed.
\end{proof}

% \Cref{thm:ipm-convergence} is concerned with the convergence and runtime of \cref{alg:ipm} while \cref{thm:approximate-newton-convergence} shows the correctness and runtime of \cref{alg:newton-direction}.
% The two algorithms and theorems can be combined by using \cref{alg:newton-direction} in \cref{line:compute-approx-newton} of \cref{alg:ipm} which yields the following theorem.

% \begin{theorem}
%   Assume that \(\gamma \in (0, 1)\) and \(\sigma \in (0, \frac{4}{5})\) are constant and that the initial point \((\vek{x}^0, \vek{y}^0, \vek{s}^0) \in \mathcal{G}\) satisfies \(\vek{x}^0 \circ \vek{s}^0 \geq (1-\gamma)\vek{1}\) and \((\vek{x}^0, \vek{s}^0) \geq (\vek{x}^*, \vek{s}^*)\) for some \((\vek{x}^*, \vek{y}^*, \vek{s}^*) \in \mathcal{F}^*\).
%   Furthermore, assume that \cref{alg:ipm} uses \cref{alg:newton-direction} with \(\e_{\vek{v}} = \gamma \sigma \mu^k / 4\) to determine the approximate Newton direction in every step.
%   Then the algorithm terminates successfully using only \(O(???)\) flops with probability at least \(99\%\) for \(\delta = O(n^{-2})\).
%  \end{theorem}
