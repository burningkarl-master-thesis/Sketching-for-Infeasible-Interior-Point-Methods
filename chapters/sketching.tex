\chapter{Sketching -- A Useful Dimensionality Reduction Technique}\label{chap:sketching}

Based on~\cite{Avron-FasterRandomizedInfeasibleIPMs} we will use sketching to determine a preconditioner for \(\mat{A} \mat{D}^2 \mat{A}^T\).
These results not only apply to this specific matrix but all symmetric positive definite matrices of the form \(\mat{B}^T \mat{B}\) for some \(\mat{B} \in \R^{n \times m}\) with \(m \ll n\).
To emphasise this, the theorems will all involve a general matrix \(\mat{B}\) and their consequences for \(\mat{A}\mat{D}^2\mat{A}^T\) will be discussed afterwards.

\section{Overview and a Sparse Sketching Result}\label{sec:sketching-overview}

The following introduction to sketching is based on~\cite{Woodruff-Sketching} and begins with the central concept of a subspace embedding:
\begin{definition}\label{def:subspace-embedding}
Let \(\e \in (0, 1)\). A \emph{\((1 \pm \e)\) \(\ell_2\)-subspace embedding} for the column space of a matrix \(\mat{B} \in \R^{n \times m}\) is a matrix \(\mat{W} \in \R^{w \times n}\) such that
\[ (1-\e) \norm{\mat{B}\vek{z}}_2^2 \leq \norm{\mat{W}\mat{B}\vek{z}}_2^2 \leq (1+\e)\norm{\mat{B}\vek{z}}_2^2 \]
for all \(\vek{z} \in \R^m\).
\end{definition}
Note that this definition does not depend on a particular basis for the representation of the column space of \(\mat{B}\).
Nonetheless, we will often refer to \(\mat{W}\) as a \((1\pm\e)\) \(\ell_2\)-subspace embedding of \(\mat{B}\).
Intuitively, a subspace embedding of some \(m\)-dimensional subspace of \(\R^n\) is a linear map from the (large) space \(\R^n\) to a (smaller) space \(\R^w\) that approximately preserves all of the distances of the subspace.
There are multiple equivalent characterizations of subspace embeddings used in the literature.

\begin{theorem}
Assume that \(\mat{B} \in \R^{n \times m}\) has full column rank and let
\[\mathcal{S} = \set{ \mat{B}\vek{z} | \vek{z} \in \R^m } = \set{ \mat{U}\vek{z} | \vek{z} \in \R^m } \subset \R^n\]
for some matrix \(\mat{U} \in \R^{n \times m}\) with orthonormal columns.
Then the following statements are equivalent
\begin{enumerate}
  \item \(\mat{W}\) is a \((1\pm\e)\) \(\ell_2\)-subspace embedding for the column space of \(\mat{B}\)
  \item \((1-\e) \norm{\mat{B}\vek{z}}_2^2 \leq \norm{\mat{W}\mat{B}\vek{z}}_2^2 \leq (1+\e)\norm{\mat{B}\vek{z}}_2^2\) for all \(\vek{z} \in \R^d\)
  \item \(\abs{\norm{\mat{W}\vek{s}}_2^2 - 1} \leq \e\) for all \(\vek{s} \in \mathcal{S}\) with \(\norm{\vek{s}}_2 = 1\)
  \item \(\norm{\mat{U}^T \mat{W}^T \mat{W} \mat{U} - \mat{I}_d}_2 \leq \e\)
\end{enumerate}
\end{theorem}
\begin{proof}
The first equivalence holds by definition. Statement 3 is equivalent to
\[ 1-\e \leq \norm{\mat{W}\mat{s}}_2^2 \leq 1+\e \quad \forall \vek{s} \in \mathcal{S} \text{ with } \norm{\vek{s}}_2 = 1. \]
In this form statement 2 immediately implies 3 and choosing \(\vek{s} = \norm{\mat{B}\vek{z}}_2^{-1} \mat{B}\vek{z}\) for \(\mat{B}\vek{z} \neq \vek{0}\) and multiplying both sides by \(\norm{\mat{B}\vek{z}}_2^2\) we obtain that the two statements are indeed equivalent.

Note that the matrix \(\mat{U}^T \mat{W}^T \mat{W} \mat{U} - \mat{I}_d\) is real and symmetric and therefore its 2-norm is given by the maximum absolute value of the Rayleigh quotient~\cite[Equation (5.2)]{Demmel-AppliedNumericalLinearAlgebra}.
Using \(\norm{\mat{U}\vek{e}}_2 = \norm{\vek{e}}_2\), this implies
\begin{align*}
  \norm{\mat{U}^T \mat{W}^T \mat{W} \mat{U} - \mat{I}_d}_2
  &= \max_{\vek{e} \in \R^d, \norm{\vek{e}}_2 = 1} \Abs{\vek{e}^T \Paren{\mat{U}^T \mat{W}^T \mat{W} \mat{U} - \mat{I}_d} \vek{e} } \\
  &= \max_{\vek{e} \in \R^d, \norm{\vek{e}}_2 = 1} \Abs{{(\mat{U}\vek{e})}^T \mat{W}^T \mat{W} (\mat{U}\vek{e}) - \vek{e}^T \vek{e}} \\
  &= \max_{\vek{s} \in \mathcal{S}, \norm{\vek{s}}_2 = 1} \Abs{\vek{s}^T \mat{W}^T \mat{W} \vek{s} - 1} \\
  &= \max_{\vek{s} \in \mathcal{S}, \norm{\vek{s}}_2 = 1} \Abs{\norm{\mat{W}\vek{s}}_2^2 - 1}.
\end{align*}
and proves the equivalence of statements 3 and 4.
\end{proof}

Using the QR decomposition of \(\mat{B}\) it is always possible to design a \enquote{perfect} subspace embedding with \(\e = 0\) and \(w = m\)
but computing this QR decomposition is expensive.
Instead, we can use random matrices \(\mat{W}\) which have a high chance of being a subspace embedding.

\begin{definition}\label{def:oblivious-subspace-embedding}
Let \(m, n, w \in \N\) and \(\delta, \e \in (0, 1)\) be given.
A probability distribution \(\Pi\) over matrices \(\mat{W} \in \R^{w \times n}\) is an \emph{oblivious \(\ell_2\)-subspace embedding} (OSE) if, for any matrix \(\mat{B} \in \R^{n \times m}\) a matrix \(\mat{W}\) drawn from \(\Pi\) is a \((1\pm\e)\) \(\ell_2\)-subspace embedding with probability at least \(1 - \delta\).
\end{definition}

Because we will later compute a decomposition of the sketched matrix \(\mat{W}\mat{B}\) we want \(w\) to be as small as possible.
A classic result in sketching is that there are OSEs with \(w = O((m + \log(1/\delta))\e^{-2})\) without any dependence on \(n\).
\Textcite[Theorem 6]{Woodruff-Sketching} shows how this follows from the so-called Johnson-Lindenstrauss Lemma and that it can be achieved by choosing the matrix entries to be properly normalized independent Gaussian random variables.
Unfortunately, a matrix \(\mat{W}\) with Gaussian entries is dense so computing the matrix product \(\mat{W}\mat{B}\) takes \(O(w n m)\) operations in general.
If the matrix \(\mat{B}\) is sparse we need an OSE where all sampled matrices are sparse to take advantage of that.
The following sparse OSE result is taken from~\cite{Cohen-NearlyTightObliviousSubspaceEmbeddings}.
\Textcite{Avron-FasterRandomizedInfeasibleIPMs} use the sketching result from~\cite{Cohen-OptimalApproximateMatrixProduct} but we replaced it by a simpler one giving the same complexity guarantees in the end.

\begin{definition}
An \emph{\((s, w)\)-sparse embedding matrix} \(\mat{W} \in \R^{w \times n}\) is constructed in the following way:
For each column \(s\) entries are sampled randomly without replacement and for each such entry its value is randomly drawn from \(\{ - \frac{1}{\sqrt{s}}, \frac{1}{\sqrt{s}} \}\).
All other entries of \(\mat{W}\) are set to zero.
This defines a probability distribution over \(\R^{w \times n}\) where each sampled matrix has exactly \(s \cdot n\) nonzero entries.
\end{definition}

\begin{theorem}[Theorem 4.2 in~\cite{Cohen-NearlyTightObliviousSubspaceEmbeddings}]\label{thm:sparse-ose}
For any \(\delta < 1/2, \ \e < 1/2\) and \(m, n \in \N\) there exist \(s, w \in \N\) with
\[ w = O \Paren{\frac{m \log(m/\delta)}{\e^2}} \quad \text{and} \quad s = O \Paren{\frac{\log(m/\delta)}{\e}} \]
such that the \((s, w)\)-sparse embedding matrices form an oblivious \(\ell_2\)-subspace embedding.
\end{theorem}

\section{Sketching-Based Preconditioning}\label{sec:sketching-preconditioning}

To simplify the runtime bounds and the following discussion we will from now on fix \(\e = 1/3 < 1/2\) and furthermore assume that \(\mat{W} \in \R^{w \times n}\) is a sparse embedding matrix and a \((1\pm\e)\) \(\ell_2\)-subspace embedding of the column space of \(\mat{B} \in \R^{n \times m}\).
From the previous result we get that choosing
\begin{equation} \label{eqn:sparse-ose-params}
  w = O \Paren{m \log(m/\delta)} \quad \text{and} \quad s = O \Paren{\log(m/\delta)}
\end{equation}
suffices to guarantee that the latter assumption holds with probability at least \(1 - \delta\).
After we obtain a bound on the number of IPM iterations we will choose \(\delta\) in such a way that the failure probability over all iterations is low.

How will these sketching results help with our goal of solving \(\mat{A} \mat{D}^2 \mat{A}^T \Delta\vek{y} = \vek{p}\) with an iterative method?
We will first show how a decomposition of the sketched matrix \(\mat{W}\mat{B}\) can be used to determine a two-sided preconditioner for \(\mat{B}^T \mat{B}\) which reduces the condition number down to a constant and then see how this can be applied to the equation above.

\Textcite{Avron-FasterRandomizedInfeasibleIPMs} use an SVD of the sketched matrix \(\mat{W}\mat{B}\) to get a two-sided preconditioner but we decided to go with the more practical choice of constructing it using the QR decomposition \(\mat{Q}\mat{R} = \mat{W}\mat{B}\).
Here \(\mat{R} \in \R^{m \times m}\) is upper triangular and \(\mat{Q} \in \R^{w \times m}\) has orthonormal columns.
To ensure that the QR decomposition exists we need that \(\mat{B}\) has full column rank. Then, because \(\e < 1\) none of the elements in the column space of \(\mat{B}\) can be in the nullspace of \(\mat{W}\) so \(\rank(\mat{W}\mat{B}) = m\).
The proposed preconditioned matrix has the form
\begin{equation}\label{eqn:preconditioned-matrix}
 \mat{R}^{-T} \mat{B}^T \mat{B} \mat{R}^{-1}.
\end{equation}
Note that applying the preconditioner from the left and from the right in this way ensures that the matrix stays symmetric positive definite and we can apply the CG algorithm to any linear system involving this matrix.
Additionally, because \(\mat{R}\) is upper triangular multiplying \(\mat{R}^{-1}\) or \(\mat{R}^{-T}\) with a vector can be done efficiently using a triangular solve.
The following theorem shows that this preconditioner leads to bounds on the condition number of the matrix which can be used to bound the number of CG iterations needed to reach a certain error estimate.

\begin{lemma}\label{thm:condition-number-bound}
Suppose \(\mat{W}\) is a \((1\pm\e)\) \(\ell_2\)-subspace embedding for \(\mat{B}\) for \(\e = 1/3\) and \(\mat{Q}\mat{R}\) is the QR decomposition of \(\mat{W}\mat{B}\).
The singular values of \(\mat{R}^{-T}\mat{B}^T\) and \(\mat{B}\mat{R}^{-1}\) are bounded by
\[ \frac{3}{4} = \frac{1}{1 + \e} \leq \sigma_i^2(\mat{R}^{-T}\mat{B}^T) = \sigma_i^2(\mat{B}\mat{R}^{-1}) \leq \frac{1}{1 - \e} = \frac{3}{2} \quad \text{for } i = 1, \ldots, m \]
which implies that the condition number of their product \(\mat{R}^{-T} \mat{B}^T \mat{B} \mat{R}^{-1}\) is bounded by \((1+\e) / (1-\e) = 2\).
\end{lemma}
\begin{proof}
The matrices \(\mat{R}^{-T}\mat{B}^T\) and \(\mat{B}\mat{R}^{-1}\) are transposes of each other and hence have the same singular values.
By the Courant-Fisher minmax theorem (e.g.~\cite[Theorem 3.1.2]{HornJohnson-TopicsInMatrixAnalysis})
the minimum and maximum squared singular values of \(\mat{B}\mat{R}^{-1}\) are the same as the minimum and maximum values of \( \norm{\mat{B}\mat{R}^{-1} \tilde{\vek{z}}}_2^2 / \norm{\tilde{\vek{z}}}_2^2\) over all \(\tilde{\vek{z}} \in \R^n\).
By \cref{def:subspace-embedding} applied to \(\vek{z} = \mat{R}^{-1} \tilde{\vek{z}}\) we have
\[
  (1-\e) \norm{\mat{B}\mat{R}^{-1} \tilde{\vek{z}}}_2^2
  \leq \norm{\mat{W}\mat{B}\mat{R}^{-1}\tilde{\vek{z}}}_2^2
  = \norm{\mat{Q}\mat{R} \mat{R}^{-1}\tilde{\vek{z}}}_2^2
  = \norm{\mat{Q}\tilde{\vek{z}}}_2^2
  = \norm{\tilde{\vek{z}}}_2^2
\]
for all \(\tilde{\vek{z}} \in \R^m\) and similarly \((1+\e)\norm{\mat{B}\mat{R}^{-1} \tilde{\vek{z}}}_2^2 \geq \norm{\tilde{\vek{z}}}_2^2\).
Therefore
\[ \frac{1}{1 + \e} \leq \frac{\norm{\mat{B}\mat{R}^{-1} \tilde{\vek{z}}}_2^2}{\norm{\tilde{\vek{z}}}_2^2} \leq \frac{1}{1 - \e} \]
which proves the bounds on the singular values.
The condition number of a matrix is defined as the largest singular value divided by the smallest singular value.
Because \(\mat{R}^{-T} \mat{B}^T \mat{B} \mat{R}^{-1} = {(\mat{B}\mat{R}^{-1})}^T (\mat{B}\mat{R}^{-1})\) the singular values of this matrix are just the squares of the singular values of \(\mat{B}\mat{R}^{-1}\), so
\[ \kappa_2(\mat{R}^{-T} \mat{B}^T \mat{B} \mat{R}^{-1})
   = \frac{\sigma_{\max}^2(\mat{B}\mat{R}^{-1})}{\sigma_{\min}^2(\mat{B}\mat{R}^{-1})} \leq \frac{1+\e}{1-\e} \]
and we are done.
\end{proof}

\begin{lemma}\label{thm:cg-residual-bound}
Suppose \(\mat{W}\) is a \((1\pm\e)\) \(\ell_2\)-subspace embedding for \(\mat{B}\) for \(\e = 1/3\) and \(\mat{Q}\mat{R}\) is the QR decomposition of \(\mat{W}\mat{B}\).
Let \(\vek{q}^j\) be the iterates when solving \[ \mat{R}^{-T} \mat{B}^T \mat{B} \mat{R}^{-1} \vek{q} = \mat{R}^{-T} \vek{p} \] with the CG algorithm and let \(\vek{f}^j\) be the residuals defined by
\( \vek{f}^j = \mat{R}^{-T} \mat{B}^T \mat{B} \mat{R}^{-1} \vek{q}^j - \mat{R}^{-T} \vek{p} \).
Then the \(\ell_2\)-norm of the residuals converges at least linearly to zero:
\[ \norm{\vek{f}^{j+1}}_2 \leq \frac{1}{2} \norm{ \vek{f}^j }_2 \text{ for all } j \in \N \]
\end{lemma}
\begin{proof}
Theorem 8 of~\cite{Bouyouli-ConjugateGradientConvergence} showed that
\[ \norm{\vek{f}^{j+1}}_2 \leq \frac{\kappa_2(\mat{R}^{-T} \mat{B}^T \mat{B} \mat{R}^{-1}) - 1}{2} \norm{\vek{f}^j}_2 .\]
Using the bound on the condition number from \cref{thm:condition-number-bound} we obtain
\[ \frac{\kappa_2(\mat{R}^{-T} \mat{B}^T \mat{B} \mat{R}^{-1}) - 1}{2}
   \leq \frac{2 - 1}{2} = \frac{1}{2} \]
which proves the claim.
\end{proof}

The previous two theorems show that computing the QR decomposition of a sketched matrix \(\mat{W}\mat{B}\) gives an effective way to solve any linear system of the form
\( \mat{B}^T \mat{B}\tilde{\vek{q}} = \vek{p} \)
by replacing it with
\begin{equation}\label{eqn:general-preconditioned-linear-system}
  \mat{R}^{-T}\mat{B}^T\mat{B}\mat{R}^{-1}\vek{q} = \mat{R}^{-T}\vek{p} \quad \text{and} \quad \tilde{\vek{q}} = \mat{R}^{-1}\vek{q}
\end{equation}
and solving the first part using the CG method.
Applying this technique to \cref{eqn:normal-equation} we get
\begin{equation}\label{eqn:preconditioned-normal-equation}
  \mat{R}^{-T}\mat{A}\mat{D}^2\mat{A}^T\mat{R}^{-1}\vek{q} = \mat{R}^{-T}\vek{p} \quad \text{and} \quad \Delta\vek{y} = \mat{R}^{-1}\vek{q}
\end{equation}
by setting \(\mat{B} = \mat{D}\mat{A}^T\).

\section{Choosing the Perturbation Vector}\label{sec:sektching-perturbation-vector}

Now as mentioned in \cref{chap:ipm} we need to define a perturbation vector \(\vek{v} \in \R^n\) to shift the error term in the Newton system.
Because we apply the CG method to \cref{eqn:preconditioned-normal-equation} an error term \(\vek{f} = \mat{R}^{-T}\mat{A}\mat{D}^2\mat{A}^T\mat{R}^{-1}\vek{q} - \mat{R}^{-T}\vek{p}\) propagates so that
\begin{equation}
  \mat{A} \mat{D}^2 \mat{A}^T \hat{\Delta\vek{y}} = \vek{p} + \mat{R}^T\vek{f}
\end{equation}
and the perturbation vector has to satisfy \(\mat{A}\mat{S}^{-1}\vek{v} = \mat{R}^T \vek{f}\) for a given \(\vek{f}\).
We use the method of \textcite{Avron-FasterRandomizedInfeasibleIPMs} to choose this vector by utilising the decomposition of the sketched matrix \(\mat{W}\mat{D}\mat{A}^T\) we have already determined.
This also allows us to bound the size of the perturbation vector by the size of \(\vek{f}\).

\begin{lemma}\label{thm:perturbation-vector}
Suppose \(\mat{W}\) is a \((s, w)\)-sparse embedding matrix and \(\mat{Q}\mat{R}\) is the QR decomposition of \(\mat{W}\mat{D}\mat{A}^T\).
Given a vector \(\vek{f} \in \R^m\) the vector
\[ \vek{v} = {(\mat{S}\mat{X})}^{1/2} \mat{W}^T \mat{Q} \vek{f} \]
satisfies \( \mat{A}\mat{S}^{-1}\vek{v} = \mat{R}^T \vek{f} \) and \(\norm{\vek{v}}_2 \leq n \sqrt{\mu} \norm{\vek{f}}_2\).
\end{lemma}
\begin{proof}
Using the definition of \(\mat{Q}\) and \(\mat{R}\) we obtain
\begin{align*}
  \mat{A} \mat{S}^{-1} \vek{v}
  &= \mat{A} \mat{S}^{-1} {(\mat{S}\mat{X})}^{1/2} \mat{W}^T \mat{Q} \vek{f}
   = \mat{A} \mat{D} \mat{W}^T \mat{Q} \vek{f} \\
  &= {(\mat{Q} \mat{R})}^T \mat{Q} \vek{f}
   = \mat{R}^T \mat{Q}^T \mat{Q} \vek{f}
   = \mat{R}^T \vek{f}
\end{align*}
which proves the first part.
To bound the norm consider
\[ 
  \norm{\vek{v}}_2
  = \norm{{(\mat{S}\mat{X})}^{1/2} \mat{W}^T \mat{Q} \vek{f}}_2
  \leq \norm{{(\mat{S}\mat{X})}^{1/2}}_2 \norm{\mat{W}^T}_2 \norm{\mat{Q} \vek{f}}_2.
\]
Now, \(\norm{{(\mat{S}\mat{X})}^{1/2}}_2 \leq \norm{{(\mat{S}\mat{X})}^{1/2}}_F = \sqrt{n \mu}\) by definition of \(\mu\) and \(\norm{\mat{Q}\vek{f}}_2 = \norm{\vek{f}}_2\) because \(\mat{Q}\) has orthonormal columns.
Lastly, we need to bound \(\norm{\mat{W}^T}_2\).
Remember that \(\mat{W}\) is constructed by choosing exactly \(s\) nonzero entries per column and each of these entries is drawn from \(\{-\frac{1}{\sqrt{s}}, \frac{1}{\sqrt{s}}\}\).
Let \(\mathcal{J}_i\) denote the set of indices of the nonzero entries of column \(i\) in \(\mat{W}\) and let \(\vek{z}_{\mathcal{J}_i} \in \R^s\) denote the restriction of \(\vek{z} \in \R^w\) to these indices.
Using standard norm inequalities
\begin{align*}
  \norm{\mat{W}^T \vek{z}}_2^2 &= \sum_{i=1}^{n} \Paren{ \sum_{j=1}^{w} \mat{W}_{ji} \vek{z}_j }^2 = \sum_{i=1}^{n} \Paren{ \sum_{j \in \mathcal{J}_i} \mat{W}_{ji} \vek{z}_j }^2 \\
  &\leq \sum_{i=1}^{n} \Paren{ \sum_{j \in \mathcal{J}_i} \abs{\mat{W}_{ji}} \abs{\vek{z}_j} }^2 = \sum_{i=1}^{n} \Paren{ \frac{1}{\sqrt{s}} \norm{\vek{z}_{\mathcal{J}_i}}_1 }^2 \\
  &\leq \sum_{i=1}^{n} \Paren{ \frac{1}{\sqrt{s}} \sqrt{s} \norm{\vek{z}_{\mathcal{J}_i}}_2 }^2 = \sum_{i=1}^{n} \norm{\vek{z}_{\mathcal{J}_i}}_2^2 \\
  &\leq n \norm{\vek{z}}_2^2
\end{align*}
which implies \(\norm{\mat{W}^T}_2 \leq \sqrt{n}\).
Combining these results,
\[ 
  \norm{\vek{v}}_2
  \leq \norm{{(\mat{S}\mat{X})}^{1/2}}_2 \norm{\mat{W}^T}_2 \norm{\mat{Q} \vek{f}}_2
  \leq \sqrt{n \mu} \sqrt{n} \norm{\vek{f}}_2 = n \sqrt{\mu} \norm{\vek{f}}_2
\]
as claimed.
\end{proof}

Note that compared to the corresponding result in Lemma 4 of~\cite{Avron-FasterRandomizedInfeasibleIPMs} the bound \(\norm{\vek{v}}_2 \leq n \sqrt{\mu} \norm{\vek{f}}_2\) is slightly worse but does not introduce any additional randomness and does not even depend on \(\mat{W}\) being a subspace embedding.
It is only assumed that the QR decomposition of \(\mat{W}\mat{D}\mat{A}^T\) exists.

\section{Computing an Approximate Newton Direction}\label{sec:sketching-approximate-newton}

The previous considerations give rise to \cref{alg:newton-direction} that tries to determine an approximate Newton direction as in \cref{eqn:perturbed-search-direction} with \(\norm{\vek{v}}_2 \leq \e_{\vek{v}}\).
The behavior of this algorithm if \(\mat{W}\) is not a subspace embedding for \(\mat{D}\mat{A}^T\) is not specified.
We will just assume that the algorithm fails if this is the case and discuss the details of handling this case in practice in \cref{chap:experiments}.

\begin{algorithm}
  \SetKwInOut{Input}{Input}
  \Input{
    \(\mat{A} \in \R^{m \times n}\) with rank \(m\),
    \(\vek{b} \in \R^m\), \(\vek{c} \in \R^n\),
    \(\vek{x}, \vek{s} \in \R^n_{>0}\),
    \(\vek{y} \in \R^m\),
    \(\sigma \in (0, 1)\),
    tolerance \(\e_{\vek{v}} > 0\),
    failure probability \(\delta \in (0, \frac{1}{2})\)}
  Compute \(\vek{r}_p = \mat{A}\vek{x} - \vek{b}\), \(\vek{r}_d = \mat{A}^T \vek{y} + \vek{s} - \vek{c}\), \(\mat{X} = \diag(\vek{x})\), \(\mat{S} = \diag(\vek{s})\)\;\label{line:compute-residuals}
  Compute \(\vek{p} = -\vek{r}_p + \mat{A}(-\mat{S}^{-1}\mat{X}\vek{r}_d + \vek{x} - \sigma \mu \mat{S}^{-1} \vek{1})\)\;\label{line:compute-p}
  Construct a random sparse embedding matrix \(\mat{W} \in \R^{w \times m}\) as in \cref{thm:sparse-ose} for the given \(\delta\) and \(\e = 1/3\)\;\label{line:draw-sketching-matrix}
  Compute the QR decomposition \(\mat{Q} \mat{R} = \mat{W} \mat{D} \mat{A}^T\)\;\label{line:compute-qr}
  Run the CG method with \(\vek{q}^0 = \vek{0}\) until \(\mat{R}^{-T} \mat{A} \mat{D}^2 \mat{A}^T \mat{R}^{-1} \vek{q} = \mat{R}^{-T} \vek{p} + \vek{f}\) for some error \(\vek{f}\) satisfying \(\norm{\vek{f}}_2 \leq \e_{\vek{v}}/(n \sqrt{\mu})\)\;\label{line:cg}
  Compute \(\vek{v} = {(\mat{S} \mat{X})}^{1/2} \mat{W}^T \mat{Q} \vek{f}\)\;\label{line:compute-v}
  Compute \(\hat{\Delta\vek{y}} = \mat{R}^{-1}\vek{q}\)\;\label{line:compute-delta-y}
  Compute \(\hat{\Delta\vek{s}} = -\vek{r}_d - \mat{A}^T \hat{\Delta\vek{y}}\)\;\label{line:compute-delta-s}
  Compute \(\hat{\Delta\vek{x}} = -\vek{x} + \sigma \mu \mat{S}^{-1} \vek{1} - \mat{S}^{-1}\mat{X} \hat{\Delta\vek{s}} - \mat{S}^{-1} \vek{v}\)\;\label{line:compute-delta-x}
  \Return{\((\hat{\Delta\vek{x}}, \hat{\Delta\vek{y}}, \hat{\Delta\vek{s}})\)}\;
  \caption{Approximate Newton direction}\label{alg:newton-direction}
\end{algorithm}

\begin{theorem}\label{thm:approximate-newton-convergence}
  Assume that \(\gamma \in (0, 1)\) and \(\sigma \in (0, 1)\) are constant, that the initial point \((\vek{x}^0, \vek{y}^0, \vek{s}^0) \in \mathcal{G}\) satisfies \(\vek{x}^0 \geq \vek{x}^*\) and \(\vek{s}^0 \geq \max \{\vek{s}^*, \abs{\mat{A}^T \vek{y}^0 - \vek{c}}\}\) for some \((\vek{x}^*, \vek{y}^*, \vek{s}^*) \in \mathcal{F}^*\) and that \(\mat{A}\) has full row rank.
  Let \((\vek{x}, \vek{y}, \vek{s}) \in \mathcal{N}_{-\infty}(\gamma)\) be such that \(\vek{r} = \eta \vek{r}^0\) for some \(\eta \in [0, 1]\).
  \cref{alg:newton-direction} computes an approximation of the Newton direction that satisfies \cref{eqn:perturbed-search-direction} with \(\norm{\vek{v}}_2 \leq \e_{\vek{v}}\) with probability at least \(1 - \delta\).
  Moreover, if the algorithm does not fail, the CG method only needs \(O(\log(n^2 \mu / \e_{\vek{v}}))\) CG iterations and the whole algorithm needs \(O(\log(m/\delta) (\nnz(\mat{A}) + m^3) + \log(n^2 \mu / \e_{\vek{v}})(\nnz(\mat{A}) + m^2))\) operations assuming \(n \leq \nnz(\mat{A})\).
\end{theorem}