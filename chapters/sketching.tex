\chapter{Sketching}

Important concepts to cover
\begin{enumerate}
  \item Goal of finding a preconditioner
  \item Definition of a \((1\pm\e)\)-subspace embedding
  \item Definition of an oblivious subspace embedding
  \item Equivalent statements for subspace embeddings
  \item State sparse sketching result
  \item Apply sparse sketching result to \((\mat{A}\mat{D})^T\) and show that \(\mat{R}^{-1}\) is a good two-sided preconditioner
\end{enumerate}

\hrule

Based on \cite{Avron-FasterRandomizedInfeasibleIPMs} we will use sketching to determine a preconditioner for \(\mat{A} \mat{D}^2 \mat{A}^T\) and iterative methods solve the normal equation. Furthermore, the solution gained by directly evaluating \cref{eqn:s-from-normal,eqn:x-from-normal} is perturbed to ensure any inaccuracies in \(\Delta\vek{y}\) only affect the last line of \cref{newton-system}.

The following introduction to sketching is based on \cite{Woodruff-Sketching}.

\begin{definition} \label{def:subspace-embedding}
Let \(\e \in (0, 1)\). A \uline{\((1 \pm \e)\) \(\ell_2\)-subspace embedding} for the column space of a matrix \(\mat{B} \in \R^{n \times d}\) is a matrix \(\mat{W} \in \R^{w \times n}\) such that
\[ (1-\e) \norm{\mat{B}\vek{x}}_2^2 \leq \norm{\mat{W}\mat{B}\vek{x}}_2^2 \leq (1+\e)\norm{\mat{B}\vek{x}}_2^2 \]
for all \(\vek{x} \in \R^d\).
\end{definition}
Note that this definition does not depend on a particular basis for the representation of the column space of \(\mat{B}\).
Nonetheless, we will often refer to \(\mat{W}\) as a \((1\pm\e)\)-subspace embedding of \(\mat{B}\).

\begin{theorem}
Assume that \(\mat{B}\) has full column rank and let \(\mathcal{S} = \set{ \mat{B}\vek{x} | \vek{x} \in \R^d } = \set{ \mat{U}\vek{x} | \vek{x} \in \R^d }\) for some matrix \(\mat{U} \in \R^{n \times d}\) with orthonormal columns.
Then the following statements are equivalent
\begin{enumerate}
  \item \(\mat{W}\) is a \((1\pm\e)\) \(\ell_2\)-subspace embedding for the column space of \(\mat{B}\)
  \item \((1-\e) \norm{\mat{B}\vek{x}}_2^2 \leq \norm{\mat{W}\mat{B}\vek{x}}_2^2 \leq (1+\e)\norm{\mat{B}\vek{x}}_2^2\) for all \(\vek{x} \in \R^d\)
  \item \(\abs{\norm{\mat{W}\vek{s}}_2^2 - 1} \leq \e\) for all \(\vek{s} \in \mathcal{S}\) with \(\norm{\vek{s}}_2 = 1\)
  \item \(\norm{\mat{U}^T \mat{W}^T \mat{W} \mat{U} - \mat{I}_d}_2 \leq \e\)
\end{enumerate}
\end{theorem}
\begin{proof}
The first equivalence holds by definition. Statement 3 is equivalent to
\[ 1-\e \leq \norm{\mat{W}\mat{s}}_2^2 \leq 1+\e \quad \forall \vek{s} \in \mathcal{S} \text{ with } \norm{\vek{s}}_2 = 1. \]
Choosing \(\vek{s} = \norm{\mat{B}\vek{x}}_2^{-1} \mat{B}\vek{x}\) and multiplying both sides by \(\norm{\mat{B}\vek{x}}_2^2\) we obtain that it is equivalent to statement 2.

%TODO Citation for linear algebra

Note that the matrix \(\mat{U}^T \mat{W}^T \mat{W} \mat{U} - \mat{I}_d\) is real and symmetric and therefore has real eigenvalues.
Moreover, its singular values are the absolute values of its eigenvalues.
The range of eigenvalues of a symmetric matrix \(\mat{A}\) is given by \(\set{ \vek{e}^T \mat{A} \vek{e} | \vek{e} \in \R^d, \norm{\vek{e}}_2 = 1 }\) [Citation] so
\begin{align*}
  \norm{\mat{U}^T \mat{W}^T \mat{W} \mat{U} - \mat{I}_d}_2
  &= \sigma_1(\mat{U}^T \mat{W}^T \mat{W} \mat{U} - \mat{I}_d) \\
  &= \max_{\vek{e} \in \R^d, \norm{\vek{e}}_2 = 1} \Abs{\vek{e}^T \Paren{\mat{U}^T \mat{W}^T \mat{W} \mat{U} - \mat{I}_d} \vek{e} } \\
  &= \max_{\vek{e} \in \R^d, \norm{\vek{e}}_2 = 1} \Abs{(\mat{U}\vek{e})^T \mat{W}^T \mat{W} (\mat{U}\vek{e}) - \vek{e}^T \vek{e}} \\
  &= \max_{\vek{s} \in \mathcal{S}, \norm{\vek{s}}_2 = 1} \Abs{\vek{s}^T \mat{W}^T \mat{W} \vek{s} - 1} \\
  &= \max_{\vek{s} \in \mathcal{S}, \norm{\vek{s}}_2 = 1} \Abs{\norm{\mat{W}\vek{s}}_2^2 - 1}.
\end{align*}
Using \(\norm{\mat{U}\vek{e}}_2 = \norm{\vek{e}}_2\), this proves the equivalence of statements 3 and 4.
\end{proof}

\begin{definition} \label{def:oblivious-subspace-embedding}
Let \(w, n, d \in \N\) and \(\e, \delta \in (0, 1)\) be given.
A probability distribution \(\Pi\) over matrices \(\mat{W} \in \R^{w \times n}\) is an \uline{oblivious \(\ell_2\)-subspace embedding} (OSE) if, for any matrix \(\mat{B} \in \R^{n \times d}\) a matrix \(\mat{W}\) drawn from \(\Pi\) is a \((1\pm\e)\) \(\ell_2\)-subspace embedding with probability at least \(1 - \delta\).
\end{definition}

Of course, these OSEs do not exist for all values of \(w\), \(n\) and \(d\).
In particular, we need \(w \geq d\) because otherwise some elements in the \(d\)-dimensional column space of \(\mat{B}\) will be in the nullspace of \(\mat{W}\).
If \(w \geq n\) we can trivially choose \(\mat{W}\) to have ones on the diagonal and zeros everywhere else, so we are interested in OSEs such that \(d \leq w \leq n\).
These reduce the size of our matrix \(\mat{B}\) while retaining most of the information in some sense.
We expect best results for \(d \ll n\) where we can hope for a large reduction in the matrix size.

A classic result in sketching is that matrices with Gaussian entries form an OSE if \(w\) is chosen appropriately depending on \(n\), \(d\), \(\e\) and \(\delta\), namely \(w = O((d + 1/\delta) \e^{-2})\) works.
%TODO Provide appropriate citation here and extend to distribution Johnson-Lindenstrauss

A matrix \(\mat{W}\) with Gaussian entries is dense so computing the matrix product \(\mat{W}\mat{B}\) takes \(O(w n d)\) operations in general.
If the matrix \(\mat{B}\) is sparse we need an OSE where all sampled matrices are sparse to take advantage of that.
The following sparse OSE result is taken from \cite{Cohen-NearlyTightObliviousSubspaceEmbeddings}.
\Textcite{Avron-FasterRandomizedInfeasibleIPMs} use the sketching result from \cite{Cohen-OptimalApproximateMatrixProduct} but for simplicity we replaced it by a simpler one giving the same complexity guarantees in the end(?).

\begin{definition}
A \uline{sparse embedding matrix} \(\mat{W} \in \R^{w \times n}\) is constructed in the following way:
For each column \(s\) entries are sampled randomly without replacement and for each such entry its value is randomly drawn from \(\{ - \frac{1}{\sqrt{s}}, \frac{1}{\sqrt{s}} \}\).
All other entries of \(\mat{W}\) are set to zero.
This defines a probability distribution over \(\R^{w \times n}\) where each sampled matrix has exactly \(s \cdot n\) nonzero entries.
\end{definition}

\begin{theorem}[Theorem 4.2 in \cite{Cohen-NearlyTightObliviousSubspaceEmbeddings}]  \label{thm:sparse-ose}
For any \(B > 2, \ \delta < 1/2, \ \e < 1/2\) there are
\[ w = O \Paren{\frac{B d \log(d/\delta)}{\e^2}} \quad \text{and} \quad s = O \Paren{\frac{\log_B(d/\delta)}{\e}}\]
such that the sparse embedding matrices form an oblivious \(\ell_2\)-subspace embedding.
\end{theorem}

\begin{corollary}
For any \(\delta < 1/2, \ \e < 1/2\) there are
\[ w = O \Paren{\frac{d \log(d/\delta)}{\e^2}} \quad \text{and} \quad s = O \Paren{\frac{\log(d/\delta)}{\e}}\]
such that the sparse embedding matrices form an oblivious \(\ell_2\)-subspace embedding.
\end{corollary}
\begin{proof}
Fix some \(B > 2\).
\end{proof}

Such an ensemble of sketching matrices is used to construct a preconditioner for \(\mat{A} \mat{D}^2 \mat{A}^T\) in the form of a nonsingular matrix \(\mat{P} \in \R^{m \times m}\) such that
\( \mat{P}^{-T} \mat{A} \mat{D}^2 \mat{A}^T \mat{P}^{-1} \approx \mat{I}_m \).
Instead of solving the normal equation we then solve
\( \mat{P}^{-T} \mat{A} \mat{D}^2 \mat{A}^T \mat{P}^{-1} \vek{v} = \mat{P}^{-T} \vek{p} \)
using an efficient iterative solver and \(\Delta\vek{y} = \mat{P}^{-1} \vek{q}\).
Note that applying the preconditioner from the left and from the right in this form ensures that the matrix stays positive definite and we can apply the CG algorithm to this linear system.

Inspired by \textcite{Avron-FasterRandomizedInfeasibleIPMs} we use a QR decomposition of the sketched matrix \(\mat{D}\mat{A}^T\).
We choose \(\mat{P} = \mat{R}\) where \(\mat{Q}\mat{R}\) is the QR decomposition of \(\mat{W}\mat{D}\mat{A}^T\).
The following theorem shows that this preconditioner leads to bounds on the condition number of \(\mat{R}^{-T} \mat{A} \mat{D}^2 \mat{A}^T \mat{R}^{-1}\) which can be used to bound the number of CG iterations needed to reach a certain error estimate.

\begin{theorem} \label{thm:condition-number-bound}
Let \(\mat{W} \in \R^{w \times n}\) be a \((1 \pm \e)\) \(\ell_2\)-subspace embedding of \(\mat{D}\mat{A}^T\) and \(\mat{Q}\mat{R} = \mat{W}\mat{D}\mat{A}^T\) the QR decomposition of the sketched matrix with \(\mat{Q} \in \R^{w \times m}\) and \(\mat{R} \in \R^{m \times m}\). Then the singular values of \(\mat{R}^{-T}\mat{A}\mat{D}\) and \(\mat{D}\mat{A}^T \mat{R}^{-1}\) are bounded by
\[ \frac{1}{1 + \e} \leq \sigma_i^2(\mat{R}^{-T}\mat{A}\mat{D}) = \sigma_i^2(\mat{D}\mat{A}^T \mat{R}^{-1}) \leq \frac{1}{1 - \e} \quad \text{for } i = 1, \ldots, m \]
which implies that the condition number of their product \(\mat{R}^{-T} \mat{A} \mat{D}^2 \mat{A}^T \mat{R}^{-1}\) is bounded by \((1+\e) / (1-\e)\).
\end{theorem}
\begin{proof}
The matrices \(\mat{R}^{-T}\mat{A}\mat{D}\) and \(\mat{D}\mat{A}^T \mat{R}^{-1}\) are transposes of each other and hence have the same singular values.
The minimum and maximum squared singular values of \(\mat{D}\mat{A}^T \mat{R}^{-1}\) are the same as the minimum and maximum values of \( \norm{\mat{D}\mat{A}^T \mat{R}^{-1} \vek{z}}_2^2 / \norm{\vek{z}}_2^2\) over all \(\vek{x} \in \R^n\).
By \cref{def:subspace-embedding} applied to \(\vek{x} = \mat{R}^{-1} \vek{z}\) we have
\[
  (1-\e) \norm{\mat{D}\mat{A}^T \mat{R}^{-1} \vek{z}}_2^2
  \leq \norm{\mat{W}\mat{D}\mat{A}^T \mat{R}^{-1}\vek{z}}_2^2
  = \norm{\mat{Q}\mat{R} \mat{R}^{-1}\vek{z}}_2^2
  = \norm{\mat{Q}\vek{z}}_2^2
  = \norm{\vek{z}}_2^2
\]
for all \(\vek{z} \in \R^m\) and similarly \((1+\e)\norm{\mat{D}\mat{A}^T \mat{R}^{-1} \vek{z}}_2^2 \geq \norm{\vek{z}}_2^2\).
Therefore
\[ \frac{1}{1 + \e} \leq \frac{\norm{\mat{D}\mat{A}^T \mat{R}^{-1} \vek{z}}_2^2}{\norm{\vek{z}}_2^2} \leq \frac{1}{1 - \e} \]
which proves the bounds on the singular values.
The condition number of a matrix is defined as the largest singular value divided by the smallest singular value.
Because \(\mat{R}^{-T} \mat{A} \mat{D}^2 \mat{A}^T \mat{R}^{-1} = (\mat{D} \mat{A}^T \mat{R}^{-1})^T (\mat{D} \mat{A}^T \mat{R}^{-1})\) the singular values of this matrix are just the squares of the singular values of \(\mat{D} \mat{A}^T \mat{R}^{-1}\), so
\[ \kappa_2(\mat{R}^{-T} \mat{A} \mat{D}^2 \mat{A}^T \mat{R}^{-1})
   = \frac{\sigma_{\max}^2(\mat{D} \mat{A}^T \mat{R}^{-1})}{\sigma_{\min}^2(\mat{D} \mat{A}^T \mat{R}^{-1})} \leq \frac{1+\e}{1-\e} \]
and we are done.
\end{proof}

\begin{theorem} \label{thm:cg-residual-bound}
Suppose \(\mat{R}\) is defined as in \cref{thm:condition-number-bound}.
Let \(\vek{q}^j\) be the iterates when solving \( \mat{R}^{-T} \mat{A} \mat{D}^2 \mat{A}^T \mat{R}^{-1} \vek{q} = \mat{R}^{-T} \vek{p} \) with the CG algorithm and let \(\vek{f}^j\) be the residuals defined by
\( \vek{f}^j = \mat{R}^{-T} \mat{A} \mat{D}^2 \mat{A}^T \mat{R}^{-1} \vek{q}^j - \mat{R}^{-T} \vek{p} \).
Then for \(\e < 1/2\) the \(\ell_2\)-norm of the residuals converges at least linearly to zero
\[ \norm{\vek{f}^{j+1}}_2 \leq \Paren{\frac{\e}{1-\e}} \norm{ \vek{f}^j }_2 \quad \forall j = 1, 2, \ldots \]
\end{theorem}
\begin{proof}
Theorem 8 of \cite{Bouyouli-ConjugateGradientConvergence} showed that
\[ \norm{\vek{f}^{j+1}}_2 \leq \frac{\kappa_2(\mat{R}^{-T} \mat{A} \mat{D}^2 \mat{A}^T \mat{R}^{-1}) - 1}{2} \norm{\vek{f}^j}_2 .\]
Using the bound on the condition number from \cref{thm:condition-number-bound} we obtain
\[ \frac{\kappa_2(\mat{R}^{-T} \mat{A} \mat{D}^2 \mat{A}^T \mat{R}^{-1}) - 1}{2}
   \leq \frac{\frac{1+\e}{1-\e} - 1}{2} = \frac{\e}{1 - \e} < 1 \]
which proves the claim.
\end{proof}