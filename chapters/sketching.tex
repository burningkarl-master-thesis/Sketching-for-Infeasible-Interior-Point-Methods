\chapter{Sketching}\label{chap:sketching}

% Important concepts to cover
% \begin{enumerate}
%   \item Goal of finding a preconditioner
%   \item Definition of a \((1\pm\e)\)-subspace embedding
%   \item Definition of an oblivious subspace embedding
%   \item Equivalent statements for subspace embeddings
%   \item State sparse sketching result
%   \item Apply sparse sketching result to \({(\mat{A}\mat{D})}^T\) and show that \(\mat{R}^{-1}\) is a good two-sided preconditioner
% \end{enumerate}

% \hrule

Based on~\cite{Avron-FasterRandomizedInfeasibleIPMs} we will use sketching to determine a preconditioner for \(\mat{A} \mat{D}^2 \mat{A}^T\).
These results not only apply to this specific matrix but all symmetric positive definite matrices of the form \(\mat{B}^T \mat{B}\) for some \(\mat{B} \in \R^{n \times m}\) with \(m \ll n\).
To emphasise this generality the proofs will all involve a general \(\mat{B}\) and their consequences for \(\mat{A}\mat{D}^2\mat{A}^T\) will be given as corollaries.

The following introduction to sketching is based on~\cite{Woodruff-Sketching} and begins with the central concept of a subspace embedding:
\begin{definition}\label{def:subspace-embedding}
Let \(\e \in (0, 1)\). A \emph{\((1 \pm \e)\) \(\ell_2\)-subspace embedding} for the column space of a matrix \(\mat{B} \in \R^{n \times m}\) is a matrix \(\mat{W} \in \R^{w \times n}\) such that
\[ (1-\e) \norm{\mat{B}\vek{z}}_2^2 \leq \norm{\mat{W}\mat{B}\vek{z}}_2^2 \leq (1+\e)\norm{\mat{B}\vek{z}}_2^2 \]
for all \(\vek{z} \in \R^m\).
\end{definition}
Note that this definition does not depend on a particular basis for the representation of the column space of \(\mat{B}\).
Nonetheless, we will often refer to \(\mat{W}\) as a \((1\pm\e)\) \(\ell_2\)-subspace embedding of \(\mat{B}\).
Intuitively, a subspace embedding of some \(m\)-dimensional subspace of \(\R^n\) is a linear map from the (large) space \(\R^n\) to a (smaller) space \(\R^w\) that approximately preserves all of the distances of the subspace.
There are multiple equivalent characterizations of subspace embeddings used in the literature.

\begin{theorem}
Assume that \(\mat{B} \in \R^{n \times m}\) has full column rank and let
\[\mathcal{S} = \set{ \mat{B}\vek{z} | \vek{z} \in \R^m } = \set{ \mat{U}\vek{z} | \vek{z} \in \R^m } \subset \R^n\]
for some matrix \(\mat{U} \in \R^{n \times m}\) with orthonormal columns.
Then the following statements are equivalent
\begin{enumerate}
  \item \(\mat{W}\) is a \((1\pm\e)\) \(\ell_2\)-subspace embedding for the column space of \(\mat{B}\)
  \item \((1-\e) \norm{\mat{B}\vek{z}}_2^2 \leq \norm{\mat{W}\mat{B}\vek{z}}_2^2 \leq (1+\e)\norm{\mat{B}\vek{z}}_2^2\) for all \(\vek{z} \in \R^d\)
  \item \(\abs{\norm{\mat{W}\vek{s}}_2^2 - 1} \leq \e\) for all \(\vek{s} \in \mathcal{S}\) with \(\norm{\vek{s}}_2 = 1\)
  \item \(\norm{\mat{U}^T \mat{W}^T \mat{W} \mat{U} - \mat{I}_d}_2 \leq \e\)
\end{enumerate}
\end{theorem}
\begin{proof}
The first equivalence holds by definition. Statement 3 is equivalent to
\[ 1-\e \leq \norm{\mat{W}\mat{s}}_2^2 \leq 1+\e \quad \forall \vek{s} \in \mathcal{S} \text{ with } \norm{\vek{s}}_2 = 1. \]
In this form statement 2 immediately implies 3 and choosing \(\vek{s} = \norm{\mat{B}\vek{z}}_2^{-1} \mat{B}\vek{z}\) and multiplying both sides by \(\norm{\mat{B}\vek{z}}_2^2\) we obtain that it is indeed equivalent to statement 2.

Note that the matrix \(\mat{U}^T \mat{W}^T \mat{W} \mat{U} - \mat{I}_d\) is real and symmetric and therefore its 2-norm is given by the maximum absolute value of the Rayleigh quotient~\cite[Equation (5.2)]{Demmel-AppliedNumericalLinearAlgebra}.
Using \(\norm{\mat{U}\vek{e}}_2 = \norm{\vek{e}}_2\), this implies
\begin{align*}
  \norm{\mat{U}^T \mat{W}^T \mat{W} \mat{U} - \mat{I}_d}_2
  &= \max_{\vek{e} \in \R^d, \norm{\vek{e}}_2 = 1} \Abs{\vek{e}^T \Paren{\mat{U}^T \mat{W}^T \mat{W} \mat{U} - \mat{I}_d} \vek{e} } \\
  &= \max_{\vek{e} \in \R^d, \norm{\vek{e}}_2 = 1} \Abs{{(\mat{U}\vek{e})}^T \mat{W}^T \mat{W} (\mat{U}\vek{e}) - \vek{e}^T \vek{e}} \\
  &= \max_{\vek{s} \in \mathcal{S}, \norm{\vek{s}}_2 = 1} \Abs{\vek{s}^T \mat{W}^T \mat{W} \vek{s} - 1} \\
  &= \max_{\vek{s} \in \mathcal{S}, \norm{\vek{s}}_2 = 1} \Abs{\norm{\mat{W}\vek{s}}_2^2 - 1}.
\end{align*}
and proves the equivalence of statements 3 and 4.
\end{proof}

Using the QR decomposition of \(\mat{B}\) it is always possible to design a \enquote{perfect} subspace embedding with \(\e = 0\) and \(w = m\)
but computing this QR decomposition is expensive.
Instead, we can use random matrices \(\mat{W}\) which have a high chance of being a subspace embedding.

\begin{definition}\label{def:oblivious-subspace-embedding}
Let \(m, n, w \in \N\) and \(\delta, \e \in (0, 1)\) be given.
A probability distribution \(\Pi\) over matrices \(\mat{W} \in \R^{w \times n}\) is an \emph{oblivious \(\ell_2\)-subspace embedding} (OSE) if, for any matrix \(\mat{B} \in \R^{n \times m}\) a matrix \(\mat{W}\) drawn from \(\Pi\) is a \((1\pm\e)\) \(\ell_2\)-subspace embedding with probability at least \(1 - \delta\).
\end{definition}

Because we will later compute a decomposition of the sketched matrix \(\mat{W}\mat{B}\) we want \(w\) to be as small as possible.
A classic result in sketching is that there are OSEs with \(w = O((m + \log(1/\delta))\e^{-2})\) without any dependence on \(n\).
\Textcite[Theorem 6]{Woodruff-Sketching} shows how this follows from the so-called Johnson-Lindenstrauss Lemma and that it can be achieved by choosing the matrix entries to be properly normalized independent Gaussian random variables.
Unfortunately, a matrix \(\mat{W}\) with Gaussian entries is dense so computing the matrix product \(\mat{W}\mat{B}\) takes \(O(w n m)\) operations in general.
If the matrix \(\mat{B}\) is sparse we need an OSE where all sampled matrices are sparse to take advantage of that.
The following sparse OSE result is taken from~\cite{Cohen-NearlyTightObliviousSubspaceEmbeddings}.
\Textcite{Avron-FasterRandomizedInfeasibleIPMs} use the sketching result from~\cite{Cohen-OptimalApproximateMatrixProduct} but for simplicity we replaced it by a simpler one giving the same complexity guarantees in the end.

\begin{definition}
An \emph{\((s, w)\)-sparse embedding matrix} \(\mat{W} \in \R^{w \times n}\) is constructed in the following way:
For each column \(s\) entries are sampled randomly without replacement and for each such entry its value is randomly drawn from \(\{ - \frac{1}{\sqrt{s}}, \frac{1}{\sqrt{s}} \}\).
All other entries of \(\mat{W}\) are set to zero.
This defines a probability distribution over \(\R^{w \times n}\) where each sampled matrix has exactly \(s \cdot n\) nonzero entries.
\end{definition}

\begin{theorem}[Theorem 4.2 in~\cite{Cohen-NearlyTightObliviousSubspaceEmbeddings}]\label{thm:sparse-ose}
For any \(\delta < 1/2, \ \e < 1/2\) and \(m, n \in \N\) the \((s, w)\)-sparse embedding matrices form an oblivious \(\ell_2\)-subspace embedding for
\[ w = O \Paren{\frac{m \log(m/\delta)}{\e^2}} \quad \text{and} \quad s = O \Paren{\frac{\log(m/\delta)}{\e}}. \]
\end{theorem}

% Simplicity for the runtime, avoid confusion: \e = 1/3
To simplify the runtime bounds and the following discussion we will from now on fix \(\e = 1/3 < 1/2\) and furthermore assume that \(\mat{W} \in \R^{w \times n}\) is a sparse embedding matrix and a \((1\pm\e)\) \(\ell_2\)-subspace embedding of the column space of \(\mat{B} \in \R^{n \times m}\).
From the previous result we get that choosing
\begin{equation} \label{eqn:sparse-ose-params}
  w = O \Paren{m \log(m/\delta)} \quad \text{and} \quad s = O \Paren{\log(m/\delta)}
\end{equation}
suffices to guarantee that the latter assumption holds with probability at least \(1 - \delta\).
After we obtain a bound on the number of IPM iterations we will choose \(\delta\) in such a way that the failure probability over all iterations is low.
Note that because \(\e < 1\) none of the elements in the column space of \(\mat{B}\) can be in the nullspace of \(\mat{W}\) so \(\rank(\mat{W}\mat{B}) = m\).

Inspired by \textcite{Avron-FasterRandomizedInfeasibleIPMs} we use a decomposition of the sketched matrix \(\mat{D}\mat{A}^T\) to find a preconditioner for the normal equation.
They constructed the preconditioner from the SVD of \(\mat{W}\mat{D}\mat{A}^T\) but we decided to go with the more practical choice of constructing it using the QR decomposition \(\mat{Q}\mat{R} = \mat{W}\mat{D}\mat{A}^T\).
Here \(\mat{R} \in \R^{m \times m}\) is upper triangular and \(\mat{Q} \in \R^{w \times m}\) has orthonormal columns.
The preconditioned system has the form
\begin{equation} \label{eqn:preconditioned-system2}
 \mat{R}^{-T} \mat{A} \mat{D}^2 \mat{A}^T \mat{R}^{-1} \vek{q} = \mat{R}^{-T} \vek{p}
\end{equation}
The following theorem shows that this preconditioner leads to bounds on the condition number of \(\mat{R}^{-T} \mat{A} \mat{D}^2 \mat{A}^T \mat{R}^{-1}\) which can be used to bound the number of CG iterations needed to reach a certain error estimate.

% We will construct a preconditioner for \(\mat{A} \mat{D}^2 \mat{A}^T\) in the form of a nonsingular matrix \(\mat{P} \in \R^{m \times m}\) such that
% \( \mat{P}^{-T} \mat{A} \mat{D}^2 \mat{A}^T \mat{P}^{-1} \approx \mat{I}_m \).
% Instead of solving the normal equation we then solve
% \begin{equation} \label{eqn:preconditioned-system}
%  \mat{P}^{-T} \mat{A} \mat{D}^2 \mat{A}^T \mat{P}^{-1} \vek{q} = \mat{P}^{-T} \vek{p}
% \end{equation}
% using an efficient iterative solver and \(\Delta\vek{y} = \mat{P}^{-1} \vek{q}\).
% Note that applying the preconditioner from the left and from the right in this form ensures that the matrix stays symmetric positive definite and we can apply the CG algorithm to this linear system.

\begin{theorem}\label{thm:condition-number-bound}
The singular values of \(\mat{R}^{-T}\mat{A}\mat{D}\) and \(\mat{D}\mat{A}^T \mat{R}^{-1}\) are bounded by
\[ \frac{3}{4} = \frac{1}{1 + \e} \leq \sigma_i^2(\mat{R}^{-T}\mat{A}\mat{D}) = \sigma_i^2(\mat{D}\mat{A}^T \mat{R}^{-1}) \leq \frac{1}{1 - \e} = \frac{3}{2} \quad \text{for } i = 1, \ldots, m \]
which implies that the condition number of their product \(\mat{R}^{-T} \mat{A} \mat{D}^2 \mat{A}^T \mat{R}^{-1}\) is bounded by \((1+\e) / (1-\e) = 2\).
\end{theorem}
\begin{proof}
The matrices \(\mat{R}^{-T}\mat{A}\mat{D}\) and \(\mat{D}\mat{A}^T \mat{R}^{-1}\) are transposes of each other and hence have the same singular values.
The minimum and maximum squared singular values of \(\mat{D}\mat{A}^T \mat{R}^{-1}\) are the same as the minimum and maximum values of \( \norm{\mat{D}\mat{A}^T \mat{R}^{-1} \tilde{\vek{z}}}_2^2 / \norm{\tilde{\vek{z}}}_2^2\) over all \(\tilde{\vek{z}} \in \R^n\).
By \cref{def:subspace-embedding} applied to \(\vek{z} = \mat{R}^{-1} \tilde{\vek{z}}\) we have
\[
  (1-\e) \norm{\mat{D}\mat{A}^T \mat{R}^{-1} \tilde{\vek{z}}}_2^2
  \leq \norm{\mat{W}\mat{D}\mat{A}^T \mat{R}^{-1}\tilde{\vek{z}}}_2^2
  = \norm{\mat{Q}\mat{R} \mat{R}^{-1}\tilde{\vek{z}}}_2^2
  = \norm{\mat{Q}\tilde{\vek{z}}}_2^2
  = \norm{\tilde{\vek{z}}}_2^2
\]
for all \(\tilde{\vek{z}} \in \R^m\) and similarly \((1+\e)\norm{\mat{D}\mat{A}^T \mat{R}^{-1} \tilde{\vek{z}}}_2^2 \geq \norm{\tilde{\vek{z}}}_2^2\).
Therefore
\[ \frac{1}{1 + \e} \leq \frac{\norm{\mat{D}\mat{A}^T \mat{R}^{-1} \tilde{\vek{z}}}_2^2}{\norm{\tilde{\vek{z}}}_2^2} \leq \frac{1}{1 - \e} \]
which proves the bounds on the singular values.
The condition number of a matrix is defined as the largest singular value divided by the smallest singular value.
Because \(\mat{R}^{-T} \mat{A} \mat{D}^2 \mat{A}^T \mat{R}^{-1} = {(\mat{D} \mat{A}^T \mat{R}^{-1})}^T (\mat{D} \mat{A}^T \mat{R}^{-1})\) the singular values of this matrix are just the squares of the singular values of \(\mat{D} \mat{A}^T \mat{R}^{-1}\), so
\[ \kappa_2(\mat{R}^{-T} \mat{A} \mat{D}^2 \mat{A}^T \mat{R}^{-1})
   = \frac{\sigma_{\max}^2(\mat{D} \mat{A}^T \mat{R}^{-1})}{\sigma_{\min}^2(\mat{D} \mat{A}^T \mat{R}^{-1})} \leq \frac{1+\e}{1-\e} \]
and we are done.
\end{proof}

\begin{theorem}\label{thm:cg-residual-bound}
Let \(\vek{q}^j\) be the iterates when solving \( \mat{R}^{-T} \mat{A} \mat{D}^2 \mat{A}^T \mat{R}^{-1} \vek{q} = \mat{R}^{-T} \vek{p} \) with the CG algorithm and let \(\vek{f}^j\) be the residuals defined by
\( \vek{f}^j = \mat{R}^{-T} \mat{A} \mat{D}^2 \mat{A}^T \mat{R}^{-1} \vek{q}^j - \mat{R}^{-T} \vek{p} \).
Then the \(\ell_2\)-norm of the residuals converges at least linearly to zero
\[ \norm{\vek{f}^{j+1}}_2 \leq \frac{1}{2} \norm{ \vek{f}^j }_2 \quad \forall j = 1, 2, \ldots \]
\end{theorem}
\begin{proof}
Theorem 8 of~\cite{Bouyouli-ConjugateGradientConvergence} showed that
\[ \norm{\vek{f}^{j+1}}_2 \leq \frac{\kappa_2(\mat{R}^{-T} \mat{A} \mat{D}^2 \mat{A}^T \mat{R}^{-1}) - 1}{2} \norm{\vek{f}^j}_2 .\]
Using the bound on the condition number from \cref{thm:condition-number-bound} we obtain
\[ \frac{\kappa_2(\mat{R}^{-T} \mat{A} \mat{D}^2 \mat{A}^T \mat{R}^{-1}) - 1}{2}
   \leq \frac{2 - 1}{2} = \frac{1}{2} \]
which proves the claim.
\end{proof}

After designing the preconditioner we also need to choose the perturbation vector \(\vek{v}\) satisfying \(\mat{A}\mat{S}^{-1}\vek{v} = \mat{R}^T \vek{f}\) where \(\vek{f}\), the error term introduced by the CG algorithm, is given.
We use the method of \textcite{Avron-FasterRandomizedInfeasibleIPMs} which utilises the sketched matrix \(\mat{W}\mat{D}\mat{A}^T\) for which we have already determined a decomposition.
This also allows us to bound the size of the perturbation vector by the size of \(\vek{f}\).

\begin{theorem}
Given a vector \(\vek{f} \in \R^m\) the vector
\[ \vek{v} = {(\mat{S}\mat{X})}^{1/2} \mat{W}^T \mat{Q} \vek{f} \]
satisfies \( \mat{A}\mat{S}^{-1}\vek{v} = \mat{R}^T \vek{f} \) and \(\norm{\vek{v}}_2 \leq n \sqrt{\mu} \norm{\vek{f}}_2\).
\end{theorem}
\begin{proof}
Using the definition of \(\mat{Q}\) and \(\mat{R}\) we obtain
\begin{align*}
  \mat{A} \mat{S}^{-1} \vek{v}
  &= \mat{A} \mat{S}^{-1} {(\mat{S}\mat{X})}^{1/2} \mat{W}^T \mat{Q} \vek{f}
   = \mat{A} \mat{D} \mat{W}^T \mat{Q} \vek{f} \\
  &= {(\mat{Q} \mat{R})}^T \mat{Q} \vek{f}
   = \mat{R}^T \mat{Q}^T \mat{Q} \vek{f}
   = \mat{R}^T \vek{f}
\end{align*}
which proves the first part.
To bound the norm consider
\[ 
  \norm{\vek{v}}_2
  = \norm{{(\mat{S}\mat{X})}^{1/2} \mat{W}^T \mat{Q} \vek{f}}_2
  \leq \norm{{(\mat{S}\mat{X})}^{1/2}}_2 \norm{\mat{W}^T}_2 \norm{\mat{Q} \vek{f}}_2.
\]
Now, \(\norm{{(\mat{S}\mat{X})}^{1/2}}_2 \leq \norm{{(\mat{S}\mat{X})}^{1/2}}_F = \sqrt{n \mu}\) by definition of \(\mu\) and \(\norm{\mat{Q}\vek{f}}_2 = \norm{\vek{f}}_2\) because \(\mat{Q}\) has orthonormal columns.
Lastly, we need to bound \(\norm{\mat{W}^T}_2\).
Remember that \(\mat{W}\) is constructed by choosing exactly \(s\) nonzero entries per column and each of these entries is drawn from \(\{-\frac{1}{\sqrt{s}}, \frac{1}{\sqrt{s}}\}\).
Let \(\mathcal{J}_i\) denote the set of indices of the nonzero entries of column \(i\) in \(\mat{W}\) and let \(\vek{z}_{\mathcal{J}_i} \in \R^s\) denote the restriction of \(\vek{z} \in \R^w\) to these indices.
Using standard norm inequalities
\begin{align*}
  \norm{\mat{W}^T \vek{z}}_2^2 &= \sum_{i=1}^{n} \Paren{ \sum_{j=1}^{w} \mat{W}_{ji} \vek{z}_j }^2 = \sum_{i=1}^{n} \Paren{ \sum_{j \in \mathcal{J}_i} \mat{W}_{ji} \vek{z}_j }^2 \\
  &\leq \sum_{i=1}^{n} \Paren{ \sum_{j \in \mathcal{J}_i} \abs{\mat{W}_{ji}} \abs{\vek{z}_j} }^2 = \sum_{i=1}^{n} \Paren{ \frac{1}{\sqrt{s}} \norm{\vek{z}_{\mathcal{J}_i}}_1 }^2 \\
  &\leq \sum_{i=1}^{n} \Paren{ \frac{1}{\sqrt{s}} \sqrt{s} \norm{\vek{z}_{\mathcal{J}_i}}_2 }^2 = \sum_{i=1}^{n} \norm{\vek{z}_{\mathcal{J}_i}}_2^2 \\
  &\leq n \norm{\vek{z}}_2^2
\end{align*}
which implies \(\norm{\mat{W}^T}_2 \leq \sqrt{n}\).
Combining these results,
\[ 
  \norm{\vek{v}}_2
  \leq \norm{{(\mat{S}\mat{X})}^{1/2}}_2 \norm{\mat{W}^T}_2 \norm{\mat{Q} \vek{f}}_2
  \leq \sqrt{n \mu} \sqrt{n} \norm{\vek{f}}_2 = n \sqrt{\mu} \norm{\vek{f}}_2
\]
as claimed.
\end{proof}
