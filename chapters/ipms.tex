\chapter{Interior-Point Methods for Linear Programming}

Important concepts to cover
\begin{enumerate}
 \item Notation (A, b, c, x, y, s, residuals, mu)
 \item Optimality conditions
 \item Newton system: Complete system (?), augmented system, normal equation
 \item Focus on infeasible, inexact, long-step IPMs
 \begin{enumerate}
   \item Central path
   \item \(\mathcal{N}_{-\infty}\) neighbourhood
   \item Error in the normal equation translates to error in \(\mat{A}\Delta\vek{x} = \vek{r}_p\)
 \end{enumerate}
\end{enumerate}

\hrule

For naming primal, dual and slack variables as well as the choice of the neighbourhood we follow \cite{Monteiro-ConvergenceAnalysisLongStepInfeasibleIPMs}.

The standard form linear programming (LP) problem is given by
\begin{equation}
 \min \vek{c}^T \vek{x} \quad \text{subject to} \quad \mat{A}\vek{x} = \vek{b}, \ \vek{x} \geq 0
\end{equation}
and its dual is
\begin{equation}
  \max \vek{b}^T \vek{y} \quad \text{subject to} \quad \mat{A}^T \vek{y} + \vek{s} = \vek{c}, \ \vek{s} \geq 0
\end{equation}
where \(\mat{A} \in \R^{m \times n}\), \(\vek{b} \in \R^m\) and \(\vek{c} \in \R^n\) are given and \(\vek{x}, \vek{s} \in \R^n\) and \(\vek{y} \in \R^m\) are the variables.
For feasible \(\vek{x}, \vek{y}, \vek{s}\) we always have \(\vek{c}^T \vek{x} \geq \vek{b}^T \vek{y}\) and the optimal solutions satisfy \(\vek{c}^T \vek{x} = \vek{b}^T \vek{y}\), so \(\vek{x}^T \vek{s} = \vek{c}^T \vek{x} - \vek{b}^T \vek{y}\) can be considered an optimality measure.

Necessary and sufficient optimality conditions for both problems are given by
\begin{subequations} \label{optimality-conditions}
  \begin{align}
    \mat{A} \vek{x} &= \vek{b} \label{primal-feasibility} \\
    \mat{A}^T \vek{y} + \vek{s} &= \vek{c} \label{dual-feasibility} \\
    \vek{x} \circ \vek{s} &= 0 \label{complementarity} \\
    \vek{x}, \vek{s} &\geq 0 \label{nonnegativity}
  \end{align}
\end{subequations}

How close a given triple \((\vek{x}, \vek{y}, \vek{s})\) with \(\vek{x}, \vek{s} \geq 0\) is to satisfying these conditions is measured by
\begin{subequations}
  \begin{align}
    \vek{r}_p &= \vek{r}_p(\vek{x}) = \mat{A}\vek{x} - \vek{b} \\
    \vek{r}_d &= \vek{r}_d(\vek{y}, \vek{s}) = \mat{A}^T \vek{y} + \vek{s} - \vek{c} \\
    \vek{r}   &= \vek{r}(\vek{x}, \vek{y}, \vek{s}) = (\vek{r}_p, \vek{r}_d) \\
    \mu       &= \mu(\vek{x}, \vek{s}) = \vek{x}^T \vek{s} / n
  \end{align}
\end{subequations}

In primal-dual interior-point methods one maintains iterates \((\vek{x}^k, \vek{y}^k, \vek{s}^k)\) that satisfy \(\vek{x}^k, \vek{s}^k > 0\).
Feasible IPMs require the iterates to always be feasible (satisfying \cref{primal-feasibility} and \cref{dual-feasibility}) while infeasible IPMs allow some non-zero residuals \(\vek{r}^k\) as long as their size goes to zero at approximately the same rate as \(\mu\).

The \emph{central path} is given by \(\Set{(\vek{x}_\tau, \vek{y}_\tau, \vek{s}_\tau) | \tau > 0}\) where \(\vek{x}_\tau, \vek{y}_\tau, \vek{s}_\tau\) are uniquely determined by
\begin{subequations}
  \begin{align}
    \mat{A}\vek{x}_\tau - \vek{b} &= \tau \vek{r}_p^0 \\
    \mat{A}^T \vek{y}_\tau + \vek{s}_\tau - \vek{c} &= \tau \vek{r}_d^0 \\
    \vek{x}_\tau \circ \vek{s}_\tau &= \tau \mu_0 \vek{1} \\
    \vek{x}_\tau, \vek{s}_\tau &> 0
  \end{align}
\end{subequations}
if a solution satisfying \cref{optimality-conditions} exists (Wright says strict feasibility is required, ?), see \cite{Mizuno-PolynomialTimeConvergenceInexactIPM}.
In this case, the elements of the central path converge towards a solution as \(\tau \to 0\).
In a \emph{long-step} infeasible IPM all iterates are required to belong to a neighbourhoood of this central path given by
\begin{equation}
  \mathcal{N}_{-\infty}(\gamma) = \Set{ (\vek{x}, \vek{y}, \vek{s}) | \vek{x}, \vek{s} > 0, \ \vek{x} \circ \vek{s} \geq (1 - \gamma) \mu \vek{1}, \ \frac{\norm{\vek{r}}}{\norm{\vek{r}^0}} \leq \frac{\mu}{\mu^0} }
\end{equation}
where \(\gamma \in (0, 1)\) is a parameter controlling the size of the neighbourhood.
If it can be shown that \(\mu \to 0\) during the algorithm then by the choice of the neighbourhood the residuals must also converge to \(0\).

The search direction in every iteration is determined by a Newton system, similar to the one used when trying to find a root of 
\begin{equation}
  F(\vek{x}, \vek{y}, \vek{s}) \deq \begin{pmatrix} \mat{A}\vek{x} - \vek{b} \\ \mat{A}^T \vek{y} + \vek{s} - \vek{c} \\ \vek{x} \circ \vek{s} \end{pmatrix}
\end{equation}
The total derivative of \(F\) is given by
\begin{equation}
  F'(\vek{x}, \vek{y}, \vek{s}) = \begin{pmatrix}
    \mat{A} & 0         & 0       \\
    0       & \mat{A}^T & \mat{I} \\
    \mat{S} & 0         & \mat{X} \\
  \end{pmatrix}
\end{equation}
where \(\mat{X} = \diag(\vek{x})\) and \(\mat{S} = \diag(\vek{s})\).
The actual Newton system determining \((\Delta\vek{x}, \Delta\vek{y}, \Delta\vek{s})\) is slightly modified by adding a centering term \(\sigma \mu \vek{1}\) where \(\sigma \in [0,1]\) which allows for larger steps inside the neighbourhood:
\begin{equation} \label{eqn:newton-system}
  \underbrace{
  \begin{pmatrix}
    \mat{A} & 0         & 0       \\
    0       & \mat{A}^T & \mat{I} \\
    \mat{S} & 0         & \mat{X} \\
  \end{pmatrix}
  }_{F'(\vek{x}, \vek{y}, \vek{s})}
  \begin{pmatrix} \Delta\vek{x} \\ \Delta\vek{y} \\ \Delta\vek{s} \end{pmatrix}
  = -
  \underbrace{
  \begin{pmatrix} \vek{r}_p \\ \vek{r}_d \\ \vek{x} \circ \vek{s} \end{pmatrix}
  }_{F(\vek{x}, \vek{y}, \vek{s})}
  + \begin{pmatrix} 0 \\ 0 \\ \sigma \mu \vek{1} \end{pmatrix}
\end{equation}

This linear system of equations can be rewritten as
\begin{subequations} \label{eqn:augmented-system}
  \begin{align}
    \begin{pmatrix}
      \mat{A}              & 0 \\
      \mat{X}^{-1} \mat{S} & \mat{A}^T \\
    \end{pmatrix}
    \begin{pmatrix}
      \Delta\vek{x} \\
      \Delta\vek{y}
    \end{pmatrix}
    &=
    \begin{pmatrix}
      -\vek{r}_p \\
      -\vek{r}_d -\vek{s} + \sigma \mu \mat{X}^{-1} \vek{1}
    \end{pmatrix} \\
    \Delta\vek{s} &= -\vek{s} + \sigma \mu \mat{X}^{-1} \vek{1} - \mat{X}^{-1} \mat{S} \Delta\vek{x}
  \end{align}
\end{subequations}
which is called the \emph{augmented system}.
Eliminating \(\Delta\vek{x}\) from the first equation and using \(\mat{D} = \mat{S}^{-1/2} \mat{X}^{1/2}\) we get
\begin{subequations} \label{eqn:normal-equation-system}
  \begin{align}
    \mat{A} \mat{D}^2 \mat{A}^T \Delta\vek{y} &= -\vek{r}_p + \mat{A} ( -\mat{S}^{-1} \mat{X} \vek{r}_d + \vek{x} - \sigma \mu \mat{S}^{-1} \vek{1} ) \eqqcolon \vek{p} \label{eqn:normal-equation} \\
    \Delta\vek{s} &= -\vek{r}_d - \mat{A}^T \Delta\vek{y} \label{eqn:s-from-normal} \\
    \Delta\vek{x} &= -x + \sigma \mu \mat{S}^{-1} \vek{1} - \mat{S}^{-1} \mat{X} \Delta\vek{s} \label{eqn:x-from-normal}
  \end{align}
\end{subequations}
The first component is called the \emph{normal equation} because it can be written as the normal equation of a least squares system if \(\vek{r}_p = \vek{0}\).

We will construct a preconditioner for \(\mat{A} \mat{D}^2 \mat{A}^T\) in the form of a nonsingular matrix \(\mat{P} \in \R^{m \times m}\) such that
\( \mat{P}^{-T} \mat{A} \mat{D}^2 \mat{A}^T \mat{P}^{-1} \approx \mat{I}_m \).
Instead of solving the normal equation we then solve
\begin{equation} \label{eqn:preconditioned-system}
 \mat{P}^{-T} \mat{A} \mat{D}^2 \mat{A}^T \mat{P}^{-1} \vek{q} = \mat{P}^{-T} \vek{p}
\end{equation}
using an efficient iterative solver and \(\Delta\vek{y} = \mat{P}^{-1} \vek{q}\).
Note that applying the preconditioner from the left and from the right in this form ensures that the matrix stays symmetric positive definite and we can apply the CG algorithm to this linear system.

Using an iterative method to solve \cref{eqn:preconditioned-system} introduces an error term \(\vek{f}\). Let \(\Delta\tilde{\vek{x}}, \Delta\tilde{\vek{y}}, \Delta\tilde{\vek{s}}\) be calculated using \cref{eqn:s-from-normal,eqn:x-from-normal} given that \(\Delta\tilde{\vek{y}}\) solves
\begin{equation}
 \mat{P}^{-T} \mat{A} \mat{D}^2 \mat{A}^T \Delta\tilde{\vek{y}} = \mat{P}^{-T} \vek{p} + \vek{f} \iff \mat{A} \mat{D}^2 \mat{A}^T \Delta\tilde{\vek{y}} = \vek{p} + \mat{P}^{T} \vek{f}
\end{equation}
instead of \cref{eqn:normal-equation}. In this case
\begin{equation}
  \begin{pmatrix}
    \mat{A} & 0         & 0       \\
    0       & \mat{A}^T & \mat{I} \\
    \mat{S} & 0         & \mat{X} \\
  \end{pmatrix}
  \begin{pmatrix} \Delta\tilde{\vek{x}} \\ \Delta\tilde{\vek{y}} \\ \Delta\tilde{\vek{s}} \end{pmatrix}
  =
  \begin{pmatrix} -\vek{r}_p \\ -\vek{r}_d \\ -\vek{x} \circ \vek{s} + \sigma \mu \vek{1} \end{pmatrix}
  +
  \begin{pmatrix} \mat{P}^T \vek{f} \\ 0 \\ 0 \end{pmatrix}
\end{equation}
because calculating \(\Delta\tilde{\vek{x}}, \Delta\tilde{\vek{y}}, \Delta\tilde{\vek{s}}\) in this way is equivalent to using \cref{eqn:normal-equation-system} with \(\vek{b} + \mat{P}^T \vek{f}\) instead of \(\vek{b}\).

For our convergence proofs to work we need that the error in calculating the Newton direction only affects the last line of \cref{eqn:newton-system}.
This can be achieved by adding a perturbation vector \(\mat{S}^{-1}\vek{v} \in \R^n\) to \(\Delta\tilde{\vek{x}}\) which satisfies \(\mat{A}\mat{S}^{-1}\vek{v} = \mat{P}^T \vek{f}\).
The new instructions to get an approximate solution to the Newton system are now given by
\begin{subequations}
  \begin{align}
    \mat{A} \mat{D}^2 \mat{A}^T \Delta\hat{\vek{y}} &= \vek{p} + \mat{P}^{T} \vek{f} \\
    \Delta\hat{\vek{s}} &= -\vek{r}_d - \mat{A}^T \Delta\hat{\vek{y}} \\
    \Delta\hat{\vek{x}} &= -x + \sigma \mu \mat{S}^{-1} \vek{1} - \mat{S}^{-1} \mat{X} \Delta\hat{\vek{s}} - \mat{S}^{-1} \vek{v}
  \end{align}
\end{subequations}
This approximate solution solves the following system
\begin{equation}
  \begin{pmatrix}
    \mat{A} & 0         & 0       \\
    0       & \mat{A}^T & \mat{I} \\
    \mat{S} & 0         & \mat{X} \\
  \end{pmatrix}
  \begin{pmatrix} \Delta\hat{\vek{x}} \\ \Delta\hat{\vek{y}} \\ \Delta\hat{\vek{s}} \end{pmatrix}
  =
  \begin{pmatrix} -\vek{r}_p \\ -\vek{r}_d \\ -\vek{x} \circ \vek{s} + \sigma \mu \vek{1} \end{pmatrix}
  -
  \begin{pmatrix} 0 \\ 0 \\ \vek{v} \end{pmatrix}
\end{equation}
so we successfully shifted the error term from the first to the third component.

To control the size of the error \(\norm{\vek{v}}_2\) in terms of the error \(\norm{\vek{f}}_2\) we need to design a clever way of choosing \(\vek{v}\) which will be done in the next chapter.