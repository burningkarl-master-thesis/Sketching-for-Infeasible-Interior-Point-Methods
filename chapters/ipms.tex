\chapter{A Brief Introduction to IPMs for Linear Programming}\label{chap:ipm}

\section{Central Concepts}

The standard form linear programming (LP) problem is given by
\begin{equation}\label{eqn:primal-lp}
 \min \vek{c}^T \vek{x} \quad \text{subject to} \quad \mat{A}\vek{x} = \vek{b}, \ \vek{x} \geq \vek{0} \tag{P}
\end{equation}
and its dual is
\begin{equation}\label{eqn:dual-lp}
  \max \vek{b}^T \vek{y} \quad \text{subject to} \quad \mat{A}^T \vek{y} + \vek{s} = \vek{c}, \ \vek{s} \geq \vek{0} \tag{D}
\end{equation}
where \(\mat{A} \in \R^{m \times n}\), \(\vek{b} \in \R^m\) and \(\vek{c} \in \R^n\) are given and \(\vek{x}, \vek{s} \in \R^n\) and \(\vek{y} \in \R^m\) are the variables.
A triple \((\vek{x}, \vek{y}, \vek{s})\) is called \emph{feasible} if it satisfies the constraints \(\mat{A}\vek{x} = \vek{b}\), \(\mat{A}^T \vek{y} + \vek{s} = \vek{c}\) and \(\vek{x}, \vek{s} \geq \vek{0}\).
The two linear programs \labelcref{eqn:primal-lp} and \labelcref{eqn:dual-lp} are closely connected.
For feasible \((\vek{x}, \vek{y}, \vek{s})\) we always have \(\vek{c}^T \vek{x} \geq \vek{b}^T \vek{y}\) and the joint solutions satisfy \(\vek{c}^T \vek{x} = \vek{b}^T \vek{y}\).
Therefore, if there are feasible points both linear programs are feasible and bounded and solutions exist.
Furthermore, as the optimal value of the objective function for both LPs is the same, it is upper bounded by \(\vek{c}^T \vek{x}\) and lower bounded by \(\vek{b}^T \vek{y}\) at any feasible point.
\(\vek{x}^T \vek{s} = \vek{c}^T \vek{x} - \vek{b}^T \vek{y}\) can thus be interpreted as a measure of how close to optimality a given feasible point is.

As already seen, necessary and sufficient optimality conditions are given by the feasibility conditions and \(\vek{x}^T \vek{s} = \vek{c}^T \vek{x} - \vek{b}^T \vek{y} = 0\).
Because the entries of \(\vek{x}\) and \(\vek{s}\) are both nonnegative this is equivalent to the complementarity condition \(\vek{x} \circ \vek{s} = \vek{0}\) requiring that for each \(1 \leq i \leq n\) at least one of \(x_i\) and \(s_i\) is zero.
Necessary and sufficient optimality conditions for both problems are therefore given by
\begin{subequations}\label{eqn:optimality-conditions}
  \begin{align}
    \label{eqn:primal-feasibility}
    \vek{r}_p = \mat{A}\vek{x} - \vek{b} &= \vek{0} \\
    \label{eqn:dual-feasibility}
    \vek{r}_d = \mat{A}^T \vek{y} + \vek{s} - \vek{c} &= \vek{0} \\
    \label{eqn:nonnegativity}
    \vek{x}, \vek{s} &\geq \vek{0} \\
    \label{eqn:complementarity}
    \vek{x} \circ \vek{s} &= \vek{0}
  \end{align}
\end{subequations}
Here, \(\vek{r}_p\) and \(\vek{r}_d\) denote the primal and dual residuals and \(\vek{r} = (\vek{r}_p, \vek{r}_d)\) denotes their concatenation as a vector in \(\R^{m+n}\).
We denote the set of feasible points with \(\mathcal{F}\) and the set of solutions as \(\mathcal{F}^*\).
The feasible points which satisfy \cref{eqn:nonnegativity} strictly (all entries need to be positive) are called \emph{strictly feasible} and the set of strictly feasible points is denoted by \(\mathcal{F}^o\).
Because we will focus on a so-called infeasible IPM it is also useful to define \(\mathcal{G} \coloneqq \R^n_{>0} \times \R^m \times \R^n_{>0}\) as the set of points that satisfy \cref{eqn:nonnegativity} strictly without requiring feasibility.

Assume strictly feasible points exist, then the unique solutions \((\vek{x}_\tau, \vek{y}_\tau, \vek{s}_\tau)\) of
\begin{subequations}
  \begin{align}
    \mat{A}\vek{x}_\tau - \vek{b} &= \vek{0} \\
    \mat{A}^T \vek{y}_\tau + \vek{s}_\tau - \vek{c} &= \vek{0} \\
    \vek{x}_\tau, \vek{s}_\tau &> \vek{0} \label{eqn:strict-nonnegativity} \\
    \vek{x}_\tau \circ \vek{s}_\tau &= \tau \vek{1}.
  \end{align}
\end{subequations}
for \(\tau > 0\) form a continuous path of strictly feasible points which is called the \emph{central path}, see~\cite[Theorem 2.8]{Wright-PrimalDualInteriorPointMethods}.
Moreover, the elements of the central path converge towards a solution as \(\tau \to 0\).

Primal-dual IPMs now try to follow this path towards a solution by maintaining iterates in some neighbourhood of the central path and computing the search direction in each step as a Newton direction towards an element of the central path.
In this thesis we focus on an IPM variant with good practical properties.
Its neighbourhood is defined by
\begin{equation}
  \mathcal{N}_{-\infty}(\gamma) = \Set{ (\vek{x}, \vek{y}, \vek{s}) \in \mathcal{G} | \vek{x} \circ \vek{s} \geq (1 - \gamma) \mu \vek{1}, \ \frac{\norm{\vek{r}}_2}{\norm{\vek{r}^0}_2} \leq \frac{\mu}{\mu^0} }. \label{eqn:neighbourhood}
\end{equation}
where \(\mu = \vek{x}^T \vek{s}/n\), i.e.\ the average value of \(x_i s_i\), and the parameter \(\gamma \in (0, 1)\) controls the size of the neighbourhood.
The condition \((\vek{x}, \vek{y}, \vek{s}) \in \mathcal{G}\) is shared by all IPMs.
It keeps the \(\vek{x}\) and \(\vek{s}\) components of the iterates in the interior of the nonnegative orthant.
The second condition qualifies this IPM as a \emph{long-step} IPM as this one-sided \(\infty\)-norm bound allows for longer steps inside the neighbourhood compared to a two-sided 2-norm bound such as \(\norm{\vek{x} \circ \vek{s} - \mu \vek{1}}_2 \leq \gamma \mu\).
Neighbourhoods involving those bounds are known as \emph{short-step} IPMs and give better theoretical guarantees while having worse practical performance~\cite[Chapter 5]{Wright-PrimalDualInteriorPointMethods}.
Lastly, the bound on the residuals (where \(\vek{r}^0\) and \(\mu^0\) are defined by the starting point \((\vek{x}^0, \vek{y}^0, \vek{s}^0) \in \mathcal{G}\)) implies that we will allow infeasible iterates as long as their residuals decrease to zero at least as fast as \(\mu\) does.
Any algorithm that maintains iterates inside \(\mathcal{N}_{-\infty}(\gamma)\) and is able to decrease the corresponding \(\mu\) values to zero produces iterates which become arbitrarily close to optimality because \(\norm{\vek{r}}_2\) and \(x_i s_i\) converge to zero.

\section{The Normal Equation}\label{sec:ipm-search-direction}

The search direction in IPMs is determined by a Newton system that aims to find the root of 
\begin{equation}
  F(\vek{x}, \vek{y}, \vek{s}) \deq \begin{pmatrix} \mat{A}\vek{x} - \vek{b} \\ \mat{A}^T \vek{y} + \vek{s} - \vek{c} \\ \vek{x} \circ \vek{s} - \sigma \mu \vek{1} \end{pmatrix}
\end{equation}
where \(\sigma \in (0, 1)\) is a parameter.
In other words, we aim at an element further along the central path by choosing \(\tau = \sigma \mu\).
The search direction \((\Delta\vek{x}, \Delta\vek{y}, \Delta\vek{s})\) is therefore determined by 
\begin{equation}\label{eqn:newton-system}
  \underbrace{
  \begin{pmatrix}
    \mat{A} & \mat{0}   & \mat{0} \\
    \mat{0} & \mat{A}^T & \mat{I} \\
    \mat{S} & \mat{0}   & \mat{X} \\
  \end{pmatrix}
  }_{F'(\vek{x}, \vek{y}, \vek{s})}
  \begin{pmatrix} \Delta\vek{x} \\ \Delta\vek{y} \\ \Delta\vek{s} \end{pmatrix}
  =
  \underbrace{
  \begin{pmatrix} -\vek{r}_p \\ -\vek{r}_d \\ - \vek{x} \circ \vek{s} + \sigma \mu \vek{1} \end{pmatrix}
  }_{-F(\vek{x}, \vek{y}, \vek{s})}
\end{equation}
where \(\mat{X} = \diag(\vek{x})\) and \(\mat{S} = \diag(\vek{s})\).
Because \(\mat{X}\) and \(\mat{S}\) are diagonal they can be easily inverted, and we can simplify the linear system into smaller ones and a series of matrix-vector products.
\Cref{eqn:newton-system} is equivalent to 
\begin{subequations}\label{eqn:augmented-system-system}
  \begin{align}
    \begin{pmatrix}
      \mat{X}^{-1} \mat{S} & \mat{A}^T \\
      \mat{A}              & \mat{0}
    \end{pmatrix}
    \begin{pmatrix}
      \Delta\vek{x} \\
      \Delta\vek{y}
    \end{pmatrix}
    &=
    \begin{pmatrix}
      -\vek{r}_d -\vek{s} + \sigma \mu \mat{X}^{-1} \vek{1} \\
      -\vek{r}_p
    \end{pmatrix} \label{eqn:augmented-system} \\
    \Delta\vek{s} &= -\vek{s} + \sigma \mu \mat{X}^{-1} \vek{1} - \mat{X}^{-1} \mat{S} \Delta\vek{x} \label{eqn:s-from-augmented}
  \end{align}
\end{subequations}
in which the first equation is called the \emph{augmented system} and 
\begin{subequations}\label{eqn:normal-equation-system}
  \begin{align}
    \mat{A} \mat{D}^2 \mat{A}^T \Delta\vek{y} &= -\vek{r}_p + \mat{A} ( -\mat{S}^{-1} \mat{X} \vek{r}_d + \vek{x} - \sigma \mu \mat{S}^{-1} \vek{1} ) \eqqcolon \vek{p} \label{eqn:normal-equation} \\
    \Delta\vek{s} &= -\vek{r}_d - \mat{A}^T \Delta\vek{y} \label{eqn:s-from-normal} \\
    \Delta\vek{x} &= -\vek{x} + \sigma \mu \mat{S}^{-1} \vek{1} - \mat{S}^{-1} \mat{X} \Delta\vek{s} \label{eqn:x-from-normal}
  \end{align}
\end{subequations}
in which the first equation is called the \emph{normal equation}, see~\cite[p. 16]{Wright-PrimalDualInteriorPointMethods}.
Here, \(\mat{D}\) is defined as the diagonal matrix \(\mat{S}^{-1/2} \mat{X}^{1/2}\) to simplify the notation.
\Cref{eqn:normal-equation} is called the normal equation because it can be written as the normal equation of a least-squares system with coefficient matrix \(\mat{D}\mat{A}^T\) if \(\vek{r}_p = \vek{0}\).

\section{Shifting Error Terms}\label{sec:ipm-shifting-error-terms}

Our goal is to solve the normal equation using a preconditioner and an iterative solver.
To figure out how many iterations of this solver are needed to obtain a sufficiently accurate solution, assume that some error term \(\tilde{\vek{f}}\) is introduced.
This means \(\Delta\vek{y}\) solves
\begin{equation}
 \mat{A} \mat{D}^2 \mat{A}^T \Delta\vek{y} = \vek{p} + \tilde{\vek{f}}
\end{equation}
instead of \cref{eqn:normal-equation} and \(\Delta\vek{x}\) and \(\Delta\vek{s}\) are obtained from \cref{eqn:s-from-normal,eqn:x-from-normal} as before.
Because calculating the search direction in this way is equivalent to using \cref{eqn:normal-equation-system} with \(\vek{b} + \tilde{\vek{f}}\) instead of \(\vek{b}\),
the error propagates from the normal equation to the primal feasibility component in \cref{eqn:newton-system}:
\begin{equation}
  \begin{pmatrix}
    \mat{A} & \mat{0}   & \mat{0} \\
    \mat{0} & \mat{A}^T & \mat{I} \\
    \mat{S} & \mat{0}   & \mat{X} \\
  \end{pmatrix}
  \begin{pmatrix} \Delta\vek{x} \\ \Delta\vek{y} \\ \Delta\vek{s} \end{pmatrix}
  =
  \begin{pmatrix} -\vek{r}_p \\ -\vek{r}_d \\ -\vek{x} \circ \vek{s} + \sigma \mu \vek{1} \end{pmatrix}
  +
  \begin{pmatrix} \tilde{\vek{f}} \\ \vek{0} \\ \vek{0} \end{pmatrix}
\end{equation}

For the convergence proofs of \textcite{Monteiro-ConvergenceAnalysisLongStepInfeasibleIPMs} to work we need that the error in calculating the Newton direction only affects the complementarity component of the Newton system.
This can be achieved by constructing a perturbation vector \(\vek{v} \in \R^n\) satisfying \(\mat{A}\mat{S}^{-1}\vek{v} = \tilde{\vek{f}}\) and subtracting \(\mat{S}^{-1}\vek{v}\) from \(\Delta\vek{x}\).
The perturbed Newton direction \((\hat{\Delta\vek{x}}, \hat{\Delta\vek{y}}, \hat{\Delta\vek{s}})\) is determined using
\begin{subequations}\label{eqn:perturbed-search-direction-choice}
  \begin{align}
    \mat{A} \mat{D}^2 \mat{A}^T \hat{\Delta\vek{y}} &= \vek{p} + \tilde{\vek{f}} \\
    \hat{\Delta\vek{s}} &= -\vek{r}_d - \mat{A}^T \hat{\Delta\vek{y}} \\
    \hat{\Delta\vek{x}} &= -\vek{x} + \sigma \mu \mat{S}^{-1} \vek{1} - \mat{S}^{-1} \mat{X} \hat{\Delta\vek{s}} - \mat{S}^{-1} \vek{v}
  \end{align}
\end{subequations}
and solves
\begin{equation}\label{eqn:perturbed-search-direction}
  \begin{pmatrix}
    \mat{A} & \mat{0}   & \mat{0} \\
    \mat{0} & \mat{A}^T & \mat{I} \\
    \mat{S} & \mat{0}   & \mat{X} \\
  \end{pmatrix}
  \begin{pmatrix} \hat{\Delta\vek{x}} \\ \hat{\Delta\vek{y}} \\ \hat{\Delta\vek{s}} \end{pmatrix}
  =
  \begin{pmatrix} -\vek{r}_p \\ -\vek{r}_d \\ -\vek{x} \circ \vek{s} + \sigma \mu \vek{1} \end{pmatrix}
  -
  \begin{pmatrix} \vek{0} \\ \vek{0} \\ \vek{v} \end{pmatrix}
\end{equation}
as shown by \textcite[p. 10]{Monteiro-ConvergenceAnalysisLongStepInfeasibleIPMs}.
Hence, we successfully shifted the error term in the Newton system.
We will see that we can construct \(\vek{v}\) from \(\tilde{\vek{f}}\) in such a way that the size of the error \(\norm{\vek{v}}_2\) is bounded in terms of the error \(\norm{\tilde{\vek{f}}}_2\) in the next chapter.
To see why this is important, consider the following IPM algorithm and the corresponding convergence result
and note that apart from conditions for the initial point it only requires the search direction to satisfy \cref{eqn:perturbed-search-direction} and \(\norm{\vek{v}}_2\) to be sufficiently small.
The proof for \cref{thm:ipm-convergence} will be given in \cref{chap:convergence}.

\section{The Infeasible IPM Algorithm}

\begin{algorithm}[htbp]
  \SetKwInOut{Input}{Input}
  \Input{
    \(\mat{A} \in \R^{m \times n}\),
    \(\vek{b} \in \R^m\),
    \(\vek{c} \in \R^n\),
    \(\gamma \in (0, 1)\),
    \(\sigma \in (0, 1)\),
    tolerance \(\e_\mu \in (0, 1)\) and
    an initial point \((\vek{x}^0, \vek{y}^0, \vek{s}^0)\)
  }
  Initialize \(k = 0\) and \(\mu^0 = {(\vek{x}^0)}^T \vek{s}^0 /n\)\;
  \While{\(\mu^k = {(\vek{x}^k)}^T \vek{s}^k /n > \e_\mu \mu^0\)}{
    Compute \(\vek{r}_p^k = \mat{A}\vek{x}^k - \vek{b}\), \(\vek{r}_d^k = \mat{A}^T \vek{y}^k + \vek{s}^k - \vek{c}\), \(\mat{X} = \diag(\vek{x}^k)\), \(\mat{S} = \diag(\vek{s}^k)\)\;\label{line:prepare-approximate-newton}
    Solve the following linear system such that \(\norm{\vek{v}}_2 \leq  \gamma \sigma \mu^k/4\)
    \begin{equation} \label{eqn:approximate-newton}
      \begin{pmatrix}
        \mat{A} & 0         & 0       \\
        0       & \mat{A}^T & \mat{I} \\
        \mat{S} & 0         & \mat{X} \\
      \end{pmatrix}
      \begin{pmatrix} \hat{\Delta\vek{x}} \\ \hat{\Delta\vek{y}} \\ \hat{\Delta\vek{s}} \end{pmatrix}
      =
      \begin{pmatrix} -\vek{r}_p^k \\ -\vek{r}_d^k \\ -\vek{x}^k \circ \vek{s}^k + \sigma \mu^k \vek{1} \end{pmatrix}
      -
      \begin{pmatrix} 0 \\ 0 \\ \vek{v} \end{pmatrix} \text{\;}
    \end{equation} \\\label{line:compute-approximate-newton}
    Determine the maximum \(\tilde{\alpha} \in [0, 1]\) such that for all \(\alpha \in [0, \tilde{\alpha}]\) we have \((\vek{x}^k, \vek{y}^k, \vek{s}^k) + \alpha (\hat{\Delta\vek{x}}, \hat{\Delta\vek{y}}, \hat{\Delta\vek{s}}) \in \mathcal{N}_{-\infty}(\gamma)\)\;\label{line:alpha-tilde}
    Compute \(\bar{\alpha} = \argmin \set{ {(\vek{x}^k + \alpha \hat{\Delta\vek{x}})}^T (\vek{s}^k + \alpha \hat{\Delta\vek{s}}) | \alpha \in [0, \tilde{\alpha}]}\)\;\label{line:alpha-bar}
    Set \((\vek{x}^{k+1}, \vek{y}^{k+1}, \vek{s}^{k+1}) = (\vek{x}^k, \vek{y}^k, \vek{s}^k) + \bar{\alpha} (\hat{\Delta\vek{x}}, \hat{\Delta\vek{y}}, \hat{\Delta\vek{s}}) \)\;\label{line:compute-next-iterate}
    Increment \(k \leftarrow k+1\)\;\label{line:increment-k}
  }
  \Return{\((\vek{x}^k, \vek{y}^k, \vek{s}^k)\)}\;
  \caption{Infeasible IPM}\label{alg:ipm}
\end{algorithm}

\begin{theorem}\label{thm:ipm-convergence}
  Assume that \(\gamma \in (0, 1)\) and \(\sigma \in (0, \frac{4}{5})\) are constant and that the initial point \((\vek{x}^0, \vek{y}^0, \vek{s}^0) \in \mathcal{G}\) satisfies \(\vek{x}^0 \circ \vek{s}^0 \geq (1-\gamma)\vek{1}\) and \((\vek{x}^0, \vek{s}^0) \geq (\vek{x}^*, \vek{s}^*)\) for some \((\vek{x}^*, \vek{y}^*, \vek{s}^*) \in \mathcal{F}^*\).
  Then \cref{alg:ipm} generates an iterate \((\vek{x}^k, \vek{y}^k, \vek{s}^k)\) satisfying \(\mu^k \leq \e_\mu \mu^0\) and \(\norm{\vek{r}^k} \leq \e_\mu \norm{\vek{r}^0}\) within \(O(n^2 \log(1/\e_\mu))\) iterations.
\end{theorem}
