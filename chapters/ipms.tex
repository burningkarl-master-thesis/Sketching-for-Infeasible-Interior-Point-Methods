\chapter{Interior-Point Methods for Linear Programming}\label{chap:ipm}

% Important concepts to cover
% \begin{enumerate}
%  \item Notation (A, b, c, x, y, s, residuals, mu)
%  \item Optimality conditions
%  \item Newton system: Complete system (?), augmented system, normal equation
%  \item Focus on infeasible, inexact, long-step IPMs
%  \begin{enumerate}
%    \item Central path
%    \item \(\mathcal{N}_{-\infty}\) neighbourhood
%    \item Error in the normal equation translates to error in \(\mat{A}\Delta\vek{x} = \vek{r}_p\)
%  \end{enumerate}
% \end{enumerate}

% \hrule

The standard form linear programming (LP) problem is given by
\begin{equation}\label{eqn:primal-lp}
 \min \vek{c}^T \vek{x} \quad \text{subject to} \quad \mat{A}\vek{x} = \vek{b}, \ \vek{x} \geq \vek{0} \tag{P}
\end{equation}
and its dual is
\begin{equation}\label{eqn:dual-lp}
  \max \vek{b}^T \vek{y} \quad \text{subject to} \quad \mat{A}^T \vek{y} + \vek{s} = \vek{c}, \ \vek{s} \geq \vek{0} \tag{D}
\end{equation}
where \(\mat{A} \in \R^{m \times n}\), \(\vek{b} \in \R^m\) and \(\vek{c} \in \R^n\) are given and \(\vek{x}, \vek{s} \in \R^n\) and \(\vek{y} \in \R^m\) are the variables.
A triple \((\vek{x}, \vek{y}, \vek{s})\) is called \emph{feasible} if it satisifies the constraints \(\mat{A}\vek{x} = \vek{b}\), \(\mat{A}^T \vek{y} + \vek{s} = \vek{c}\) and \(\vek{x}, \vek{s} \geq \vek{0}\).
The two linear programs \labelcref{eqn:primal-lp} and \labelcref{eqn:dual-lp} are closely connected.
For feasible \((\vek{x}, \vek{y}, \vek{s})\) we always have \(\vek{c}^T \vek{x} \geq \vek{b}^T \vek{y}\) and the joint solutions satisfy \(\vek{c}^T \vek{x} = \vek{b}^T \vek{y}\).
Therefore, if there are feasible points both linear programs are feasible and bounded and solutions exist.
Furthermore, as the optimal value of the objective function for both LPs is the same, it is upper bounded by \(\vek{c}^T \vek{x}\) and lower bounded by \(\vek{b}^T \vek{y}\) at any feasible point.
\(\vek{x}^T \vek{s} = \vek{c}^T \vek{x} - \vek{b}^T \vek{y}\) can thus be interpreted as a measure of how close to optimality a given feasible point is.

As already seen, necessary and sufficient optimality conditions are given by the feasibility conditions and \(\vek{x}^T \vek{s} = \vek{c}^T \vek{x} - \vek{b}^T \vek{y} = 0\).
Because the entries of \(\vek{x}\) and \(\vek{s}\) are both nonnegative this is equivalent to the complementarity condition \(\vek{x} \circ \vek{s} = \vek{0}\) requiring that for each \(1 \leq i \leq n\) at least one of \(x_i\) and \(s_i\) is zero.
Necessary and sufficient optimality conditions for both problems are therefore given by
\begin{subequations}\label{eqn:optimality-conditions}
  \begin{align}
    \label{eqn:primal-feasibility}
    \vek{r}_p = \mat{A}\vek{x} - \vek{b} &= \vek{0} \\
    \label{eqn:dual-feasibility}
    \vek{r}_d = \mat{A}^T \vek{y} + \vek{s} - \vek{c} &= \vek{0} \\
    \label{eqn:nonnegativity}
    \vek{x}, \vek{s} &\geq \vek{0} \\
    \label{eqn:complementarity}
    \vek{x} \circ \vek{s} &= \vek{0}
  \end{align}
\end{subequations}
Here, \(\vek{r}_p\) and \(\vek{r}_d\) denote the primal and dual residuals and \(\vek{r} = (\vek{r}_p, \vek{r}_d)\) denotes their concatenation as a vector in \(\R^{m+n}\).
We denote the set of feasible points with \(\mathcal{F}\) and the set of solutions as \(\mathcal{F}^*\).
The feasible points which satisfy \cref{eqn:nonnegativity} strictly (all entries need to be positive) are called \emph{strictly feasible} and the set of strictly feasible points is denoted by \(\mathcal{F}^o\).

Assume strictly feasible points exist, then the unique solutions \((\vek{x}_\tau, \vek{y}_\tau, \vek{s}_\tau)\) of
\begin{subequations}
  \begin{align}
    \mat{A}\vek{x}_\tau - \vek{b} &= \vek{0} \\
    \mat{A}^T \vek{y}_\tau + \vek{s}_\tau - \vek{c} &= \vek{0} \\
    \vek{x}_\tau, \vek{s}_\tau &> \vek{0} \label{eqn:strict-nonnegativity} \\
    \vek{x}_\tau \circ \vek{s}_\tau &= \tau \vek{1}.
  \end{align}
\end{subequations}
for \(\tau > 0\) form a continous path of strictly feasible points which is called the \emph{central path}, see~\cite[Theorem 2.8]{Wright-PrimalDualInteriorPointMethods}.
Moreover, the elements of the central path converge towards a solution as \(\tau \to 0\).

Primal-dual IPMs now try to follow this path towards a solution by maintaining iterates in some neighbourhood of the central path and computing the search direction in each step as a Newton direction towards an element of the central path.
In this thesis we focus on an IPM variant with good practical properties.
Its neighbourhood is defined by
\begin{equation}
  \mathcal{N}_{-\infty}(\gamma) = \Set{ (\vek{x}, \vek{y}, \vek{s}) | \vek{x}, \vek{s} > \vek{0}, \ \vek{x} \circ \vek{s} \geq (1 - \gamma) \mu \vek{1}, \ \frac{\norm{\vek{r}}_2}{\norm{\vek{r}^0}_2} \leq \frac{\mu}{\mu^0} }. \label{eqn:neighbourhood}
\end{equation}
where \(\mu = \vek{x}^T \vek{s}/n\), i.e.\ the average value of \(x_i s_i\), and the parameter \(\gamma \in (0, 1)\) controls the size of the neighbourhood.
The first condition is one that all IPMs share.
It keeps the \(\vek{x}\) and \(\vek{s}\) components of the iterates in the interior of the nonnegative orthant.
The second one qualifies this IPM as a \emph{long-step} IPM as this one-sided \(\infty\)-norm bound allows for longer steps inside the neighbourhood compared to a two-sided 2-norm bound such as \(\norm{\vek{x} \circ \vek{s} - \mu \vek{1}}_2 \leq \gamma\).
Neighbourhoods involving those bounds are known as \emph{short-step} IPMs and give better theoretical guarantees while having worse pratical performance.
Lastly, the bound on the residuals (where \(\vek{r}^0\) and \(\mu^0\) are defined by the starting point \((\vek{x}^0, \vek{y}^0, \vek{s}^0)\)) implies that we will allow infeasible iterates as long as their residuals decrease to zero at least as fast as \(\mu\) does.
Any algorithm that maintains iterates inside \(\mathcal{N}_{-\infty}(\gamma)\) and is able to decrease the corresponding \(\mu\) values to zero produces iterates which become arbitrarily close to optimality because \(\norm{\vek{r}}_2\) and \(x_i s_i\) converge to zero.

The search direction in every iteration is determined by a Newton system, similar to the one used when trying to find a root of 
\begin{equation}
  F(\vek{x}, \vek{y}, \vek{s}) \deq \begin{pmatrix} \mat{A}\vek{x} - \vek{b} \\ \mat{A}^T \vek{y} + \vek{s} - \vek{c} \\ \vek{x} \circ \vek{s} \end{pmatrix}
\end{equation}
The total derivative of \(F\) is given by
\begin{equation}
  F'(\vek{x}, \vek{y}, \vek{s}) = \begin{pmatrix}
    \mat{A} & \mat{0}   & \mat{0} \\
    \mat{0} & \mat{A}^T & \mat{I} \\
    \mat{S} & \mat{0}   & \mat{X} \\
  \end{pmatrix}
\end{equation}
where \(\mat{X} = \diag(\vek{x})\) and \(\mat{S} = \diag(\vek{s})\).
The actual Newton system determining \((\Delta\vek{x}, \Delta\vek{y}, \Delta\vek{s})\) is slightly modified by adding a centering term \(\sigma \mu \vek{1}\) where \(\sigma \in [0,1]\) which allows for larger steps inside the neighbourhood:
\begin{equation} \label{eqn:newton-system}
  \underbrace{
  \begin{pmatrix}
    \mat{A} & \mat{0}   & \mat{0} \\
    \mat{0} & \mat{A}^T & \mat{I} \\
    \mat{S} & \mat{0}   & \mat{X} \\
  \end{pmatrix}
  }_{F'(\vek{x}, \vek{y}, \vek{s})}
  \begin{pmatrix} \Delta\vek{x} \\ \Delta\vek{y} \\ \Delta\vek{s} \end{pmatrix}
  = -
  \underbrace{
  \begin{pmatrix} \vek{r}_p \\ \vek{r}_d \\ \vek{x} \circ \vek{s} \end{pmatrix}
  }_{F(\vek{x}, \vek{y}, \vek{s})}
  + \begin{pmatrix} \vek{0} \\ \vek{0} \\ \sigma \mu \vek{1} \end{pmatrix}
\end{equation}

This linear system of equations can be rewritten as
\begin{subequations}\label{eqn:augmented-system}
  \begin{align}
    \begin{pmatrix}
      \mat{A}              & \mat{0} \\
      \mat{X}^{-1} \mat{S} & \mat{A}^T \\
    \end{pmatrix}
    \begin{pmatrix}
      \Delta\vek{x} \\
      \Delta\vek{y}
    \end{pmatrix}
    &=
    \begin{pmatrix}
      -\vek{r}_p \\
      -\vek{r}_d -\vek{s} + \sigma \mu \mat{X}^{-1} \vek{1}
    \end{pmatrix} \\
    \Delta\vek{s} &= -\vek{s} + \sigma \mu \mat{X}^{-1} \vek{1} - \mat{X}^{-1} \mat{S} \Delta\vek{x}
  \end{align}
\end{subequations}
which is called the \emph{augmented system}.
Eliminating \(\Delta\vek{x}\) from the first equation and using \(\mat{D} = \mat{S}^{-1/2} \mat{X}^{1/2}\) we get
\begin{subequations}\label{eqn:normal-equation-system}
  \begin{align}
    \mat{A} \mat{D}^2 \mat{A}^T \Delta\vek{y} &= -\vek{r}_p + \mat{A} ( -\mat{S}^{-1} \mat{X} \vek{r}_d + \vek{x} - \sigma \mu \mat{S}^{-1} \vek{1} ) \eqqcolon \vek{p} \label{eqn:normal-equation} \\
    \Delta\vek{s} &= -\vek{r}_d - \mat{A}^T \Delta\vek{y} \label{eqn:s-from-normal} \\
    \Delta\vek{x} &= -\vek{x} + \sigma \mu \mat{S}^{-1} \vek{1} - \mat{S}^{-1} \mat{X} \Delta\vek{s} \label{eqn:x-from-normal}
  \end{align}
\end{subequations}
The first component is called the \emph{normal equation} because it can be written as the normal equation of a least squares system if \(\vek{r}_p = \vek{0}\).

We will construct a preconditioner for \(\mat{A} \mat{D}^2 \mat{A}^T\) in the form of a nonsingular matrix \(\mat{P} \in \R^{m \times m}\) such that
\( \mat{P}^{-T} \mat{A} \mat{D}^2 \mat{A}^T \mat{P}^{-1} \approx \mat{I}_m \).
Instead of solving the normal equation we then solve
\begin{equation} \label{eqn:preconditioned-system}
 \mat{P}^{-T} \mat{A} \mat{D}^2 \mat{A}^T \mat{P}^{-1} \vek{q} = \mat{P}^{-T} \vek{p}
\end{equation}
using an efficient iterative solver and \(\Delta\vek{y} = \mat{P}^{-1} \vek{q}\).
Note that applying the preconditioner from the left and from the right in this form ensures that the matrix stays symmetric positive definite and we can apply the CG algorithm to this linear system.

Using an iterative method to solve \cref{eqn:preconditioned-system} introduces an error term \(\vek{f}\). Let \(\Delta\tilde{\vek{x}}, \Delta\tilde{\vek{y}}, \Delta\tilde{\vek{s}}\) be calculated using \cref{eqn:s-from-normal,eqn:x-from-normal} given that \(\Delta\tilde{\vek{y}}\) solves
\begin{equation}
 \mat{P}^{-T} \mat{A} \mat{D}^2 \mat{A}^T \Delta\tilde{\vek{y}} = \mat{P}^{-T} \vek{p} + \vek{f} \iff \mat{A} \mat{D}^2 \mat{A}^T \Delta\tilde{\vek{y}} = \vek{p} + \mat{P}^{T} \vek{f}
\end{equation}
instead of \cref{eqn:normal-equation}. In this case
\begin{equation}
  \begin{pmatrix}
    \mat{A} & \mat{0}   & \mat{0} \\
    \mat{0} & \mat{A}^T & \mat{I} \\
    \mat{S} & \mat{0}   & \mat{X} \\
  \end{pmatrix}
  \begin{pmatrix} \Delta\tilde{\vek{x}} \\ \Delta\tilde{\vek{y}} \\ \Delta\tilde{\vek{s}} \end{pmatrix}
  =
  \begin{pmatrix} -\vek{r}_p \\ -\vek{r}_d \\ -\vek{x} \circ \vek{s} + \sigma \mu \vek{1} \end{pmatrix}
  +
  \begin{pmatrix} \mat{P}^T \vek{f} \\ \vek{0} \\ \vek{0} \end{pmatrix}
\end{equation}
because calculating \(\Delta\tilde{\vek{x}}, \Delta\tilde{\vek{y}}, \Delta\tilde{\vek{s}}\) in this way is equivalent to using \cref{eqn:normal-equation-system} with \(\vek{b} + \mat{P}^T \vek{f}\) instead of \(\vek{b}\).

For our convergence proofs to work we need that the error in calculating the Newton direction only affects the last line of \cref{eqn:newton-system}.
This can be achieved by adding a perturbation vector \(\mat{S}^{-1}\vek{v} \in \R^n\) to \(\Delta\tilde{\vek{x}}\) which satisfies \(\mat{A}\mat{S}^{-1}\vek{v} = \mat{P}^T \vek{f}\).
The new instructions to get an approximate solution to the Newton system are now given by
\begin{subequations}
  \begin{align}
    \mat{A} \mat{D}^2 \mat{A}^T \Delta\hat{\vek{y}} &= \vek{p} + \mat{P}^{T} \vek{f} \\
    \Delta\hat{\vek{s}} &= -\vek{r}_d - \mat{A}^T \Delta\hat{\vek{y}} \\
    \Delta\hat{\vek{x}} &= -\vek{x} + \sigma \mu \mat{S}^{-1} \vek{1} - \mat{S}^{-1} \mat{X} \Delta\hat{\vek{s}} - \mat{S}^{-1} \vek{v}
  \end{align}
\end{subequations}
This approximate solution solves the following system
\begin{equation}
  \begin{pmatrix}
    \mat{A} & \mat{0}   & \mat{0} \\
    \mat{0} & \mat{A}^T & \mat{I} \\
    \mat{S} & \mat{0}   & \mat{X} \\
  \end{pmatrix}
  \begin{pmatrix} \Delta\hat{\vek{x}} \\ \Delta\hat{\vek{y}} \\ \Delta\hat{\vek{s}} \end{pmatrix}
  =
  \begin{pmatrix} -\vek{r}_p \\ -\vek{r}_d \\ -\vek{x} \circ \vek{s} + \sigma \mu \vek{1} \end{pmatrix}
  -
  \begin{pmatrix} \vek{0} \\ \vek{0} \\ \vek{v} \end{pmatrix}
\end{equation}
so we successfully shifted the error term from the first to the third component.

To control the size of the error \(\norm{\vek{v}}_2\) in terms of the error \(\norm{\vek{f}}_2\) we need to design a clever way of choosing \(\vek{v}\) which will be done in the next chapter.
